{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"TimeCaVe documentation","text":"<p>Welcome to TimeCaVe's documentation! The package that helps you validate your time series models!</p>"},{"location":"#what-is-timecave","title":"What is TimeCaVe?","text":"<p>TimeCaVe is a Python package that provides several off-the-shelf validation methods for time series modelling tasks. These methods were conceived with time series forecasting in mind, though, in principle, they could also be applied to other time series related tasks.</p> <p>Our generic implementation is flexible enough for them to be used with ANY kind of time series model, including classical time series forecasting  models (e.g. AR models), classical machine learning models (e.g. decision trees) and deep learning models specifically tailored to handle sequence data (e.g. LSTMs).</p> <p>In addition to the validation methods themselves, TimeCaVe provides several utility functions and classes. These include specific metrics to measure the performance of a validation method, data generation methods, functions to aid in the data collection process, and functions to extract important features from the data.</p>"},{"location":"#why-use-timecave","title":"Why use TimeCaVe?","text":"<p>Model validation is a crucial step in the machine learning modelling process, and time series forecasting is no different. However, while implementations of model validation methods for tabular data abound, there is a shortage of publicly available packages that provide easy-to-use implementations of validation methods for time series data. This is exactly what TimeCaVe does.</p>"},{"location":"#main-features","title":"Main features:","text":"<ul> <li>Versatility: Our flexible, generic implementations are able to accomodate a wide variety of time series forecasting models.</li> <li>Scikit-learn-like syntax: TimeCaVe's validation methods are implemented in a similar way to those of Scikit-learn, thereby smoothing the learning curve.</li> <li>Extra functionality: In addition to splitting your data, TimeCaVe allows you to plot your partitioned data, compute relevant statistics for all training and validation sets, and access information regarding how much data is being used for training and validation. All this to ensure you make the best possible use of your data.</li> <li>Data generation: Generate synthetic time series data easily with TimeCaVe's data generation capabilities.</li> <li>Validation metrics: Several metrics are provided to help you select the most appropriate validation method, if need be.</li> <li>Data collection: TimeCaVe provides utility functions that can help determine the necessary amount of samples to capture a given dominant frequency.</li> </ul>"},{"location":"#documentation-guide","title":"Documentation Guide","text":"<p>For a quick overview of how to install and use TimeCaVe, please check our Getting Started guide.</p> <p>Detailed information about each function or class is provided in TimeCaVe's API reference.</p> <p>For more information regarding the package authors, please refer to the About section.</p>"},{"location":"ChangeLog/","title":"TimeCaVe Changelog","text":""},{"location":"ChangeLog/#100-21082024","title":"1.0.0 21/08/2024","text":"<ul> <li>First major release</li> </ul>"},{"location":"about/","title":"About","text":"<p>TimeCaVe is a Python package that provides off-the-shelf model validation methods for time series modelling tasks. The package was developed by two graduate research assistants at IST, University of Lisbon, Portugal. Links to their Github and LinkedIn profiles can be found at the bottom-right of this page.</p> <p>This project would not have seen the light of day without the support of the Mechanical Engineering Institute (IDMEC) and the  Laboratory of Intelligent Systems for Data Analysis, Modeling and Optimization (IS4).</p> <p></p> <p></p>"},{"location":"starters/","title":"Getting Started","text":""},{"location":"starters/#installation","title":"Installation","text":""},{"location":"starters/#using-pip","title":"Using pip","text":"<p>TimeCaVe can be directly installed from PyPi using pip:</p> <pre><code>pip install timecave\n</code></pre> <p>To install the development version, simply type:</p> <pre><code>pip install \"timecave[dev]\"\n</code></pre> <p>This will install dependencies that have been used to develop TimeCaVe and its documentation, such as Black and MKDocs.</p>"},{"location":"starters/#using-git","title":"Using git","text":"<p>TimeCaVe can also be installed using git. To do so, clone the repository:</p> <pre><code>git clone https://github.com/MiguelLoureiro98/timecave.git\n</code></pre> <p>Then, move into the cloned repository and install the package using pip:</p> <pre><code>cd timecave\npip install .\n</code></pre> <p>Again, to install development dependencies, simply type:</p> <pre><code>pip install \".[dev]\"\n</code></pre>"},{"location":"starters/#basic-usage","title":"Basic Usage","text":"<p>TimeCaVe is, above all else, built to provide easy-to-use validation methods for time series forecasting models. The syntax is relatively similar to that of the methods provided by Scikit-learn (e.g. K-fold). Here is an example of how to use one of the methods provided by this package (Block Cross-Validation):</p> <pre><code>import numpy as np\nfrom timecave.validation_methods.CV import BlockCV\n\nts = np.arange(0, 10)\n\n# Split the data into 5 folds\nsplitter = BlockCV(5, ts);\n\nfor train, test, _ in splitter.split():\n\n    training_data = ts[train];\n    validation_data = ts[test];\n\n    # Train and validate your model\n</code></pre> <p>For more information on how to use the package, please refer to our API reference, where detailed descriptions of every function and class are provided.</p>"},{"location":"API_ref/","title":"TimeCave API Reference","text":"<p>This is the API reference for the TimeCaVe library. This package includes two subpackages (Validation methods and Data generation) as well as three independent submodules (Data characteristics, Metrics and Utilities).</p> <p>This is the place to look if you need more information about any function or class provided by the TimeCaVe package. This includes an introductory summary, a detailed list of their parameters, a description of the algorithm (when applicable), and a few examples.</p>"},{"location":"API_ref/#subpackages","title":"Subpackages:","text":"<ul> <li>Validation methods: Implements model validation methods for time series forecasting tasks.</li> <li>Data generation: Provides routines to generate synthetic time series data.</li> </ul>"},{"location":"API_ref/#modules","title":"Modules:","text":"<ul> <li>Data characteristics: Allows the user to extract important statistical and frequency-domain features from the time series.</li> <li>Metrics: Provides specific metrics to evaluate the performance of a validation method.</li> <li>Utilities: Provides some utility functions to aid in the data collection process. </li> </ul>"},{"location":"API_ref/data_characteristics/","title":"Data characteristics module","text":""},{"location":"API_ref/data_characteristics/#timecave.data_characteristics","title":"<code>timecave.data_characteristics</code>","text":"<p>This module contains functions to compute time series features.</p> <p>The 'get_features' function extracts all features supported by this package. Functions to extract the strength of trend, mean-crossing rate, and median-crossing rate are also provided. For the remaining features, the tsfel package should be used.</p> <p>Functions:</p> Name Description <code>get_features</code> <p>Extract 13 different features from a time series.</p> <code>strength_of_trend</code> <p>Compute the time series' strength of trend.</p> <code>mean_crossing_rate</code> <p>Compute the time series' mean-crossing rate.</p> <code>median_crossing_rate</code> <p>Compute the time series' median-crossing rate.</p>"},{"location":"API_ref/data_characteristics/get_features/","title":"get_features","text":""},{"location":"API_ref/data_characteristics/get_features/#timecave.data_characteristics.get_features","title":"<code>timecave.data_characteristics.get_features(ts, fs)</code>","text":"<p>Compute time series features.</p> <p>This function extracts features from a time series. The tsfel package is used to extract most features, and should be used if only these are required. The exceptions are the 'Strength of Trend', 'Mean-crossing rate', and 'Median-crossing rate' features, for which custom functions were developed (these were also made available to the user). </p> <p>Parameters:</p> Name Type Description Default <code>ts</code> <code>ndarray | Series</code> <p>Univariate time series.</p> required <code>fs</code> <code>float | int</code> <p>Sampling frequency (Hz).</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>Data frame containing all time series features supported by this package.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If <code>ts</code> is neither an Numpy array nor a Pandas series.</p> <code>TypeError</code> <p>If <code>fs</code> is neither a float nor an integer.</p> <code>ValueError</code> <p>If <code>fs</code> is negative.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from timecave.data_characteristics import get_features\n&gt;&gt;&gt; t = np.arange(0, 10, 0.01);\n&gt;&gt;&gt; time_series = np.sin(2 * np.pi * t);\n&gt;&gt;&gt; sampling_frequency = 1 / 0.01;\n&gt;&gt;&gt; get_features(time_series, sampling_frequency)\n           Mean        Median  Min  Max  Variance  P2P_amplitude  Trend_slope  Spectral_centroid  Spectral_rolloff  Spectral_entropy  Strength_of_trend  Mean_crossing_rate  Median_crossing_rate\n0  3.552714e-18 -3.673940e-16 -1.0  1.0       0.5            2.0    -0.000191                1.0               1.0      6.485530e-29          15.926086             0.02002              0.019019\n</code></pre> <p>If the time series is neither an array nor a series, an exception is thrown:</p> <pre><code>&gt;&gt;&gt; get_features([0, 1, 2], sampling_frequency)\nTraceback (most recent call last):\n...\nTypeError: Time series must be either a Numpy array or a Pandas series.\n</code></pre> <p>The same happens if the sampling frequency is neither a float nor an integer:</p> <pre><code>&gt;&gt;&gt; get_features(time_series, \"Hello\")\nTraceback (most recent call last):\n...\nTypeError: The sampling frequency should be either a float or an integer.\n</code></pre> <p>A different exception is raised if the sampling frequency is negative:</p> <pre><code>&gt;&gt;&gt; get_features(time_series, -1)\nTraceback (most recent call last):\n...\nValueError: The sampling frequency should be larger than zero.\n</code></pre> Source code in <code>timecave/data_characteristics.py</code> <pre><code>def get_features(ts: np.ndarray | pd.Series, fs: float | int) -&gt; pd.DataFrame:\n\n    \"\"\"\n    Compute time series features.\n\n    This function extracts features from a time series. The tsfel package is used to extract most features,\n    and should be used if only these are required. The exceptions are the 'Strength of Trend',\n    'Mean-crossing rate', and 'Median-crossing rate' features, for which custom functions were\n    developed (these were also made available to the user). \n\n    Parameters\n    ----------\n    ts : np.ndarray | pd.Series\n        Univariate time series.\n\n    fs : float | int\n        Sampling frequency (Hz).\n\n    Returns\n    -------\n    pd.DataFrame\n        Data frame containing all time series features supported by this package.\n\n    Raises\n    ------\n    TypeError\n        If `ts` is neither an Numpy array nor a Pandas series.\n\n    TypeError\n        If `fs` is neither a float nor an integer.\n\n    ValueError\n        If `fs` is negative.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; from timecave.data_characteristics import get_features\n    &gt;&gt;&gt; t = np.arange(0, 10, 0.01);\n    &gt;&gt;&gt; time_series = np.sin(2 * np.pi * t);\n    &gt;&gt;&gt; sampling_frequency = 1 / 0.01;\n    &gt;&gt;&gt; get_features(time_series, sampling_frequency)\n               Mean        Median  Min  Max  Variance  P2P_amplitude  Trend_slope  Spectral_centroid  Spectral_rolloff  Spectral_entropy  Strength_of_trend  Mean_crossing_rate  Median_crossing_rate\n    0  3.552714e-18 -3.673940e-16 -1.0  1.0       0.5            2.0    -0.000191                1.0               1.0      6.485530e-29          15.926086             0.02002              0.019019\n\n    If the time series is neither an array nor a series, an exception is thrown:\n\n    &gt;&gt;&gt; get_features([0, 1, 2], sampling_frequency)\n    Traceback (most recent call last):\n    ...\n    TypeError: Time series must be either a Numpy array or a Pandas series.\n\n    The same happens if the sampling frequency is neither a float nor an integer:\n\n    &gt;&gt;&gt; get_features(time_series, \"Hello\")\n    Traceback (most recent call last):\n    ...\n    TypeError: The sampling frequency should be either a float or an integer.\n\n    A different exception is raised if the sampling frequency is negative:\n\n    &gt;&gt;&gt; get_features(time_series, -1)\n    Traceback (most recent call last):\n    ...\n    ValueError: The sampling frequency should be larger than zero.\n    \"\"\"\n\n    _check_type(ts);\n    _check_sampling_rate(fs);\n\n    #feature_list = [\"0_Mean\", \"0_Median\", \"0_Min\", \"0_Max\", \"0_Variance\", \"0_Peak to peak distance\"];\n\n    #cfg = tsfel.get_features_by_domain(\"statistical\");\n    #stat_feat_df = tsfel.time_series_features_extractor(cfg, ts, fs);\n\n    #relevant_feat_df = stat_feat_df[feature_list].copy();\n    #new_names = [feat[2:] for feat in feature_list];\n    #cols = {name: new_name for (name, new_name) in zip(feature_list, new_names)};\n    #relevant_feat_df = relevant_feat_df.rename(columns=cols);\n    #relevant_feat_df = relevant_feat_df.rename(columns={\"Peak to peak distance\": \"P2P_amplitude\"});\n\n    mean = tsfel.calc_mean(ts);\n    median = tsfel.calc_median(ts);\n    minimum = tsfel.calc_min(ts);\n    maximum = tsfel.calc_max(ts);\n    variance = tsfel.calc_var(ts);\n    p2p = tsfel.pk_pk_distance(ts);\n    feature_list = [mean, median, minimum, maximum, variance, p2p];\n    feature_names = [\"Mean\", \"Median\", \"Min\", \"Max\", \"Variance\", \"P2P_amplitude\"];\n\n    relevant_feat_df = pd.DataFrame(data={name: [feat] for name, feat in zip(feature_names, feature_list)});\n    relevant_feat_df[\"Trend_slope\"] = tsfel.slope(ts);\n    relevant_feat_df[\"Spectral_centroid\"] = tsfel.spectral_centroid(ts, fs);\n    relevant_feat_df[\"Spectral_rolloff\"] = tsfel.spectral_roll_off(ts, fs);\n    relevant_feat_df[\"Spectral_entropy\"] = tsfel.spectral_entropy(ts, fs);\n    relevant_feat_df[\"Strength_of_trend\"] = strength_of_trend(ts);\n    relevant_feat_df[\"Mean_crossing_rate\"] = mean_crossing_rate(ts);\n    relevant_feat_df[\"Median_crossing_rate\"] = median_crossing_rate(ts);\n\n    return relevant_feat_df;\n</code></pre>"},{"location":"API_ref/data_characteristics/mean_cr/","title":"mean_crossing_rate","text":""},{"location":"API_ref/data_characteristics/mean_cr/#timecave.data_characteristics.mean_crossing_rate","title":"<code>timecave.data_characteristics.mean_crossing_rate(ts)</code>","text":"<p>Compute the series' mean-crossing rate.</p> <p>This function computes the mean-crossing rate of a given time series. The mean-crossing rate is defined as the rate at which the values of a time series change from being below its mean value to above said value. In practice, the mean is subtracted from the time series, and the zero-crossing rate is then computed.</p> <p>Parameters:</p> Name Type Description Default <code>ts</code> <code>ndarray | Series</code> <p>Univariate time series.</p> required <p>Returns:</p> Type Description <code>float</code> <p>Mean-crossing rate.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If <code>ts</code> is neither a Numpy array nor a Pandas series.</p> See also <p>median_crossing_rate:     Uses the median instead of the mean.</p> Notes <p>The mean-crossing rate is defined as the fraction of times a mean-crossing takes place in the whole time series.     A mean-crossing occurs when two adjacent values have different signs with respect to the mean     (i.e. the first one is below the mean while the second one is above it, and vice-versa).</p> \\[ MCR = \\frac{1}{n-1} \\sum_{i=2}^{n} |sign(a_i - \\mu) - sign(a_{i-1} - \\mu)|   \\] <p>where \\(n\\) is the number of samples in the time series, \\(a_i\\) are its values, and \\(\\mu\\) represents its mean. For more details, please refer to [1].</p> References <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from timecave.data_characteristics import mean_crossing_rate\n&gt;&gt;&gt; ts = np.array([0, 20, 0, 20, 0]);\n&gt;&gt;&gt; mean_crossing_rate(ts)\n1.0\n&gt;&gt;&gt; ts2 = np.ones(10);\n&gt;&gt;&gt; mean_crossing_rate(ts2)\n0.0\n&gt;&gt;&gt; ts3 = np.array([50, 50, 50, 0, 0]);\n&gt;&gt;&gt; mean_crossing_rate(ts3)\n0.25\n</code></pre> <p>If the time series is neither an array nor a series, an exception is thrown:</p> <pre><code>&gt;&gt;&gt; mean_crossing_rate([0, 1, 2])\nTraceback (most recent call last):\n...\nTypeError: Time series must be either a Numpy array or a Pandas series.\n</code></pre> Source code in <code>timecave/data_characteristics.py</code> <pre><code>def mean_crossing_rate(ts: np.ndarray | pd.Series) -&gt; float:\n\n    \"\"\"\n    Compute the series' mean-crossing rate.\n\n    This function computes the mean-crossing rate of a given time series.\n    The mean-crossing rate is defined as the rate at which the values of a time\n    series change from being below its mean value to above said value.\n    In practice, the mean is subtracted from the time series, and the zero-crossing\n    rate is then computed.\n\n    Parameters\n    ----------\n    ts : np.ndarray | pd.Series\n        Univariate time series.\n\n    Returns\n    -------\n    float\n        Mean-crossing rate.\n\n    Raises\n    ------\n    TypeError\n        If `ts` is neither a Numpy array nor a Pandas series.\n\n    See also\n    --------\n    [median_crossing_rate](med_cr.md):\n        Uses the median instead of the mean.\n\n    Notes\n    -----\n    The mean-crossing rate is defined as the fraction of times a mean-crossing takes place in the whole time series. \\\n    A mean-crossing occurs when two adjacent values have different signs with respect to the mean \\\n    (i.e. the first one is below the mean while the second one is above it, and vice-versa).\n\n    $$\n    MCR = \\\\frac{1}{n-1} \\sum_{i=2}^{n} |sign(a_i - \\mu) - sign(a_{i-1} - \\mu)|  \n    $$\n\n    where $n$ is the number of samples in the time series, $a_i$ are its values, and $\\mu$ represents its mean.\n    For more details, please refer to [[1]](#1).\n\n    References\n    ----------\n    ##1\n    Bohdan Myroniv, Cheng-Wei Wu, Yi Ren, Albert Christian, Ensa Bajo,\n    and Yu-chee Tseng. Analyzing user emotions via physiology signals. Data\n    Science and Pattern Recognition, 2, 12 2017.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; from timecave.data_characteristics import mean_crossing_rate\n    &gt;&gt;&gt; ts = np.array([0, 20, 0, 20, 0]);\n    &gt;&gt;&gt; mean_crossing_rate(ts)\n    1.0\n    &gt;&gt;&gt; ts2 = np.ones(10);\n    &gt;&gt;&gt; mean_crossing_rate(ts2)\n    0.0\n    &gt;&gt;&gt; ts3 = np.array([50, 50, 50, 0, 0]);\n    &gt;&gt;&gt; mean_crossing_rate(ts3)\n    0.25\n\n    If the time series is neither an array nor a series, an exception is thrown:\n\n    &gt;&gt;&gt; mean_crossing_rate([0, 1, 2])\n    Traceback (most recent call last):\n    ...\n    TypeError: Time series must be either a Numpy array or a Pandas series.\n    \"\"\"\n\n    _check_type(ts);\n\n    new_ts = ts - ts.mean();\n\n    if(isinstance(ts, pd.Series) is True):\n\n        ts = ts.to_numpy();\n\n    mcr = np.nonzero(np.diff(np.sign(new_ts)))[0].shape[0] / (ts.shape[0] - 1);\n\n    return mcr;\n</code></pre>"},{"location":"API_ref/data_characteristics/mean_cr/#timecave.data_characteristics.mean_crossing_rate--1","title":"1","text":"<p>Bohdan Myroniv, Cheng-Wei Wu, Yi Ren, Albert Christian, Ensa Bajo, and Yu-chee Tseng. Analyzing user emotions via physiology signals. Data Science and Pattern Recognition, 2, 12 2017.</p>"},{"location":"API_ref/data_characteristics/med_cr/","title":"median_crossing_rate","text":""},{"location":"API_ref/data_characteristics/med_cr/#timecave.data_characteristics.median_crossing_rate","title":"<code>timecave.data_characteristics.median_crossing_rate(ts)</code>","text":"<p>Compute the series' median-crossing rate.</p> <p>This function computes the median-crossing rate of a given time series. The median-crossing rate is defined as the rate at which the values of a time series change from being below its median value to above said value. In practice, the median is subtracted from the time series, and the zero-crossing rate is then computed.</p> <p>Parameters:</p> Name Type Description Default <code>ts</code> <code>ndarray | Series</code> <p>Univariate time series.</p> required <p>Returns:</p> Type Description <code>float</code> <p>Median-crossing rate.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If <code>ts</code> is neither a Numpy array nor a Pandas series.</p> See also <p>mean_crossing_rate     Uses the mean instead of the median.</p> Notes <p>The median-crossing rate is similar to the mean-crossing rate, but it uses the median as a reference value.     It can be computed from the following formula:</p> \\[ MedCR = \\frac{1}{n-1} \\sum_{i=2}^{n} |sign(a_i - Med) - sign(a_{i-1} - Med)|   \\] <p>where \\(n\\) is the number of samples in the time series, \\(a_i\\) are its values, and \\(Med\\) represents its median. The formula for the mean-crossing rate can be found in [1].</p> References <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from timecave.data_characteristics import median_crossing_rate\n&gt;&gt;&gt; ts = np.array([0, 20, 0, 20, 0]);\n&gt;&gt;&gt; median_crossing_rate(ts)\n1.0\n&gt;&gt;&gt; ts2 = np.ones(10);\n&gt;&gt;&gt; median_crossing_rate(ts2)\n0.0\n&gt;&gt;&gt; ts3 = np.array([50, 50, 50, 0, 0]);\n&gt;&gt;&gt; median_crossing_rate(ts3)\n0.25\n&gt;&gt;&gt; ts4 = np.array([0, 20, 5, 5, 5]);\n&gt;&gt;&gt; median_crossing_rate(ts4)\n0.5\n</code></pre> <p>If the time series is neither an array nor a series, an exception is thrown:</p> <pre><code>&gt;&gt;&gt; median_crossing_rate([0, 1, 2])\nTraceback (most recent call last):\n...\nTypeError: Time series must be either a Numpy array or a Pandas series.\n</code></pre> Source code in <code>timecave/data_characteristics.py</code> <pre><code>def median_crossing_rate(ts: np.ndarray | pd.Series) -&gt; float:\n\n    \"\"\"\n    Compute the series' median-crossing rate.\n\n    This function computes the median-crossing rate of a given time series.\n    The median-crossing rate is defined as the rate at which the values of a time\n    series change from being below its median value to above said value.\n    In practice, the median is subtracted from the time series, and the zero-crossing\n    rate is then computed.\n\n    Parameters\n    ----------\n    ts : np.ndarray | pd.Series\n        Univariate time series.\n\n    Returns\n    -------\n    float\n        Median-crossing rate.\n\n    Raises\n    ------\n    TypeError\n        If `ts` is neither a Numpy array nor a Pandas series.\n\n    See also\n    --------\n    [mean_crossing_rate](mean_cr.md)\n        Uses the mean instead of the median.\n\n    Notes\n    -----\n    The median-crossing rate is similar to the mean-crossing rate, but it uses the median as a reference value. \\\n    It can be computed from the following formula:\n\n    $$\n    MedCR = \\\\frac{1}{n-1} \\sum_{i=2}^{n} |sign(a_i - Med) - sign(a_{i-1} - Med)|  \n    $$\n\n    where $n$ is the number of samples in the time series, $a_i$ are its values, and $Med$ represents its median.\n    The formula for the mean-crossing rate can be found in [[1]](#1).\n\n    References\n    ----------\n    ##1\n    Bohdan Myroniv, Cheng-Wei Wu, Yi Ren, Albert Christian, Ensa Bajo,\n    and Yu-chee Tseng. Analyzing user emotions via physiology signals. Data\n    Science and Pattern Recognition, 2, 12 2017.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; from timecave.data_characteristics import median_crossing_rate\n    &gt;&gt;&gt; ts = np.array([0, 20, 0, 20, 0]);\n    &gt;&gt;&gt; median_crossing_rate(ts)\n    1.0\n    &gt;&gt;&gt; ts2 = np.ones(10);\n    &gt;&gt;&gt; median_crossing_rate(ts2)\n    0.0\n    &gt;&gt;&gt; ts3 = np.array([50, 50, 50, 0, 0]);\n    &gt;&gt;&gt; median_crossing_rate(ts3)\n    0.25\n    &gt;&gt;&gt; ts4 = np.array([0, 20, 5, 5, 5]);\n    &gt;&gt;&gt; median_crossing_rate(ts4)\n    0.5\n\n    If the time series is neither an array nor a series, an exception is thrown:\n\n    &gt;&gt;&gt; median_crossing_rate([0, 1, 2])\n    Traceback (most recent call last):\n    ...\n    TypeError: Time series must be either a Numpy array or a Pandas series.\n    \"\"\"\n\n    _check_type(ts);\n\n    if(isinstance(ts, pd.Series) is True):\n\n        ts = ts.to_numpy();\n\n    new_ts = ts - np.median(ts);\n\n    mcr = np.nonzero(np.diff(np.sign(new_ts)))[0].shape[0] / (ts.shape[0] - 1);\n\n    return mcr;\n</code></pre>"},{"location":"API_ref/data_characteristics/med_cr/#timecave.data_characteristics.median_crossing_rate--1","title":"1","text":"<p>Bohdan Myroniv, Cheng-Wei Wu, Yi Ren, Albert Christian, Ensa Bajo, and Yu-chee Tseng. Analyzing user emotions via physiology signals. Data Science and Pattern Recognition, 2, 12 2017.</p>"},{"location":"API_ref/data_characteristics/sot/","title":"strength_of_trend","text":""},{"location":"API_ref/data_characteristics/sot/#timecave.data_characteristics.strength_of_trend","title":"<code>timecave.data_characteristics.strength_of_trend(ts)</code>","text":"<p>Compute the strength of trend of a time series.</p> <p>This function computes the strength of trend of a given time series using the method employed by Cerqueira et. al (2020) (i.e. the ratio between the time series' standard deviation and that of the differenced time series).</p> <p>Parameters:</p> Name Type Description Default <code>ts</code> <code>ndarray | Series</code> <p>Univariate time series.</p> required <p>Returns:</p> Type Description <code>float</code> <p>Strength of trend.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If <code>ts</code> is neither a Numpy array nor a Pandas series.</p> Notes <p>Let \\(\\sigma\\) be the standard deviation of a given time series. The strength of trend of a series is defined     by Cerqueira et al [1] as:</p> \\[ SOT = \\frac{\\sigma_{ts}}{\\sigma_{diff}} \\] <p>where \\(ts\\) stands for the time series itself and \\(diff\\) denotes the differenced time series.</p> References <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from timecave.data_characteristics import strength_of_trend\n&gt;&gt;&gt; rng = np.random.default_rng(seed=1);\n&gt;&gt;&gt; noise = rng.uniform(low=0, high=0.01, size=10);\n&gt;&gt;&gt; constant_series = np.ones(10);\n&gt;&gt;&gt; strength_of_trend(constant_series + noise)\n0.5717034302917938\n</code></pre> <p>For a series with a strong trend, this value will be larger:</p> <pre><code>&gt;&gt;&gt; series_trend = np.arange(0, 10);\n&gt;&gt;&gt; strength_of_trend(series_trend + noise)\n543.4144869043147\n</code></pre> <p>For pure trends, the strength of trend is infinite:</p> <pre><code>&gt;&gt;&gt; strength_of_trend(series_trend)\ninf\n</code></pre> <p>If the time series is neither an array nor a series, an exception is thrown:</p> <pre><code>&gt;&gt;&gt; strength_of_trend([0, 1, 2])\nTraceback (most recent call last):\n...\nTypeError: Time series must be either a Numpy array or a Pandas series.\n</code></pre> Source code in <code>timecave/data_characteristics.py</code> <pre><code>def strength_of_trend(ts: np.ndarray | pd.Series) -&gt; float:\n\n    \"\"\"\n    Compute the strength of trend of a time series.\n\n    This function computes the strength of trend of a given time series using the method\n    employed by Cerqueira et. al (2020) (i.e. the ratio between the time series' standard deviation and\n    that of the differenced time series).\n\n    Parameters\n    ----------\n    ts : np.ndarray | pd.Series\n        Univariate time series.\n\n    Returns\n    -------\n    float\n        Strength of trend.\n\n    Raises\n    ------\n    TypeError\n        If `ts` is neither a Numpy array nor a Pandas series.\n\n    Notes\n    -----\n    Let $\\sigma$ be the standard deviation of a given time series. The strength of trend of a series is defined \\\n    by Cerqueira et al [[1]](#1) as:\n\n    $$\n    SOT = \\\\frac{\\sigma_{ts}}{\\sigma_{diff}}\n    $$\n\n    where $ts$ stands for the time series itself and $diff$ denotes the differenced time series.\n\n    References\n    ----------\n    ##1\n    Cerqueira, V., Torgo, L., Mozeti\u02c7c, I., 2020. Evaluating time series forecasting\n    models: An empirical study on performance estimation methods.\n    Machine Learning 109, 1997\u20132028.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; from timecave.data_characteristics import strength_of_trend\n    &gt;&gt;&gt; rng = np.random.default_rng(seed=1);\n    &gt;&gt;&gt; noise = rng.uniform(low=0, high=0.01, size=10);\n    &gt;&gt;&gt; constant_series = np.ones(10);\n    &gt;&gt;&gt; strength_of_trend(constant_series + noise)\n    0.5717034302917938\n\n    For a series with a strong trend, this value will be larger:\n\n    &gt;&gt;&gt; series_trend = np.arange(0, 10);\n    &gt;&gt;&gt; strength_of_trend(series_trend + noise)\n    543.4144869043147\n\n    For pure trends, the strength of trend is infinite:\n\n    &gt;&gt;&gt; strength_of_trend(series_trend)\n    inf\n\n    If the time series is neither an array nor a series, an exception is thrown:\n\n    &gt;&gt;&gt; strength_of_trend([0, 1, 2])\n    Traceback (most recent call last):\n    ...\n    TypeError: Time series must be either a Numpy array or a Pandas series.\n    \"\"\"\n\n    _check_type(ts);\n\n    if(isinstance(ts, np.ndarray) is True):\n\n        diff_ts = np.diff(ts);\n\n    else:\n\n        diff_ts = ts.diff().dropna();\n\n    ts_std = ts.std();\n    diff_std = diff_ts.std();\n\n    if(diff_std == 0):\n\n        SOT = np.inf;\n\n    else:\n\n        SOT = ts_std / diff_std;\n\n    return SOT;\n</code></pre>"},{"location":"API_ref/data_characteristics/sot/#timecave.data_characteristics.strength_of_trend--1","title":"1","text":"<p>Cerqueira, V., Torgo, L., Mozeti\u02c7c, I., 2020. Evaluating time series forecasting models: An empirical study on performance estimation methods. Machine Learning 109, 1997\u20132028.</p>"},{"location":"API_ref/data_generation/","title":"Data generation","text":"<p>This subpackage contains all the routines that allow the user to generate synthetic time series data.</p> <p>In most cases, using the time series generator class will suffice. In order to do so, type:</p> <pre><code>from timecave.data_generation.time_series_generation import TimeSeriesGenerator\n</code></pre>"},{"location":"API_ref/data_generation/#modules","title":"Modules:","text":"<ul> <li>Frequency Modulation: Contains classes used to generate time-varying sinusoids.</li> <li>Time series functions: Implements the functions that can be used to generate data.</li> <li>Time series generation: Contains the generator class that can be used to generate synthetic time series data easily.</li> </ul>"},{"location":"API_ref/data_generation/frequency_modulation/","title":"Frequency modulation","text":""},{"location":"API_ref/data_generation/frequency_modulation/#timecave.data_generation.frequency_modulation","title":"<code>timecave.data_generation.frequency_modulation</code>","text":"<p>This module contains helper functions that can be used to generate time series with time-varying frequency characteristics.</p> <p>Classes:</p> Name Description <code>BaseFrequency</code> <p>Base class for frequency modulation.</p> <code>FrequencyModulationWithStep</code> <p>Frequency modulation where the dominant frequency changes abruptly.</p> <code>FrequencyModulationLinear</code> <p>Frequency modulation where the dominant frequency changes linearly.</p>"},{"location":"API_ref/data_generation/frequency_modulation/base_freq/","title":"Base Frequency class","text":""},{"location":"API_ref/data_generation/frequency_modulation/base_freq/#timecave.data_generation.frequency_modulation.BaseFrequency","title":"<code>timecave.data_generation.frequency_modulation.BaseFrequency()</code>","text":"<p>Base class for frequency modulation.</p> <p>This class provides a base for implementing frequency modulation techniques.</p> <p>Methods:</p> Name Description <code>modulate</code> <p>Adjusts the frequency based on the given time.</p> Source code in <code>timecave/data_generation/frequency_modulation.py</code> <pre><code>def __init__(self) -&gt; None:\n    pass\n</code></pre>"},{"location":"API_ref/data_generation/frequency_modulation/base_freq/#timecave.data_generation.frequency_modulation.BaseFrequency.modulate","title":"<code>modulate(time)</code>  <code>abstractmethod</code>","text":"<p>Adjusts the frequency based on the given time.</p> <p>This method should be implemented by subclasses to perform frequency modulation.</p> <p>Parameters:</p> Name Type Description Default <code>time</code> <code>int</code> <p>Number of timesteps for which modulation is meant to be performed.</p> required <p>Returns:</p> Type Description <code>array</code> <p>Each entry of the array corresponds to the frequency at the given timestep.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>This method must be implemented by subclasses.</p> Source code in <code>timecave/data_generation/frequency_modulation.py</code> <pre><code>@abstractmethod\ndef modulate(self, time: int) -&gt; np.array:\n    \"\"\"\n    Adjusts the frequency based on the given time.\n\n    This method should be implemented by subclasses to perform frequency modulation.\n\n    Parameters\n    ----------\n    time : int\n        Number of timesteps for which modulation is meant to be performed.\n\n    Returns\n    -------\n    np.array\n        Each entry of the array corresponds to the frequency at the given timestep.\n\n    Raises\n    ------\n    NotImplementedError\n        This method must be implemented by subclasses.\n    \"\"\"\n    raise NotImplementedError(\"Subclasses must implement modulate method!\")\n</code></pre>"},{"location":"API_ref/data_generation/frequency_modulation/linear_freq/","title":"Linear Frequency modulation","text":""},{"location":"API_ref/data_generation/frequency_modulation/linear_freq/#timecave.data_generation.frequency_modulation.FrequencyModulationLinear","title":"<code>timecave.data_generation.frequency_modulation.FrequencyModulationLinear(freq_init, slope)</code>","text":"<p>               Bases: <code>BaseFrequency</code></p> <p>Represents a linear frequency modulation.</p> <p>Parameters:</p> Name Type Description Default <code>freq_init</code> <code>float or int</code> <p>The initial frequency value.</p> required <code>slope</code> <code>float</code> <p>Slope of the frequency modulation over time.</p> required <p>Methods:</p> Name Description <code>modulate</code> <p>Adjusts the frequency based on the given time.</p> Source code in <code>timecave/data_generation/frequency_modulation.py</code> <pre><code>def __init__(self, freq_init: int or float, slope: int or float):\n    self._check_freq_init(freq_init)\n    self._check_slope(slope)\n    super().__init__()\n    self._slope = slope\n    self._freq_init = freq_init\n</code></pre>"},{"location":"API_ref/data_generation/frequency_modulation/linear_freq/#timecave.data_generation.frequency_modulation.FrequencyModulationLinear.modulate","title":"<code>modulate(time)</code>","text":"<p>Adjusts the frequency based on the given time.</p> <p>This method should be implemented by subclasses to perform frequency modulation.</p> <p>Parameters:</p> Name Type Description Default <code>time</code> <code>int</code> <p>Number of timesteps for which modulation is meant to be performed.</p> required <p>Returns:</p> Type Description <code>float or int</code> <p>The modulated frequency value at the given time instant.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from timecave.data_generation.frequency_modulation import FrequencyModulationLinear\n&gt;&gt;&gt; mod = FrequencyModulationLinear(1, 10);\n&gt;&gt;&gt; mod.modulate(5)\n51\n&gt;&gt;&gt; mod.modulate(10)\n101\n</code></pre> Source code in <code>timecave/data_generation/frequency_modulation.py</code> <pre><code>def modulate(self, time: int):\n    \"\"\"\n    Adjusts the frequency based on the given time.\n\n    This method should be implemented by subclasses to perform frequency modulation.\n\n    Parameters\n    ----------\n    time : int\n        Number of timesteps for which modulation is meant to be performed.\n\n    Returns\n    -------\n    float or int\n        The modulated frequency value at the given time instant.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from timecave.data_generation.frequency_modulation import FrequencyModulationLinear\n    &gt;&gt;&gt; mod = FrequencyModulationLinear(1, 10);\n    &gt;&gt;&gt; mod.modulate(5)\n    51\n    &gt;&gt;&gt; mod.modulate(10)\n    101\n    \"\"\"\n    return self._freq_init + self._slope * time\n</code></pre>"},{"location":"API_ref/data_generation/frequency_modulation/step_freq/","title":"Step Frequency modulation","text":""},{"location":"API_ref/data_generation/frequency_modulation/step_freq/#timecave.data_generation.frequency_modulation.FrequencyModulationWithStep","title":"<code>timecave.data_generation.frequency_modulation.FrequencyModulationWithStep(freq_init, t_split)</code>","text":"<p>               Bases: <code>BaseFrequency</code></p> <p>Frequency modulation with step.</p> <p>This class implements frequency modulation with a step change.</p> <p>Parameters:</p> Name Type Description Default <code>freq_init</code> <code>float or int</code> <p>The initial frequency value.</p> required <code>t_split</code> <code>int</code> <p>The timestep at which the frequency changes.</p> required <p>Methods:</p> Name Description <code>modulate</code> <p>Adjusts the frequency based on the given time.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If 'freq_init' is not a float or int. If 't_split' is not an int.</p> <code>ValueError</code> <p>If 'freq_init' is not greater than zero. If 't_split' is not greater than zero.</p> Source code in <code>timecave/data_generation/frequency_modulation.py</code> <pre><code>def __init__(self, freq_init: float or int, t_split: float or int):\n    self._check_freq_init(freq_init)\n    self._check_t_split(t_split)\n\n    super().__init__()\n    self._freq_init = freq_init\n    self._t_split = t_split\n</code></pre>"},{"location":"API_ref/data_generation/frequency_modulation/step_freq/#timecave.data_generation.frequency_modulation.FrequencyModulationWithStep.modulate","title":"<code>modulate(time)</code>","text":"<p>Adjusts the frequency based on the given time.</p> <p>This method calculates the frequency modulation based on the given time and the initial frequency value.</p> <p>Parameters:</p> Name Type Description Default <code>time</code> <code>int</code> <p>Number of timesteps for which modulation is meant to be performed.</p> required <p>Returns:</p> Type Description <code>array</code> <p>Each entry of the array corresponds to the frequency at a given timestep.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from timecave.data_generation.frequency_modulation import FrequencyModulationWithStep\n&gt;&gt;&gt; mod = FrequencyModulationWithStep(1, 50);\n&gt;&gt;&gt; mod.modulate(25)\narray(1)\n&gt;&gt;&gt; mod.modulate(100)\narray(2)\n&gt;&gt;&gt; mod.modulate(50)\narray(1)\n</code></pre> Source code in <code>timecave/data_generation/frequency_modulation.py</code> <pre><code>def modulate(self, time: int) -&gt; np.array:\n    \"\"\"\n    Adjusts the frequency based on the given time.\n\n    This method calculates the frequency modulation based on the given time\n    and the initial frequency value.\n\n    Parameters\n    ----------\n    time : int\n        Number of timesteps for which modulation is meant to be performed.\n\n    Returns\n    ----------\n    np.array\n        Each entry of the array corresponds to the frequency at a given timestep.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from timecave.data_generation.frequency_modulation import FrequencyModulationWithStep\n    &gt;&gt;&gt; mod = FrequencyModulationWithStep(1, 50);\n    &gt;&gt;&gt; mod.modulate(25)\n    array(1)\n    &gt;&gt;&gt; mod.modulate(100)\n    array(2)\n    &gt;&gt;&gt; mod.modulate(50)\n    array(1)\n    \"\"\"\n    initial_period = 1 / self._freq_init\n    t_split_adjusted = (self._t_split // initial_period) * initial_period\n    return np.where(time &gt; t_split_adjusted, self._freq_init * 2, self._freq_init)\n</code></pre>"},{"location":"API_ref/data_generation/time_series_functions/","title":"Time Series Functions","text":""},{"location":"API_ref/data_generation/time_series_functions/#timecave.data_generation.time_series_functions","title":"<code>timecave.data_generation.time_series_functions</code>","text":"<p>This module contains functions to generate various types of time series data.</p> <p>This module provides functions to generate different types of time series data, including sinusoidal signals, indicator functions, linear patterns, exponential decays, and time series based on autoregressive moving average (ARMA) models and nonlinear autoregressive (AR) models.</p> <p>Functions:</p> Name Description <code>sinusoid_ts</code> <p>Generate a time series of a sinusoidal signal.</p> <code>frequency_varying_sinusoid_ts</code> <p>Generate a time series of a sinusoidal signal with varying frequency.</p> <code>indicator_ts</code> <p>Generate time series array based on a binary indicator function with specified start and end indices.</p> <code>scaled_right_indicator_ts</code> <p>Generate a time series array based on a indicator function that is 1 in the interval [idx, + inf[ and 0 otherwise.</p> <code>scaled_unit_impulse_function_ts</code> <p>Generate time series array based on a scaled unit impulse function with specified index.</p> <code>linear_ts</code> <p>Generate a linear time series array.</p> <code>exponential_ts</code> <p>Generates a time series based on a exponential function.</p> <code>arma_ts</code> <p>Generate a time series array based on an Autoregressive Moving Average (ARMA) model.</p> <code>nonlinear_ar_ts</code> <p>Generate a time series array based on a nonlinear autoregressive (AR) model.</p>"},{"location":"API_ref/data_generation/time_series_functions/arma/","title":"ARMA","text":""},{"location":"API_ref/data_generation/time_series_functions/arma/#timecave.data_generation.time_series_functions.arma_ts","title":"<code>timecave.data_generation.time_series_functions.arma_ts(number_samples, lags, max_root, ar=True, ma=True, **kwargs)</code>","text":"<p>Generate a time series array based on an Autoregressive Moving Average (ARMA) model.</p> <p>This function creates a time series array of given length based on an ARMA model with specified parameters such as number of lags and maximum root. It generates samples using an ARMA process.</p> <p>Parameters:</p> Name Type Description Default <code>number_samples</code> <code>int</code> <p>The total number of samples in the time series array.</p> required <code>lags</code> <code>int</code> <p>The number of lags to consider in the ARMA model.</p> required <code>max_root</code> <code>float</code> <p>The maximum root for the ARMA model. This value has to be larger than 1.1.</p> required <code>ar</code> <code>bool</code> <p>Whether to include autoregressive (AR) component in the ARMA model.</p> <code>True</code> <code>ma</code> <code>bool</code> <p>Whether to include moving average (MA) component in the ARMA model.</p> <code>True</code> <code>**kwargs</code> <code>dict</code> <p>Additional keyword arguments to pass to the ARMA process generator.  See ARMAProcess for more details.</p> <code>{}</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>A time series array generated based on the specified ARMA model parameters.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the maximum root is not larger than 1.1.</p> See also <p>nonlinear_ar_ts: Generate data from a nonlinear autoregressive process.</p> Notes <p>This method of generating synthetic time series data was first proposed by Bergmeir and Benitez (2012).  Please refer to [1] for more details on this method.</p> References <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; import matplotlib.pyplot as plt\n&gt;&gt;&gt; from timecave.data_generation.time_series_functions import arma_ts\n&gt;&gt;&gt; ts = arma_ts(1000, 5, 2);\n&gt;&gt;&gt; _ = plt.plot(np.arange(0, ts.shape[0]), ts);\n&gt;&gt;&gt; plt.show();\n</code></pre> <p></p> <p>Pure autoregressive processes can also be generated:</p> <pre><code>&gt;&gt;&gt; ts2 = arma_ts(1000, 5, 2, ma=False);\n&gt;&gt;&gt; _ = plt.plot(np.arange(0, ts2.shape[0]), ts2);\n&gt;&gt;&gt; plt.show();\n</code></pre> <p></p> <p>And so can pure moving average processes:</p> <pre><code>&gt;&gt;&gt; ts3 = arma_ts(1000, 5, 2, ar=False);\n&gt;&gt;&gt; _ = plt.plot(np.arange(0, ts3.shape[0]), ts3);\n&gt;&gt;&gt; plt.show();\n</code></pre> <p></p> Source code in <code>timecave/data_generation/time_series_functions.py</code> <pre><code>def arma_ts(\n    number_samples: int,\n    lags: int,\n    max_root: float,\n    ar: bool = True,\n    ma: bool = True,\n    **kwargs,\n):\n    \"\"\"\n    Generate a time series array based on an Autoregressive Moving Average (ARMA) model.\n\n    This function creates a time series array of given length based on an ARMA model\n    with specified parameters such as number of lags and maximum root. It generates\n    samples using an ARMA process.\n\n    Parameters\n    ----------\n    number_samples : int\n        The total number of samples in the time series array.\n\n    lags : int\n        The number of lags to consider in the ARMA model.\n\n    max_root : float\n        The maximum root for the ARMA model. This value has to be larger than 1.1.\n\n    ar : bool, default=True\n        Whether to include autoregressive (AR) component in the ARMA model.\n\n    ma : bool, default=True\n        Whether to include moving average (MA) component in the ARMA model.\n\n    **kwargs : dict\n        Additional keyword arguments to pass to the ARMA process generator. \n        See [ARMAProcess](https://www.statsmodels.org/stable/generated/statsmodels.tsa.arima_process.ArmaProcess.html) for more details.\n\n    Returns\n    -------\n    np.ndarray\n        A time series array generated based on the specified ARMA model parameters.\n\n    Raises\n    -------\n    ValueError\n        If the maximum root is not larger than 1.1.\n\n    See also\n    --------\n    [nonlinear_ar_ts](nonlinear_ar.md): Generate data from a nonlinear autoregressive process.\n\n    Notes\n    -----\n    This method of generating synthetic time series data was first proposed by Bergmeir and Benitez (2012). \n    Please refer to [[1]](#1) for more details on this method.\n\n    References\n    ----------\n    ##1\n    Christoph Bergmeir and Jos\u00e9 M Ben\u00edtez. On the use of cross-validation for\n    time series predictor evaluation. Information Sciences, 191:192\u2013213, 2012.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; import matplotlib.pyplot as plt\n    &gt;&gt;&gt; from timecave.data_generation.time_series_functions import arma_ts\n    &gt;&gt;&gt; ts = arma_ts(1000, 5, 2);\n    &gt;&gt;&gt; _ = plt.plot(np.arange(0, ts.shape[0]), ts);\n    &gt;&gt;&gt; plt.show();\n\n    ![arma](../../../images/ARMA.png)\n\n    Pure autoregressive processes can also be generated:\n\n    &gt;&gt;&gt; ts2 = arma_ts(1000, 5, 2, ma=False);\n    &gt;&gt;&gt; _ = plt.plot(np.arange(0, ts2.shape[0]), ts2);\n    &gt;&gt;&gt; plt.show();\n\n    ![ar](../../../images/AR.png)\n\n    And so can pure moving average processes:\n\n    &gt;&gt;&gt; ts3 = arma_ts(1000, 5, 2, ar=False);\n    &gt;&gt;&gt; _ = plt.plot(np.arange(0, ts3.shape[0]), ts3);\n    &gt;&gt;&gt; plt.show();\n\n    ![ma](../../../images/MA.png)\n    \"\"\"\n    _check_number_samples(number_samples)\n\n    if ar == False and ma == False:\n        raise ValueError(\"At least one of 'ar' or 'ma' must be set to True.\")\n\n    params_ar = _get_arma_parameters(lags, max_root)\n    params_ma = _get_arma_parameters(lags, max_root)\n    ar_coeff = np.r_[1, -params_ar]\n    ma_coeff = np.r_[1, params_ma]\n\n    if ar and not ma:\n        ts = ArmaProcess(ma=ar_coeff).generate_sample(nsample=number_samples, **kwargs)\n    elif not ar and ma:\n        ts = ArmaProcess(ar=[1], ma=ma_coeff).generate_sample(\n            nsample=number_samples, **kwargs\n        )\n    elif ar and ma:\n        ts = ArmaProcess(ar=ar_coeff, ma=ma_coeff).generate_sample(\n            nsample=number_samples, **kwargs\n        )\n    return ts\n</code></pre>"},{"location":"API_ref/data_generation/time_series_functions/arma/#timecave.data_generation.time_series_functions.arma_ts--1","title":"1","text":"<p>Christoph Bergmeir and Jos\u00e9 M Ben\u00edtez. On the use of cross-validation for time series predictor evaluation. Information Sciences, 191:192\u2013213, 2012.</p>"},{"location":"API_ref/data_generation/time_series_functions/exponential/","title":"Exponential","text":""},{"location":"API_ref/data_generation/time_series_functions/exponential/#timecave.data_generation.time_series_functions.exponential_ts","title":"<code>timecave.data_generation.time_series_functions.exponential_ts(number_samples, max_interval_size, decay_rate=1, initial_value=1)</code>","text":"<p>Generates a time series based on a exponential function.</p> <p>This function creates a time series array of given length where the values decay exponentially over time based on the specified decay rate and initial value.</p> <p>Parameters:</p> Name Type Description Default <code>number_samples</code> <code>int</code> <p>The total number of samples in the time series array.</p> required <code>max_interval_size</code> <code>float</code> <p>The maximum interval size for generating the time series array.</p> required <code>decay_rate</code> <code>float</code> <p>The rate at which the values decay over time.</p> <code>1</code> <code>initial_value</code> <code>float</code> <p>The initial value of the time series array.</p> <code>1</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>An exponential decay time series array where values decay exponentially over time based on the specified decay rate and initial value.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; import matplotlib.pyplot as plt\n&gt;&gt;&gt; from timecave.data_generation.time_series_functions import exponential_ts\n&gt;&gt;&gt; ts = exponential_ts(1000, 10, initial_value=10);\n&gt;&gt;&gt; _ = plt.plot(np.arange(0, ts.shape[0]), ts);\n&gt;&gt;&gt; plt.show();\n</code></pre> <p></p> Source code in <code>timecave/data_generation/time_series_functions.py</code> <pre><code>def exponential_ts(\n    number_samples: int,\n    max_interval_size: float,\n    decay_rate: float = 1,\n    initial_value: float = 1,\n) -&gt; np.ndarray:\n    \"\"\"\n    Generates a time series based on a exponential function.\n\n    This function creates a time series array of given length where the values\n    decay exponentially over time based on the specified decay rate and initial value.\n\n    Parameters\n    ----------\n    number_samples : int\n        The total number of samples in the time series array.\n\n    max_interval_size : float\n        The maximum interval size for generating the time series array.\n\n    decay_rate : float, default=1\n        The rate at which the values decay over time.\n\n    initial_value : float, default=1\n        The initial value of the time series array.\n\n    Returns\n    -------\n    np.ndarray\n        An exponential decay time series array where values decay exponentially over time based on the specified decay rate and initial value.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; import matplotlib.pyplot as plt\n    &gt;&gt;&gt; from timecave.data_generation.time_series_functions import exponential_ts\n    &gt;&gt;&gt; ts = exponential_ts(1000, 10, initial_value=10);\n    &gt;&gt;&gt; _ = plt.plot(np.arange(0, ts.shape[0]), ts);\n    &gt;&gt;&gt; plt.show();\n\n    ![exponential](../../../images/Exponential.png)\n    \"\"\"\n    _check_number_samples(number_samples)\n    time = np.linspace(0, max_interval_size, number_samples)\n    exponential_series = initial_value * np.exp(-decay_rate * time)\n    return exponential_series\n</code></pre>"},{"location":"API_ref/data_generation/time_series_functions/impulse/","title":"Impulse","text":""},{"location":"API_ref/data_generation/time_series_functions/impulse/#timecave.data_generation.time_series_functions.scaled_unit_impulse_function_ts","title":"<code>timecave.data_generation.time_series_functions.scaled_unit_impulse_function_ts(number_samples, idx, constant=1)</code>","text":"<p>Generate time series array based on a scaled unit impulse function with specified index.</p> <p>This function creates a binary indicator time series array of given length where only the sample at the specified index is marked as 1 and the rest as 0. The binary array is then scaled by a constant factor.</p> <p>Parameters:</p> Name Type Description Default <code>number_samples</code> <code>int</code> <p>The total number of samples in the time series array.</p> required <code>idx</code> <code>int</code> <p>The index at which the impulse occurs, marked as 1.</p> required <code>constant</code> <code>float</code> <p>A scaling constant to multiply the array by.</p> <code>1</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>A scaled time series array where only the sample at the specified index is marked as 1 and the rest as 0.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; import matplotlib.pyplot as plt\n&gt;&gt;&gt; from timecave.data_generation.time_series_functions import scaled_unit_impulse_function_ts\n&gt;&gt;&gt; ts = scaled_unit_impulse_function_ts(1000, 250, 20);\n&gt;&gt;&gt; _ = plt.plot(np.arange(0, ts.shape[0]), ts);\n&gt;&gt;&gt; plt.show();\n</code></pre> <p></p> Source code in <code>timecave/data_generation/time_series_functions.py</code> <pre><code>def scaled_unit_impulse_function_ts(\n    number_samples: int, idx: int, constant: float = 1\n) -&gt; np.ndarray:\n    \"\"\"\n    Generate time series array based on a scaled unit impulse function with specified index.\n\n    This function creates a binary indicator time series array of given length where\n    only the sample at the specified index is marked as 1 and the rest as 0.\n    The binary array is then scaled by a constant factor.\n\n    Parameters\n    ----------\n    number_samples : int\n        The total number of samples in the time series array.\n\n    idx : int\n        The index at which the impulse occurs, marked as 1.\n\n    constant : float, default=1\n        A scaling constant to multiply the array by.\n\n    Returns\n    -------\n    np.ndarray\n        A scaled time series array where only the sample at the specified index is marked as 1 and the rest as 0.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; import matplotlib.pyplot as plt\n    &gt;&gt;&gt; from timecave.data_generation.time_series_functions import scaled_unit_impulse_function_ts\n    &gt;&gt;&gt; ts = scaled_unit_impulse_function_ts(1000, 250, 20);\n    &gt;&gt;&gt; _ = plt.plot(np.arange(0, ts.shape[0]), ts);\n    &gt;&gt;&gt; plt.show();\n\n    ![impulse](../../../images/Impulse.png)\n    \"\"\"\n    _check_number_samples(number_samples)\n    return constant * indicator_ts(number_samples, idx, idx)\n</code></pre>"},{"location":"API_ref/data_generation/time_series_functions/indicator/","title":"Indicator function","text":""},{"location":"API_ref/data_generation/time_series_functions/indicator/#timecave.data_generation.time_series_functions.indicator_ts","title":"<code>timecave.data_generation.time_series_functions.indicator_ts(number_samples, start_index, end_index)</code>","text":"<p>Generate time series array based on a binary indicator function with specified start and end indices.</p> <p>This function creates a time series array based on a binary indicator function of given length. The specified segment is marked as 1 and the rest as 0.</p> <p>Parameters:</p> Name Type Description Default <code>number_samples</code> <code>int</code> <p>The total number of samples in the time series array.</p> required <code>start_index</code> <code>int</code> <p>The start index of the segment to be marked as 1.</p> required <code>end_index</code> <code>int</code> <p>The end index of the segment to be marked as 1.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>A time series array where the specified segment is marked as 1 and the rest as 0.</p> See also <p>scaled_right_indicator_ts: Scaled right indicator time series. Needs only a start index.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; import matplotlib.pyplot as plt\n&gt;&gt;&gt; from timecave.data_generation.time_series_functions import indicator_ts\n&gt;&gt;&gt; ts = indicator_ts(100, 20, 60);\n&gt;&gt;&gt; _ = plt.plot(np.arange(0, ts.shape[0]), ts);\n&gt;&gt;&gt; plt.show();\n</code></pre> <p></p> Source code in <code>timecave/data_generation/time_series_functions.py</code> <pre><code>def indicator_ts(number_samples: int, start_index: int, end_index: int) -&gt; np.ndarray:\n    \"\"\"\n    Generate time series array based on a binary indicator function with specified start and end indices.\n\n    This function creates a time series array based on a binary indicator function of given length. The specified segment is marked as 1 and the rest as 0.\n\n    Parameters\n    ----------\n    number_samples : int\n        The total number of samples in the time series array.\n\n    start_index : int\n        The start index of the segment to be marked as 1.\n\n    end_index : int\n        The end index of the segment to be marked as 1.\n\n    Returns\n    -------\n    np.ndarray\n        A time series array where the specified segment is marked as 1 and the rest as 0.\n\n    See also\n    --------\n    [scaled_right_indicator_ts](scaled_indicator.md): Scaled right indicator time series. Needs only a start index.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; import matplotlib.pyplot as plt\n    &gt;&gt;&gt; from timecave.data_generation.time_series_functions import indicator_ts\n    &gt;&gt;&gt; ts = indicator_ts(100, 20, 60);\n    &gt;&gt;&gt; _ = plt.plot(np.arange(0, ts.shape[0]), ts);\n    &gt;&gt;&gt; plt.show();\n\n    ![indicator](../../../images/Indicator.png)\n    \"\"\"\n    _check_number_samples(number_samples)\n    _check_index(start_index)\n    _check_index(end_index)\n    indicator = np.zeros(number_samples)\n    indicator[start_index : end_index + 1] = 1\n    return indicator\n</code></pre>"},{"location":"API_ref/data_generation/time_series_functions/linear/","title":"Linear","text":""},{"location":"API_ref/data_generation/time_series_functions/linear/#timecave.data_generation.time_series_functions.linear_ts","title":"<code>timecave.data_generation.time_series_functions.linear_ts(number_samples, max_interval_size, slope=1, intercept=0)</code>","text":"<p>Generate a linear time series array.</p> <p>This function creates a time series array of given length where the values follow a linear pattern determined by the slope and intercept parameters.</p> <p>Parameters:</p> Name Type Description Default <code>number_samples</code> <code>int</code> <p>The total number of samples in the time series array.</p> required <code>max_interval_size</code> <code>float</code> <p>The maximum interval size for generating the time series array.</p> required <code>slope</code> <code>float</code> <p>The slope of the linear pattern.</p> <code>1</code> <code>intercept</code> <code>float</code> <p>The intercept of the linear pattern.</p> <code>0</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>A time series array following a linear pattern determined by the slope and intercept parameters.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; import matplotlib.pyplot as plt\n&gt;&gt;&gt; from timecave.data_generation.time_series_functions import linear_ts\n&gt;&gt;&gt; ts = linear_ts(1000, 10);\n&gt;&gt;&gt; _ = plt.plot(np.arange(0, ts.shape[0]), ts);\n&gt;&gt;&gt; plt.show();\n</code></pre> <p></p> Source code in <code>timecave/data_generation/time_series_functions.py</code> <pre><code>def linear_ts(\n    number_samples: int,\n    max_interval_size: float,\n    slope: float = 1,\n    intercept: float = 0,\n) -&gt; np.ndarray:\n    \"\"\"\n    Generate a linear time series array.\n\n    This function creates a time series array of given length where the values\n    follow a linear pattern determined by the slope and intercept parameters.\n\n    Parameters\n    ----------\n    number_samples : int\n        The total number of samples in the time series array.\n\n    max_interval_size : float\n        The maximum interval size for generating the time series array.\n\n    slope : float, default=1\n        The slope of the linear pattern.\n\n    intercept : float, default=0\n        The intercept of the linear pattern.\n\n    Returns\n    -------\n    np.ndarray\n        A time series array following a linear pattern determined by the slope and intercept parameters.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; import matplotlib.pyplot as plt\n    &gt;&gt;&gt; from timecave.data_generation.time_series_functions import linear_ts\n    &gt;&gt;&gt; ts = linear_ts(1000, 10);\n    &gt;&gt;&gt; _ = plt.plot(np.arange(0, ts.shape[0]), ts);\n    &gt;&gt;&gt; plt.show();\n\n    ![linear](../../../images/Linear.png)\n    \"\"\"\n    _check_number_samples(number_samples)\n    time = np.linspace(0, max_interval_size, number_samples)\n    linear_series = slope * time + intercept\n    return linear_series\n</code></pre>"},{"location":"API_ref/data_generation/time_series_functions/nonlinear_ar/","title":"Nonlinear AR","text":""},{"location":"API_ref/data_generation/time_series_functions/nonlinear_ar/#timecave.data_generation.time_series_functions.nonlinear_ar_ts","title":"<code>timecave.data_generation.time_series_functions.nonlinear_ar_ts(number_samples, init_array, params, func_idxs)</code>","text":"<p>Generate a time series array based on a nonlinear autoregressive (AR) model.</p> <p>This function creates a time series array of a given length based on a nonlinear AR model with specified initial array, parameters, and function indices.</p> <p>Parameters:</p> Name Type Description Default <code>number_samples</code> <code>int</code> <p>The total number of samples in the time series array.</p> required <code>init_array</code> <code>ndarray</code> <p>The initial array for generating the time series. The lengths corresponds to the number of lags.</p> required <code>params</code> <code>list</code> <p>The parameters for the nonlinear AR model. The index representing the specific nonlinear transformation to apply:     0: Cosine function.     1: Sine function.     2: Hyperbolic tangent function.     3: Arctangent function.     4: Exponential decay function.</p> required <code>func_idxs</code> <code>list</code> <p>The indices of the nonlinear functions used in the model.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>A time series array generated based on the specified nonlinear AR model parameters.</p> Warnings <p>The lengths of <code>init_array</code>, <code>params</code> and <code>func_idxs</code> must match.</p> Notes <p>This method of generating synthetic time series data was first proposed by Bergmeir et al. (2018).  Please refer to [1] for more details on this method.</p> References <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; import matplotlib.pyplot as plt\n&gt;&gt;&gt; from timecave.data_generation.time_series_functions import nonlinear_ar_ts\n&gt;&gt;&gt; ts = nonlinear_ar_ts(1000, init_array=np.zeros(2), params=[0.5, -0.3], func_idxs=[0, 1]);\n&gt;&gt;&gt; _ = plt.plot(np.arange(0, ts.shape[0]), ts);\n&gt;&gt;&gt; plt.show();\n</code></pre> <p></p> <p>Functions other than sinusoids can be used as well:</p> <pre><code>&gt;&gt;&gt; ts2 = nonlinear_ar_ts(1000, init_array=np.zeros(4), params=[0.2, 0.6, -0.1, -0.4], func_idxs=[2, 3, 4, 3]);\n&gt;&gt;&gt; _ = plt.plot(np.arange(0, ts.shape[0]), ts2);\n&gt;&gt;&gt; plt.show();\n</code></pre> <p></p> Source code in <code>timecave/data_generation/time_series_functions.py</code> <pre><code>def nonlinear_ar_ts(\n    number_samples: int, init_array: np.array, params: list, func_idxs: list\n):\n    \"\"\"\n    Generate a time series array based on a nonlinear autoregressive (AR) model.\n\n    This function creates a time series array of a given length based on a nonlinear AR model\n    with specified initial array, parameters, and function indices.\n\n    Parameters\n    ----------\n    number_samples : int\n        The total number of samples in the time series array.\n\n    init_array : np.ndarray\n        The initial array for generating the time series. The lengths corresponds to the number of lags.\n\n    params : list\n        The parameters for the nonlinear AR model. The index representing the specific nonlinear transformation to apply:\n            0: Cosine function.\n            1: Sine function.\n            2: Hyperbolic tangent function.\n            3: Arctangent function.\n            4: Exponential decay function.\n\n    func_idxs : list\n        The indices of the nonlinear functions used in the model.\n\n    Returns\n    -------\n    np.ndarray\n        A time series array generated based on the specified nonlinear AR model parameters.\n\n    Warnings\n    --------\n    The lengths of `init_array`, `params` and `func_idxs` must match.\n\n    Notes\n    -----\n    This method of generating synthetic time series data was first proposed by Bergmeir et al. (2018). \n    Please refer to [[1]](#1) for more details on this method.\n\n    References\n    ----------\n    ##1\n    Christoph Bergmeir, Rob J Hyndman, and Bonsoo Koo. A note on the validity \n    of cross-validation for evaluating autoregressive time series prediction.\n    Computational Statistics &amp; Data Analysis, 120:70\u201383, 2018.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; import matplotlib.pyplot as plt\n    &gt;&gt;&gt; from timecave.data_generation.time_series_functions import nonlinear_ar_ts\n    &gt;&gt;&gt; ts = nonlinear_ar_ts(1000, init_array=np.zeros(2), params=[0.5, -0.3], func_idxs=[0, 1]);\n    &gt;&gt;&gt; _ = plt.plot(np.arange(0, ts.shape[0]), ts);\n    &gt;&gt;&gt; plt.show();\n\n    ![ARsin](../../../images/Nonlinear_AR_sin.png)\n\n    Functions other than sinusoids can be used as well:\n\n    &gt;&gt;&gt; ts2 = nonlinear_ar_ts(1000, init_array=np.zeros(4), params=[0.2, 0.6, -0.1, -0.4], func_idxs=[2, 3, 4, 3]);\n    &gt;&gt;&gt; _ = plt.plot(np.arange(0, ts.shape[0]), ts2);\n    &gt;&gt;&gt; plt.show();\n\n    ![ARother](../../../images/Nonlinear_AR_others.png)\n    \"\"\"\n    _check_number_samples(number_samples)\n    if len(params) != len(func_idxs):\n        raise ValueError(\"'params' and 'func_idxs' must have the same length\")\n\n    init_len = len(init_array)\n\n    x = np.empty(number_samples + init_len)\n    x[0:init_len] = init_array\n\n    for t in range(init_len, number_samples + init_len):\n        x[t] = np.random.normal(scale=0.5)\n\n        for j in range(1, init_len + 1):\n            x[t] += params[j - 1] * _nonlin_func(func_idxs[j - 1], x[t - j])\n\n    ts = x[init_len : (number_samples + init_len)]\n\n    return ts\n</code></pre>"},{"location":"API_ref/data_generation/time_series_functions/nonlinear_ar/#timecave.data_generation.time_series_functions.nonlinear_ar_ts--1","title":"1","text":"<p>Christoph Bergmeir, Rob J Hyndman, and Bonsoo Koo. A note on the validity  of cross-validation for evaluating autoregressive time series prediction. Computational Statistics &amp; Data Analysis, 120:70\u201383, 2018.</p>"},{"location":"API_ref/data_generation/time_series_functions/scaled_indicator/","title":"Scaled Right Indicator","text":""},{"location":"API_ref/data_generation/time_series_functions/scaled_indicator/#timecave.data_generation.time_series_functions.scaled_right_indicator_ts","title":"<code>timecave.data_generation.time_series_functions.scaled_right_indicator_ts(number_samples, idx, constant=1)</code>","text":"<p>Generate a time series array based on a indicator function that is 1 in the interval [idx, + inf[ and 0 otherwise.</p> <p>This function creates a time series array of given length where the segment starting from the specified index to the end is marked as 1 and the rest as 0. The binary array is then scaled by a constant factor.</p> <p>Parameters:</p> Name Type Description Default <code>number_samples</code> <code>int</code> <p>The total number of samples in the time series array.</p> required <code>idx</code> <code>int</code> <p>The index from which the segment starts to be marked as 1.</p> required <code>constant</code> <code>float</code> <p>A scaling constant to multiply the array by, by default 1.</p> <code>1</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>A scaled time series array where the segment starting from the specified index to the end is marked as 1 and the rest as 0.</p> See also <p>indicator_ts: Indicator time series.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; import matplotlib.pyplot as plt\n&gt;&gt;&gt; from timecave.data_generation.time_series_functions import scaled_right_indicator_ts\n&gt;&gt;&gt; ts = scaled_right_indicator_ts(100, 30, 5);\n&gt;&gt;&gt; _ = plt.plot(np.arange(0, ts.shape[0]), ts);\n&gt;&gt;&gt; plt.show();\n</code></pre> <p></p> Source code in <code>timecave/data_generation/time_series_functions.py</code> <pre><code>def scaled_right_indicator_ts(\n    number_samples: int, idx: int, constant: float = 1\n) -&gt; np.ndarray:\n    \"\"\"\n    Generate a time series array based on a indicator function that is 1 in the interval [idx, + inf[ and 0 otherwise.\n\n    This function creates a time series array of given length where the segment starting from the specified index to the end is marked as 1 and the rest as 0.\n    The binary array is then scaled by a constant factor.\n\n    Parameters\n    ----------\n    number_samples : int\n        The total number of samples in the time series array.\n\n    idx : int\n        The index from which the segment starts to be marked as 1.\n\n    constant : float, optional\n        A scaling constant to multiply the array by, by default 1.\n\n    Returns\n    -------\n    np.ndarray\n        A scaled time series array where the segment starting from the specified index to the end is marked as 1 and the rest as 0.\n\n    See also\n    --------\n    [indicator_ts](indicator.md): Indicator time series.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; import matplotlib.pyplot as plt\n    &gt;&gt;&gt; from timecave.data_generation.time_series_functions import scaled_right_indicator_ts\n    &gt;&gt;&gt; ts = scaled_right_indicator_ts(100, 30, 5);\n    &gt;&gt;&gt; _ = plt.plot(np.arange(0, ts.shape[0]), ts);\n    &gt;&gt;&gt; plt.show();\n\n    ![indicator](../../../images/Scaled_indicator.png)\n    \"\"\"\n    _check_number_samples(number_samples)\n    return constant * indicator_ts(number_samples, idx, number_samples - 1)\n</code></pre>"},{"location":"API_ref/data_generation/time_series_functions/sinusoid/","title":"Sinusoid","text":""},{"location":"API_ref/data_generation/time_series_functions/sinusoid/#timecave.data_generation.time_series_functions.sinusoid_ts","title":"<code>timecave.data_generation.time_series_functions.sinusoid_ts(number_samples, max_interval_size, amplitude=1, frequency=1, phase=0)</code>","text":"<p>Generate a time series of a sinusoidal signal.</p> <p>This function generates a time series of a sinusoidal signal with the specified parameters.</p> <p>Parameters:</p> Name Type Description Default <code>number_samples</code> <code>int</code> <p>The number of samples in the generated time series.</p> required <code>max_interval_size</code> <code>float</code> <p>The maximum interval size (time duration) of the generated time series.</p> required <code>amplitude</code> <code>float</code> <p>The amplitude of the sinusoidal signal.</p> <code>1</code> <code>frequency</code> <code>float</code> <p>The frequency of the sinusoidal signal in cycles per unit time.</p> <code>1</code> <code>phase</code> <code>float</code> <p>The phase offset of the sinusoidal signal in radians.</p> <code>0</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>The generated sinusoidal time series.</p> See also <p>frequency_varying_sinusoid_ts: Generate a time-varying sinusoid.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; import matplotlib.pyplot as plt\n&gt;&gt;&gt; from timecave.data_generation.time_series_functions import sinusoid_ts\n&gt;&gt;&gt; ts = sinusoid_ts(1000, 10, amplitude=3, frequency=0.5);\n&gt;&gt;&gt; _ = plt.plot(np.arange(0, ts.shape[0]), ts);\n&gt;&gt;&gt; plt.show();\n</code></pre> <p></p> <p>Higher frequency sinusoids can be generated as well:</p> <pre><code>&gt;&gt;&gt; ts2 = sinusoid_ts(1000, 10, amplitude=3, frequency=5);\n&gt;&gt;&gt; _ = plt.plot(np.arange(0, ts2.shape[0]), ts2);\n&gt;&gt;&gt; plt.show();\n</code></pre> <p></p> <p>Phase shifts can be added too:</p> <pre><code>&gt;&gt;&gt; ts3 = sinusoid_ts(1000, 10, amplitude=3, frequency=0.5, phase=0.3);\n&gt;&gt;&gt; _ = plt.plot(np.arange(0, ts2.shape[0]), ts3);\n&gt;&gt;&gt; plt.show();\n</code></pre> <p></p> Source code in <code>timecave/data_generation/time_series_functions.py</code> <pre><code>def sinusoid_ts(\n    number_samples: int,\n    max_interval_size: float or int,\n    amplitude: float = 1,\n    frequency: float or int = 1,\n    phase: float = 0,\n) -&gt; np.ndarray:\n    \"\"\"\n    Generate a time series of a sinusoidal signal.\n\n    This function generates a time series of a sinusoidal signal with the specified parameters.\n\n    Parameters\n    ----------\n    number_samples : int\n        The number of samples in the generated time series.\n\n    max_interval_size : float\n        The maximum interval size (time duration) of the generated time series.\n\n    amplitude : float, default=1\n        The amplitude of the sinusoidal signal.\n\n    frequency : float, default=1\n        The frequency of the sinusoidal signal in cycles per unit time.\n\n    phase : float, default=0\n        The phase offset of the sinusoidal signal in radians.\n\n    Returns\n    -------\n    np.ndarray\n        The generated sinusoidal time series.\n\n    See also\n    --------\n    [frequency_varying_sinusoid_ts](time_varying_sin.md): Generate a time-varying sinusoid.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; import matplotlib.pyplot as plt\n    &gt;&gt;&gt; from timecave.data_generation.time_series_functions import sinusoid_ts\n    &gt;&gt;&gt; ts = sinusoid_ts(1000, 10, amplitude=3, frequency=0.5);\n    &gt;&gt;&gt; _ = plt.plot(np.arange(0, ts.shape[0]), ts);\n    &gt;&gt;&gt; plt.show();\n\n    ![sinusoid](../../../images/Sinusoid.png)\n\n    Higher frequency sinusoids can be generated as well:\n\n    &gt;&gt;&gt; ts2 = sinusoid_ts(1000, 10, amplitude=3, frequency=5);\n    &gt;&gt;&gt; _ = plt.plot(np.arange(0, ts2.shape[0]), ts2);\n    &gt;&gt;&gt; plt.show();\n\n    ![sinusoid_freq](../../../images/Sinusoid_high_freq.png)\n\n    Phase shifts can be added too:\n\n    &gt;&gt;&gt; ts3 = sinusoid_ts(1000, 10, amplitude=3, frequency=0.5, phase=0.3);\n    &gt;&gt;&gt; _ = plt.plot(np.arange(0, ts2.shape[0]), ts3);\n    &gt;&gt;&gt; plt.show();\n\n    ![sinusoid_shift](../../../images/Shifted_sinusoid.png)\n    \"\"\"\n    _check_number_samples(number_samples)\n    _check_max_interval_size(max_interval_size)\n    time_values = np.linspace(0, max_interval_size, number_samples)\n    sinusoid = amplitude * np.sin(2 * np.pi * frequency * time_values + phase)\n\n    return sinusoid\n</code></pre>"},{"location":"API_ref/data_generation/time_series_functions/time_varying_sin/","title":"Time Varying Sinusoid","text":""},{"location":"API_ref/data_generation/time_series_functions/time_varying_sin/#timecave.data_generation.time_series_functions.frequency_varying_sinusoid_ts","title":"<code>timecave.data_generation.time_series_functions.frequency_varying_sinusoid_ts(number_samples, max_interval_size, frequency, amplitude=1, phase=0)</code>","text":"<p>Generate a time series of a sinusoidal signal with varying frequency.</p> <p>This function generates a time series of a sinusoidal signal where the frequency varies over time.</p> <p>Parameters:</p> Name Type Description Default <code>number_samples</code> <code>int</code> <p>The number of samples in the time series.</p> required <code>max_interval_size</code> <code>float</code> <p>The maximum time interval size for the time series.</p> required <code>frequency</code> <code>BaseFrequency</code> <p>An object representing the base frequency of the sinusoid, which may vary over time.</p> required <code>amplitude</code> <code>float</code> <p>The amplitude of the sinusoidal signal.</p> <code>1</code> <code>phase</code> <code>float</code> <p>The initial phase of the sinusoidal signal in radians.</p> <code>1</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>An array representing the generated time series of the sinusoidal signal with varying frequency.</p> See also <p>sinusoid_ts: Generate a simple sinusoid.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; import matplotlib.pyplot as plt\n&gt;&gt;&gt; from timecave.data_generation.frequency_modulation import FrequencyModulationWithStep, FrequencyModulationLinear\n&gt;&gt;&gt; from timecave.data_generation.time_series_functions import frequency_varying_sinusoid_ts\n</code></pre> <p>Generate a sinusoid whose frequency varies abruptly:</p> <pre><code>&gt;&gt;&gt; mod = FrequencyModulationWithStep(20, 5);\n&gt;&gt;&gt; ts = frequency_varying_sinusoid_ts(100, 10, frequency=mod);\n&gt;&gt;&gt; _ = plt.plot(np.arange(0, ts.shape[0]), ts);\n&gt;&gt;&gt; plt.show();\n</code></pre> <p></p> <p>Time series with linearly varying frequencies can be generated as well:</p> <pre><code>&gt;&gt;&gt; mod_lin = FrequencyModulationLinear(1, 0.2);\n&gt;&gt;&gt; ts2 = frequency_varying_sinusoid_ts(1000, 10, frequency=mod_lin, amplitude=5);\n&gt;&gt;&gt; _ = plt.plot(np.arange(0, ts2.shape[0]), ts2);\n&gt;&gt;&gt; plt.show();\n</code></pre> <p></p> Source code in <code>timecave/data_generation/time_series_functions.py</code> <pre><code>def frequency_varying_sinusoid_ts(\n    number_samples: int,\n    max_interval_size: float,\n    frequency: BaseFrequency,\n    amplitude: float = 1,\n    phase: float = 0,\n) -&gt; np.ndarray:\n    \"\"\"\n    Generate a time series of a sinusoidal signal with varying frequency.\n\n    This function generates a time series of a sinusoidal signal where the frequency varies over time.\n\n    Parameters\n    ----------\n    number_samples : int\n        The number of samples in the time series.\n\n    max_interval_size : float\n        The maximum time interval size for the time series.\n\n    frequency : BaseFrequency\n        An object representing the base frequency of the sinusoid, which may vary over time.\n\n    amplitude : float, default=1\n        The amplitude of the sinusoidal signal.\n\n    phase : float, default=1\n        The initial phase of the sinusoidal signal in radians.\n\n    Returns\n    -------\n    np.ndarray\n        An array representing the generated time series of the sinusoidal signal with varying frequency.\n\n    See also\n    --------\n    [sinusoid_ts](sinusoid.md): Generate a simple sinusoid.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; import matplotlib.pyplot as plt\n    &gt;&gt;&gt; from timecave.data_generation.frequency_modulation import FrequencyModulationWithStep, FrequencyModulationLinear\n    &gt;&gt;&gt; from timecave.data_generation.time_series_functions import frequency_varying_sinusoid_ts\n\n    Generate a sinusoid whose frequency varies abruptly:\n\n    &gt;&gt;&gt; mod = FrequencyModulationWithStep(20, 5);\n    &gt;&gt;&gt; ts = frequency_varying_sinusoid_ts(100, 10, frequency=mod);\n    &gt;&gt;&gt; _ = plt.plot(np.arange(0, ts.shape[0]), ts);\n    &gt;&gt;&gt; plt.show();\n\n    ![step_freq](../../../images/Step_freq.png)\n\n    Time series with linearly varying frequencies can be generated as well:\n\n    &gt;&gt;&gt; mod_lin = FrequencyModulationLinear(1, 0.2);\n    &gt;&gt;&gt; ts2 = frequency_varying_sinusoid_ts(1000, 10, frequency=mod_lin, amplitude=5);\n    &gt;&gt;&gt; _ = plt.plot(np.arange(0, ts2.shape[0]), ts2);\n    &gt;&gt;&gt; plt.show();\n\n    ![lin_freq](../../../images/Lin_freq.png)\n    \"\"\"\n    _check_number_samples(number_samples)\n    _check_max_interval_size(max_interval_size)\n\n    time = np.linspace(0, max_interval_size, number_samples)\n    frequency = frequency.modulate(time=time)\n    time_series = amplitude * np.sin(2 * np.pi * frequency * time + phase)\n    return time_series\n</code></pre>"},{"location":"API_ref/data_generation/time_series_generation/","title":"Time Series Generation","text":""},{"location":"API_ref/data_generation/time_series_generation/#timecave.data_generation.time_series_generation","title":"<code>timecave.data_generation.time_series_generation</code>","text":"<p>A module for generating time series data using the functions provided by this package.</p> <p>This module provides a class for generating time series data based on provided functions, with optional noise and weights.</p> <p>Classes:</p> Name Description <code>TimeSeriesGenerator</code> <p>A class for generating time series data using the functions provided by the time_series_functions module.</p>"},{"location":"API_ref/data_generation/time_series_generation/gen/","title":"Time Series Generator","text":""},{"location":"API_ref/data_generation/time_series_generation/gen/#timecave.data_generation.time_series_generation.TimeSeriesGenerator","title":"<code>timecave.data_generation.time_series_generation.TimeSeriesGenerator(functions, length=100, noise_level=0.1, weights=None, parameter_values=None)</code>","text":"<p>A class for generating time series data using provided functions.</p> <p>This class enables the generation of multiple time series by combining various functions with parameters. It allows customization of the time series length, noise level, weights for functions, and parameter values.</p> <p>Parameters:</p> Name Type Description Default <code>functions</code> <code>List[Callable]</code> <p>A list of functions used to generate the time series.</p> required <code>length</code> <code>int</code> <p>The length of the time series to be generated.</p> <code>100</code> <code>noise_level</code> <code>float</code> <p>The standard deviation of the Gaussian noise added to the time series.</p> <code>0.1</code> <code>weights</code> <code>List[float]</code> <p>A list of weights corresponding to each function.</p> <code>None</code> <code>parameter_values</code> <code>list[Dict]</code> <p>A list of dictionaries containing parameter values for each function. Each dictionary contains parameter names as keys and either single values, tuples (for discrete choices), or lists (for continuous ranges) as values, representing the possible values or ranges for each parameter.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>time_series</code> <code>list[ndarray]</code> <p>The generated time series.</p> <p>Methods:</p> Name Description <code>generate</code> <p>Generate time series data.</p> <code>plot</code> <p>Plot the generated time series.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the lengths of 'functions', 'parameter_values', and 'weights' don't match.</p> Source code in <code>timecave/data_generation/time_series_generation.py</code> <pre><code>def __init__(\n    self,\n    functions: List[Callable],\n    length: int = 100,\n    noise_level: float or int = 0.1,\n    weights: List[float] = None,\n    parameter_values: list[Dict] = None,\n) -&gt; None:\n    self._check_functions(functions)\n    self._check_length(length)\n    self._check_noise_level(noise_level)\n    self._check_weights(weights)\n    self._check_parameter_values(parameter_values)\n\n    if len(functions) != len(parameter_values):\n\n        raise ValueError(\n            \"Lengths of 'functions', 'parameter_values', and 'weights' must match.\"\n        )\n    if weights is not None and (\n        len(weights) != len(parameter_values) or len(functions) != len(weights)\n    ):\n\n        raise ValueError(\n            \"Lengths of 'functions', 'parameter_values', and 'weights' must match.\"\n        )\n\n    self._functions = functions\n    self._length = length\n    self._noise_level = noise_level\n    self._parameter_values = parameter_values\n    self._weights = weights\n    self.time_series = []\n    if weights is None:\n        self._weights = [1.0] * len(functions)\n</code></pre>"},{"location":"API_ref/data_generation/time_series_generation/gen/#timecave.data_generation.time_series_generation.TimeSeriesGenerator.generate","title":"<code>generate(nb_sim, og_seed=1)</code>","text":"<p>Generate time series data.</p> <p>Parameters:</p> Name Type Description Default <code>nb_sim</code> <code>int</code> <p>Number of simulations to generate.</p> required <code>og_seed</code> <code>int</code> <p>The original seed for generating random numbers.</p> <code>1</code> <p>Returns:</p> Type Description <code>List[array]</code> <p>A list of numpy arrays containing generated time series data.</p> Warning <p>The <code>number_samples</code> parameter is inferred from the <code>length</code> parameter.  Therefore, it should not be passed to the method using the <code>parameter_values</code> argument.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; import matplotlib.pyplot as plt\n&gt;&gt;&gt; from timecave.data_generation.time_series_functions import linear_ts, indicator_ts, exponential_ts\n&gt;&gt;&gt; from timecave.data_generation.time_series_generation import TimeSeriesGenerator\n</code></pre> <p>Generate 3 time series using a combination of linear, indicator, and exponential functions:</p> <pre><code>&gt;&gt;&gt; gen = TimeSeriesGenerator([linear_ts, indicator_ts, exponential_ts],\n...                            length=10,\n...                            parameter_values=[{\"max_interval_size\": [10, 10], \"slope\": [1, 5]}, # A random slope between 1 and 5 will be generated.\n...                                              {\"start_index\": 2, \"end_index\": 6},\n...                                              {\"max_interval_size\": [10, 10], \"decay_rate\": [0.1, 10]}]); # 'max_interval_size' will always be 10.\n&gt;&gt;&gt; ts = gen.generate(3);\n&gt;&gt;&gt; ts\n[array([ 1.1550022 ,  5.08763814, 10.75491998, 15.05675456, 19.71847986,\n       24.3300877 , 28.65378249, 32.42146755, 36.97335225, 41.32569794]), array([ 1.090524  ,  3.94981031,  7.7498048 , 10.77923442, 13.51272171,\n       16.82720927, 19.84383981, 21.52150532, 24.56596177, 27.69603248]), array([ 1.03173452,  4.41648421,  9.72828385, 13.96528833, 18.37799188,\n       22.61230447, 26.9295597 , 30.32151586, 34.57860333, 38.94221321])]\n</code></pre> Source code in <code>timecave/data_generation/time_series_generation.py</code> <pre><code>def generate(self, nb_sim: int, og_seed: int = 1):\n    \"\"\"\n    Generate time series data.\n\n    Parameters\n    ----------\n    nb_sim : int\n        Number of simulations to generate.\n\n    og_seed : int, default=1\n        The original seed for generating random numbers.\n\n    Returns\n    -------\n    List[np.array]\n        A list of numpy arrays containing generated time series data.\n\n    Warning\n    -------\n    The `number_samples` parameter is inferred from the `length` parameter. \n    Therefore, it should not be passed to the method using the `parameter_values` argument.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; import matplotlib.pyplot as plt\n    &gt;&gt;&gt; from timecave.data_generation.time_series_functions import linear_ts, indicator_ts, exponential_ts\n    &gt;&gt;&gt; from timecave.data_generation.time_series_generation import TimeSeriesGenerator\n\n    Generate 3 time series using a combination of linear, indicator, and exponential functions:\n\n    &gt;&gt;&gt; gen = TimeSeriesGenerator([linear_ts, indicator_ts, exponential_ts],\n    ...                            length=10,\n    ...                            parameter_values=[{\"max_interval_size\": [10, 10], \"slope\": [1, 5]}, # A random slope between 1 and 5 will be generated.\n    ...                                              {\"start_index\": 2, \"end_index\": 6},\n    ...                                              {\"max_interval_size\": [10, 10], \"decay_rate\": [0.1, 10]}]); # 'max_interval_size' will always be 10.\n    &gt;&gt;&gt; ts = gen.generate(3);\n    &gt;&gt;&gt; ts\n    [array([ 1.1550022 ,  5.08763814, 10.75491998, 15.05675456, 19.71847986,\n           24.3300877 , 28.65378249, 32.42146755, 36.97335225, 41.32569794]), array([ 1.090524  ,  3.94981031,  7.7498048 , 10.77923442, 13.51272171,\n           16.82720927, 19.84383981, 21.52150532, 24.56596177, 27.69603248]), array([ 1.03173452,  4.41648421,  9.72828385, 13.96528833, 18.37799188,\n           22.61230447, 26.9295597 , 30.32151586, 34.57860333, 38.94221321])]\n    \"\"\"\n\n    seeds = _generate_seeds(og_seed, nb_sim)\n\n    for seed in seeds:\n        np.random.seed(seed)\n        ts = np.zeros(self._length)\n        for i in range(len(self._functions)):\n            parameters = _generate_random_parameters(\n                self._parameter_values[i], seed=seed\n            )\n            ts += self._weights[i] * self._functions[i](self._length, **parameters)\n\n        ts += np.random.normal(scale=self._noise_level, size=self._length)\n\n        self.time_series.append(ts)\n\n    return self.time_series\n</code></pre>"},{"location":"API_ref/data_generation/time_series_generation/gen/#timecave.data_generation.time_series_generation.TimeSeriesGenerator.plot","title":"<code>plot(indexes=None)</code>","text":"<p>Plot the generated time series.</p> <p>Parameters:</p> Name Type Description Default <code>indexes</code> <code>array or list or range</code> <p>Indexes of time series to plot. None by default.</p> <code>None</code> Warning <p>This method should only be called once the <code>generate</code> method has been run.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; import matplotlib.pyplot as plt\n&gt;&gt;&gt; from timecave.data_generation.time_series_generation import TimeSeriesGenerator\n&gt;&gt;&gt; from timecave.data_generation.time_series_functions import linear_ts, scaled_right_indicator_ts, exponential_ts, sinusoid_ts\n</code></pre> <p>Generate 5 time series using a combination of linear and indicator functions:</p> <pre><code>&gt;&gt;&gt; gen = TimeSeriesGenerator([linear_ts, scaled_right_indicator_ts],\n...                            length=200,\n...                            parameter_values=[{\"max_interval_size\": [10, 10], \"slope\": [1, 10]},\n...                                              {\"idx\": 100, \"constant\": [5, 100]}]);\n&gt;&gt;&gt; ts = gen.generate(5);\n&gt;&gt;&gt; gen.plot([0, 1, 2, 3, 4]);\n</code></pre> <p></p> <p>Using a combination of exponential functions and sinusoids instead:</p> <pre><code>&gt;&gt;&gt; gen2 = TimeSeriesGenerator([exponential_ts, sinusoid_ts],\n...                             length=1000,\n...                             parameter_values=[{\"max_interval_size\": [10, 10], \"decay_rate\": [1, 10], \"initial_value\": [1, 10]},\n...                                               {\"max_interval_size\": [10, 10], \"amplitude\": [0.5, 3], \"frequency\": [0.1, 5]}]);\n&gt;&gt;&gt; ts2 = gen2.generate(5);\n&gt;&gt;&gt; gen2.plot([0, 1, 2, 3, 4]);\n</code></pre> <p></p> Source code in <code>timecave/data_generation/time_series_generation.py</code> <pre><code>def plot(self, indexes: np.array or list or range = None):\n    \"\"\"\n    Plot the generated time series.\n\n    Parameters\n    ----------\n    indexes : np.array or list or range, optional\n        Indexes of time series to plot. None by default.\n\n    Warning\n    -------\n    This method should only be called once the `generate` method has been run.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; import matplotlib.pyplot as plt\n    &gt;&gt;&gt; from timecave.data_generation.time_series_generation import TimeSeriesGenerator\n    &gt;&gt;&gt; from timecave.data_generation.time_series_functions import linear_ts, scaled_right_indicator_ts, exponential_ts, sinusoid_ts\n\n    Generate 5 time series using a combination of linear and indicator functions:\n\n    &gt;&gt;&gt; gen = TimeSeriesGenerator([linear_ts, scaled_right_indicator_ts],\n    ...                            length=200,\n    ...                            parameter_values=[{\"max_interval_size\": [10, 10], \"slope\": [1, 10]},\n    ...                                              {\"idx\": 100, \"constant\": [5, 100]}]);\n    &gt;&gt;&gt; ts = gen.generate(5);\n    &gt;&gt;&gt; gen.plot([0, 1, 2, 3, 4]);\n\n    ![gen_plots](../../../images/Gen_plots1.png)\n\n    Using a combination of exponential functions and sinusoids instead:\n\n    &gt;&gt;&gt; gen2 = TimeSeriesGenerator([exponential_ts, sinusoid_ts],\n    ...                             length=1000,\n    ...                             parameter_values=[{\"max_interval_size\": [10, 10], \"decay_rate\": [1, 10], \"initial_value\": [1, 10]},\n    ...                                               {\"max_interval_size\": [10, 10], \"amplitude\": [0.5, 3], \"frequency\": [0.1, 5]}]);\n    &gt;&gt;&gt; ts2 = gen2.generate(5);\n    &gt;&gt;&gt; gen2.plot([0, 1, 2, 3, 4]);\n\n    ![gen_plots2](../../../images/Gen_plots2.png)\n    \"\"\"\n    if indexes is None:\n        indexes = range(len(self.time_series))\n    elif isinstance(indexes, int):\n        indexes = [indexes]\n\n    for idx in indexes:\n        plt.plot(self.time_series[idx], label=f\"Time Series {idx}\")\n\n    plt.xlabel(\"Time\")\n    plt.ylabel(\"Value\")\n    plt.title(\"Generated Time Series\")\n    plt.legend()\n    plt.show()\n</code></pre>"},{"location":"API_ref/metrics/","title":"Validation strategy metrics module","text":""},{"location":"API_ref/metrics/#timecave.validation_strategy_metrics","title":"<code>timecave.validation_strategy_metrics</code>","text":"<p>This module contains several metrics to evaluate the performance of model validation methods.</p> <p>Functions:</p> Name Description <code>PAE</code> <p>Implements the Predictive Accuracy Error metric.</p> <code>APAE</code> <p>Implements the Absolute Predictive Accuracy Error metric.</p> <code>RPAE</code> <p>Implements the Relative Predictive Accuracy Error metric.</p> <code>RAPAE</code> <p>Implements the Relative Absolute Predictive Accuracy Error metric.</p> <code>sMPAE</code> <p>Implements the symmetric Mean Predictive Accuracy Error metric.</p> <code>MC_metric</code> <p>Statistical summary for Monte Carlo experiments regarding validation methods.</p> <code>under_over_estimation</code> <p>Separate statistical summaries for the underestimation and overestimation cases.</p> Notes <ul> <li>PAE and APAE are absolute metrics. Their values may range from \\(-L_m\\) to \\(\\infty\\) and from \\(0\\) to \\(\\infty\\), respectively. These should not be used to compare results obtained with different models or using different time series.</li> <li>RPAE and RAPAE are relative metrics, as they measure how large the validation error is with respect to the true (test) error, thus eliminating the latter's influence on the metric. Their values lie in the \\([-1, \\infty]\\) and \\([0, \\infty]\\) intervals, respectively. These can be used to compare results for different models and/or time series.</li> <li>sMPAE is a scaled, symmetric version of the PAE. It can be used to compare results for different models and/or time series.</li> </ul>"},{"location":"API_ref/metrics/MC_metric/","title":"MC_metric","text":""},{"location":"API_ref/metrics/MC_metric/#timecave.validation_strategy_metrics.MC_metric","title":"<code>timecave.validation_strategy_metrics.MC_metric(estimated_error_list, test_error_list, metric)</code>","text":"<p>Compute validation strategy metrics for N different experiments (MC stands for Monte Carlo).</p> <p>This function processes the results of a Monte Carlo experiment and outputs a statistical summary of the results.     This can be useful if one needs to analyse the performance of a given validation method on several different time series or using different models. Users may provide a custom metric if they so desire, but it must have the same function signature as the metrics provided by this package.</p> <p>Parameters:</p> Name Type Description Default <code>estimated_error_list</code> <code>list[float | int]</code> <p>List of estimated (i.e. validation) errors, one for each experiment / trial.</p> required <code>test_error_list</code> <code>list[float | int]</code> <p>List of test errors, one for each experiment / trial.</p> required <code>metric</code> <code>callable</code> <p>Validation strategy metric.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>A statistical summary of the results.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the estimator error list and the test error list differ in length.</p> See also <p>under_over_estimation:     Computes separate statistics for overestimation and underestimation cases.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from timecave.validation_strategy_metrics import PAE, MC_metric\n&gt;&gt;&gt; true_errors = [10, 30, 10, 50];\n&gt;&gt;&gt; validation_errors = [20, 20, 50, 30];\n&gt;&gt;&gt; MC_metric(validation_errors, true_errors, PAE)\n{'Mean': 5.0, 'Median': 0.0, '1st_Quartile': -12.5, '3rd_Quartile': 17.5, 'Minimum': -20.0, 'Maximum': 40.0, 'Standard_deviation': 22.9128784747792}\n</code></pre> <p>If the lengths of the estimated error and test error lists do not match, an exception is thrown:</p> <pre><code>&gt;&gt;&gt; MC_metric(validation_errors, [10], PAE)\nTraceback (most recent call last):\n...\nValueError: The estimated error and test error lists must have the same length.\n</code></pre> Source code in <code>timecave/validation_strategy_metrics.py</code> <pre><code>def MC_metric(estimated_error_list: list[float | int], test_error_list: list[float | int], metric: callable) -&gt; dict:\n\n    \"\"\"\n    Compute validation strategy metrics for N different experiments (MC stands for Monte Carlo).\n\n    This function processes the results of a Monte Carlo experiment and outputs a statistical summary of the results. \\\n    This can be useful if one needs to analyse the performance of a given validation method on several different time series or using different models.  \n    Users may provide a custom metric if they so desire, but it must have the same function signature as the metrics provided by this package.\n\n    Parameters\n    ----------\n    estimated_error_list : list[float  |  int]\n        List of estimated (i.e. validation) errors, one for each experiment / trial.\n\n    test_error_list : list[float  |  int]\n        List of test errors, one for each experiment / trial.\n\n    metric : callable\n        Validation strategy metric.\n\n    Returns\n    -------\n    dict\n        A statistical summary of the results.\n\n    Raises\n    ------\n    ValueError\n        If the estimator error list and the test error list differ in length.\n\n    See also\n    --------\n    [under_over_estimation](under_over.md):\n        Computes separate statistics for overestimation and underestimation cases.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from timecave.validation_strategy_metrics import PAE, MC_metric\n    &gt;&gt;&gt; true_errors = [10, 30, 10, 50];\n    &gt;&gt;&gt; validation_errors = [20, 20, 50, 30];\n    &gt;&gt;&gt; MC_metric(validation_errors, true_errors, PAE)\n    {'Mean': 5.0, 'Median': 0.0, '1st_Quartile': -12.5, '3rd_Quartile': 17.5, 'Minimum': -20.0, 'Maximum': 40.0, 'Standard_deviation': 22.9128784747792}\n\n    If the lengths of the estimated error and test error lists do not match, an exception is thrown:\n\n    &gt;&gt;&gt; MC_metric(validation_errors, [10], PAE)\n    Traceback (most recent call last):\n    ...\n    ValueError: The estimated error and test error lists must have the same length.\n    \"\"\"\n\n    if(len(estimated_error_list) != len(test_error_list)):\n\n        raise ValueError(\"The estimated error and test error lists must have the same length.\");\n\n    metric_array = np.zeros(len(estimated_error_list));\n\n    for ind, (val_error, test_error) in enumerate(zip(estimated_error_list, test_error_list)):\n\n        metric_array[ind] = metric(val_error, test_error);\n\n    mean = metric_array.mean();\n    minimum = metric_array.min();\n    maximum = metric_array.max();\n    median = np.median(metric_array);\n    Q1 = np.quantile(metric_array, 0.25);\n    Q3 = np.quantile(metric_array, 0.75);\n    std = metric_array.std();\n\n    results = {\"Mean\": mean,\n               \"Median\": median,\n               \"1st_Quartile\": Q1,\n               \"3rd_Quartile\": Q3,\n               \"Minimum\": minimum,\n               \"Maximum\": maximum,\n               \"Standard_deviation\": std};\n\n    return results;\n</code></pre>"},{"location":"API_ref/metrics/apae/","title":"APAE","text":""},{"location":"API_ref/metrics/apae/#timecave.validation_strategy_metrics.APAE","title":"<code>timecave.validation_strategy_metrics.APAE(estimated_error, test_error)</code>","text":"<p>Compute the Absolute Predictive Accuracy Error (APAE).</p> <p>This function computes the APAE metric. Both the estimated (i.e. validation) error and the test error must be passed as parameters.</p> <p>Parameters:</p> Name Type Description Default <code>estimated_error</code> <code>float | int</code> <p>Validation error.</p> required <code>test_error</code> <code>float | int</code> <p>True (i.e. test) error.</p> required <p>Returns:</p> Type Description <code>float</code> <p>Absolute Predictive Accuracy Error.</p> See also <p>PAE:     Predictive Accuracy Error.</p> <p>RPAE:      Relative Predictive Accuracy Error.</p> <p>RAPAE:     Relative Absolute Predictive Accuracy Error.</p> <p>sMPAE:     Symmetric Mean Predictive Accuracy Error.</p> Notes <p>The Absolute Predictive Accuracy Error is defined as the absolute value of the difference between the     estimate of a model's error given by a validation method     and the model's true error. In other words, it is the absolute value of the Predictive Accuracy Error:</p> \\[ APAE = |\\hat{L}_m - L_m| = |PAE| \\] <p>Since the APAE is always non-negative, this metric cannot be used to determine whether the validation method is overestimating or underestimating    the model's true error.</p> <p>Note that, in all likelihood, the true error will not be known. It is usually estimated using an independent test set. For more details, please refer to [1].</p> References <p>Examples:</p> <pre><code>&gt;&gt;&gt; from timecave.validation_strategy_metrics import APAE\n&gt;&gt;&gt; APAE(10, 3)\n7\n&gt;&gt;&gt; APAE(1, 5)\n4\n&gt;&gt;&gt; APAE(8, 8)\n0\n</code></pre> Source code in <code>timecave/validation_strategy_metrics.py</code> <pre><code>def APAE(estimated_error: float | int, test_error: float | int) -&gt; float:\n\n    \"\"\"\n    Compute the Absolute Predictive Accuracy Error (APAE).\n\n    This function computes the APAE metric. Both the estimated (i.e. validation) error\n    and the test error must be passed as parameters.\n\n    Parameters\n    ----------\n    estimated_error : float | int\n        Validation error.\n\n    test_error : float | int\n        True (i.e. test) error.\n\n    Returns\n    -------\n    float\n        Absolute Predictive Accuracy Error.\n\n    See also\n    --------\n    [PAE](pae.md):\n        Predictive Accuracy Error.\n\n    [RPAE](rpae.md): \n        Relative Predictive Accuracy Error.\n\n    [RAPAE](rapae.md):\n        Relative Absolute Predictive Accuracy Error.\n\n    [sMPAE](smpae.md):\n        Symmetric Mean Predictive Accuracy Error.\n\n    Notes\n    -----\n    The Absolute Predictive Accuracy Error is defined as the absolute value of the difference between the \\\n    estimate of a model's error given by a validation method \\\n    and the model's true error. In other words, it is the absolute value of the Predictive Accuracy Error:\n\n    $$\n    APAE = |\\hat{L}_m - L_m| = |PAE|\n    $$ \n\n    Since the APAE is always non-negative, this metric cannot be used to determine whether the validation method is overestimating or underestimating\\\n    the model's true error.\n\n    Note that, in all likelihood, the true error will not be known. It is usually estimated using an independent test set. For more details, please refer to [[1]](#1).\n\n    References\n    ----------\n    ##1\n    Cerqueira, V., Torgo, L., Mozeti\u02c7c, I., 2020. Evaluating time series forecasting\n    models: An empirical study on performance estimation methods.\n    Machine Learning 109, 1997\u20132028.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from timecave.validation_strategy_metrics import APAE\n    &gt;&gt;&gt; APAE(10, 3)\n    7\n    &gt;&gt;&gt; APAE(1, 5)\n    4\n    &gt;&gt;&gt; APAE(8, 8)\n    0\n    \"\"\"\n\n    return abs(estimated_error - test_error);\n</code></pre>"},{"location":"API_ref/metrics/apae/#timecave.validation_strategy_metrics.APAE--1","title":"1","text":"<p>Cerqueira, V., Torgo, L., Mozeti\u02c7c, I., 2020. Evaluating time series forecasting models: An empirical study on performance estimation methods. Machine Learning 109, 1997\u20132028.</p>"},{"location":"API_ref/metrics/pae/","title":"PAE","text":""},{"location":"API_ref/metrics/pae/#timecave.validation_strategy_metrics.PAE","title":"<code>timecave.validation_strategy_metrics.PAE(estimated_error, test_error)</code>","text":"<p>Compute the Predictive Accuracy Error (PAE).</p> <p>This function computes the PAE metric. Both the estimated (i.e. validation) error and the test error must be passed as parameters.</p> <p>Parameters:</p> Name Type Description Default <code>estimated_error</code> <code>float | int</code> <p>Validation error.</p> required <code>test_error</code> <code>float | int</code> <p>True (i.e. test) error.</p> required <p>Returns:</p> Type Description <code>float</code> <p>Predictive Accuracy Error.</p> See also <p>APAE:     Absolute Predictive Accuracy Error.</p> <p>RPAE:      Relative Predictive Accuracy Error.</p> <p>RAPAE:     Relative Absolute Predictive Accuracy Error.</p> <p>sMPAE:     Symmetric Mean Predictive Accuracy Error.</p> Notes <p>The Predictive Accuracy Error is defined as the difference between the estimate of a model's error given by a validation method    and the model's true error:</p> \\[ PAE = \\hat{L}_m - L_m \\] <p>The sign allows one to determine whether the validation method is overestimating or underestimating the model's true error:    a negative value denotes an underestimation, while a positive value corresponds to an overestimation.   Note that, in all likelihood, the true error will not be known. It is usually estimated using an independent test set. For more details, please refer to [1].</p> References <p>Examples:</p> <pre><code>&gt;&gt;&gt; from timecave.validation_strategy_metrics import PAE\n&gt;&gt;&gt; PAE(10, 3)\n7\n&gt;&gt;&gt; PAE(1, 5)\n-4\n&gt;&gt;&gt; PAE(8, 8)\n0\n</code></pre> Source code in <code>timecave/validation_strategy_metrics.py</code> <pre><code>def PAE(estimated_error: float | int, test_error: float | int) -&gt; float:\n\n    \"\"\"\n    Compute the Predictive Accuracy Error (PAE).\n\n    This function computes the PAE metric. Both the estimated (i.e. validation) error\n    and the test error must be passed as parameters.\n\n    Parameters\n    ----------\n    estimated_error : float | int\n        Validation error.\n\n    test_error : float | int\n        True (i.e. test) error.\n\n    Returns\n    -------\n    float\n        Predictive Accuracy Error.\n\n    See also\n    --------\n    [APAE](apae.md):\n        Absolute Predictive Accuracy Error.\n\n    [RPAE](rpae.md): \n        Relative Predictive Accuracy Error.\n\n    [RAPAE](rapae.md):\n        Relative Absolute Predictive Accuracy Error.\n\n    [sMPAE](smpae.md):\n        Symmetric Mean Predictive Accuracy Error.\n\n    Notes\n    -----\n    The Predictive Accuracy Error is defined as the difference between the estimate of a model's error given by a validation method\\\n    and the model's true error:\n\n    $$\n    PAE = \\hat{L}_m - L_m\n    $$ \n\n    The sign allows one to determine whether the validation method is overestimating or underestimating the model's true error:\\\n    a negative value denotes an underestimation, while a positive value corresponds to an overestimation.\\\n\n    Note that, in all likelihood, the true error will not be known. It is usually estimated using an independent test set. For more details, please refer to [[1]](#1).\n\n    References\n    ----------\n    ##1\n    Cerqueira, V., Torgo, L., Mozeti\u02c7c, I., 2020. Evaluating time series forecasting\n    models: An empirical study on performance estimation methods.\n    Machine Learning 109, 1997\u20132028.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from timecave.validation_strategy_metrics import PAE\n    &gt;&gt;&gt; PAE(10, 3)\n    7\n    &gt;&gt;&gt; PAE(1, 5)\n    -4\n    &gt;&gt;&gt; PAE(8, 8)\n    0\n    \"\"\"\n\n    return estimated_error - test_error;\n</code></pre>"},{"location":"API_ref/metrics/pae/#timecave.validation_strategy_metrics.PAE--1","title":"1","text":"<p>Cerqueira, V., Torgo, L., Mozeti\u02c7c, I., 2020. Evaluating time series forecasting models: An empirical study on performance estimation methods. Machine Learning 109, 1997\u20132028.</p>"},{"location":"API_ref/metrics/rapae/","title":"RAPAE","text":""},{"location":"API_ref/metrics/rapae/#timecave.validation_strategy_metrics.RAPAE","title":"<code>timecave.validation_strategy_metrics.RAPAE(estimated_error, test_error)</code>","text":"<p>Compute the Relative Absolute Predictive Accuracy Error (RAPAE).</p> <p>This function computes the RAPAE metric. Both the estimated (i.e. validation) error and the test error must be passed as parameters.</p> <p>Parameters:</p> Name Type Description Default <code>estimated_error</code> <code>float | int</code> <p>Validation error.</p> required <code>test_error</code> <code>float | int</code> <p>True (i.e. test) error.</p> required <p>Returns:</p> Type Description <code>float</code> <p>Relative Absolute Predictive Accuracy Error.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>test_error</code> is zero.</p> See also <p>PAE:     Predictive Accuracy Error.</p> <p>APAE:      Absolute Predictive Accuracy Error.</p> <p>RPAE:     Relative Predictive Accuracy Error.</p> <p>sMPAE:     Symmetric Mean Predictive Accuracy Error.    </p> Notes <p>The Relative Absolute Predictive Accuracy Error is defined as the Absolute Predictive Accuracy Error (APAE) divided by the     model's true error. It can also be seen as the absolute value of the Relative Predictive Accuracy Error (RPAE):</p> \\[ RAPAE = \\frac{|\\hat{L}_m - L_m|}{L_m} = \\frac{|PAE|}{L_m} = \\frac{APAE}{L_m} = |RPAE| \\] <p>This metric essentially takes the absolute value of the RPAE, and can be used in a similar fashion. However, since it uses the     absolute value, it cannot be used to determine whether a validation method is overestimating or underestimating the model's     true error. Like the RPAE, it is an asymmetric measure.</p> <p>Note that, in all likelihood, the true error will not be known. It is usually estimated using an independent test set.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from timecave.validation_strategy_metrics import RAPAE\n&gt;&gt;&gt; RAPAE(15, 5)\n2.0\n&gt;&gt;&gt; RAPAE(1, 5)\n0.8\n&gt;&gt;&gt; RAPAE(8, 8)\n0.0\n</code></pre> <p>If the true error is zero, the metric is undefined:</p> <pre><code>&gt;&gt;&gt; RAPAE(5, 0)\nTraceback (most recent call last):\n...\nValueError: The test error is zero. RAPAE is undefined.\n</code></pre> Source code in <code>timecave/validation_strategy_metrics.py</code> <pre><code>def RAPAE(estimated_error: float | int, test_error: float | int) -&gt; float:\n\n    \"\"\"\n    Compute the Relative Absolute Predictive Accuracy Error (RAPAE).\n\n    This function computes the RAPAE metric. Both the estimated (i.e. validation) error\n    and the test error must be passed as parameters.\n\n    Parameters\n    ----------\n    estimated_error : float | int\n        Validation error.\n\n    test_error : float | int\n        True (i.e. test) error.\n\n    Returns\n    -------\n    float\n        Relative Absolute Predictive Accuracy Error.\n\n    Raises\n    ------\n    ValueError\n        If `test_error` is zero.\n\n    See also\n    --------\n    [PAE](pae.md):\n        Predictive Accuracy Error.\n\n    [APAE](apae.md): \n        Absolute Predictive Accuracy Error.\n\n    [RPAE](rpae.md):\n        Relative Predictive Accuracy Error.\n\n    [sMPAE](smpae.md):\n        Symmetric Mean Predictive Accuracy Error.    \n\n    Notes\n    -----\n    The Relative Absolute Predictive Accuracy Error is defined as the Absolute Predictive Accuracy Error (APAE) divided by the \\\n    model's true error. It can also be seen as the absolute value of the Relative Predictive Accuracy Error (RPAE):\n\n    $$\n    RAPAE = \\\\frac{|\\hat{L}_m - L_m|}{L_m} = \\\\frac{|PAE|}{L_m} = \\\\frac{APAE}{L_m} = |RPAE|\n    $$ \n\n    This metric essentially takes the absolute value of the RPAE, and can be used in a similar fashion. However, since it uses the \\\n    absolute value, it cannot be used to determine whether a validation method is overestimating or underestimating the model's \\\n    true error. Like the RPAE, it is an asymmetric measure.\n\n    Note that, in all likelihood, the true error will not be known. It is usually estimated using an independent test set.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from timecave.validation_strategy_metrics import RAPAE\n    &gt;&gt;&gt; RAPAE(15, 5)\n    2.0\n    &gt;&gt;&gt; RAPAE(1, 5)\n    0.8\n    &gt;&gt;&gt; RAPAE(8, 8)\n    0.0\n\n    If the true error is zero, the metric is undefined:\n\n    &gt;&gt;&gt; RAPAE(5, 0)\n    Traceback (most recent call last):\n    ...\n    ValueError: The test error is zero. RAPAE is undefined.\n    \"\"\"\n\n    if(test_error == 0):\n\n        raise ValueError(\"The test error is zero. RAPAE is undefined.\");\n\n    return abs(estimated_error - test_error) / test_error;\n</code></pre>"},{"location":"API_ref/metrics/rpae/","title":"RPAE","text":""},{"location":"API_ref/metrics/rpae/#timecave.validation_strategy_metrics.RPAE","title":"<code>timecave.validation_strategy_metrics.RPAE(estimated_error, test_error)</code>","text":"<p>Compute the Relative Predictive Accuracy Error (RPAE).</p> <p>This function computes the RPAE metric. Both the estimated (i.e. validation) error and the test error must be passed as parameters.</p> <p>Parameters:</p> Name Type Description Default <code>estimated_error</code> <code>float | int</code> <p>Validation error.</p> required <code>test_error</code> <code>float | int</code> <p>True (i.e. test) error.</p> required <p>Returns:</p> Type Description <code>float</code> <p>Relative Predictive Accuracy Error.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>test_error</code> is zero.</p> See also <p>PAE:     Predictive Accuracy Error.</p> <p>APAE:      Absolute Predictive Accuracy Error.</p> <p>RAPAE:     Relative Absolute Predictive Accuracy Error.</p> <p>sMPAE:     Symmetric Mean Predictive Accuracy Error.</p> Notes <p>The Relative Predictive Accuracy Error is obtained by dividing the Predictive Accuracy Error (PAE) by the model's true error:</p> \\[ RPAE = \\frac{\\hat{L}_m - L_m}{L_m} = \\frac{PAE}{L_m} \\] <p>This makes this metric scale-independent with respect to the model's true error, which in turn makes it useful for comparing validation methods     across different time series and/or forecasting models. Since this is essentially a scaled version of the PAE,     the sign retains its significance (negative sign for underestimation, positive sign for overestimation).     However, it should be noted that the RPAE is asymmetric: in case of an underestimation, its values will be contained in the interval of \\([-1, 0[\\); if the error is     overestimated, however, the RPAE can take any value in the range of \\(]0, \\infty[\\). A value of zero denotes a perfect estimate.</p> <p>Note that, in all likelihood, the true error will not be known. It is usually estimated using an independent test set.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from timecave.validation_strategy_metrics import RPAE\n&gt;&gt;&gt; RPAE(15, 5)\n2.0\n&gt;&gt;&gt; RPAE(1, 5)\n-0.8\n&gt;&gt;&gt; RPAE(8, 8)\n0.0\n</code></pre> <p>If the true error is zero, the metric is undefined:</p> <pre><code>&gt;&gt;&gt; RPAE(5, 0)\nTraceback (most recent call last):\n...\nValueError: The test error is zero. RPAE is undefined.\n</code></pre> Source code in <code>timecave/validation_strategy_metrics.py</code> <pre><code>def RPAE(estimated_error: float | int, test_error: float | int) -&gt; float:\n\n    \"\"\"\n    Compute the Relative Predictive Accuracy Error (RPAE).\n\n    This function computes the RPAE metric. Both the estimated (i.e. validation) error\n    and the test error must be passed as parameters.\n\n    Parameters\n    ----------\n    estimated_error : float | int\n        Validation error.\n\n    test_error : float | int\n        True (i.e. test) error.\n\n    Returns\n    -------\n    float\n        Relative Predictive Accuracy Error.\n\n    Raises\n    ------\n    ValueError\n        If `test_error` is zero.\n\n    See also\n    --------\n    [PAE](pae.md):\n        Predictive Accuracy Error.\n\n    [APAE](apae.md): \n        Absolute Predictive Accuracy Error.\n\n    [RAPAE](rapae.md):\n        Relative Absolute Predictive Accuracy Error.\n\n    [sMPAE](smpae.md):\n        Symmetric Mean Predictive Accuracy Error.\n\n    Notes\n    -----\n    The Relative Predictive Accuracy Error is obtained by dividing the Predictive Accuracy Error (PAE) by the model's true error:\n\n    $$\n    RPAE = \\\\frac{\\hat{L}_m - L_m}{L_m} = \\\\frac{PAE}{L_m}\n    $$ \n\n    This makes this metric scale-independent with respect to the model's true error, which in turn makes it useful for comparing validation methods \\\n    across different time series and/or forecasting models. Since this is essentially a scaled version of the PAE, \\\n    the sign retains its significance (negative sign for underestimation, positive sign for overestimation). \\\n    However, it should be noted that the RPAE is asymmetric: in case of an underestimation, its values will be contained in the interval of $[-1, 0[$; if the error is \\\n    overestimated, however, the RPAE can take any value in the range of $]0, \\infty[$. A value of zero denotes a perfect estimate.\n\n    Note that, in all likelihood, the true error will not be known. It is usually estimated using an independent test set.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from timecave.validation_strategy_metrics import RPAE\n    &gt;&gt;&gt; RPAE(15, 5)\n    2.0\n    &gt;&gt;&gt; RPAE(1, 5)\n    -0.8\n    &gt;&gt;&gt; RPAE(8, 8)\n    0.0\n\n    If the true error is zero, the metric is undefined:\n\n    &gt;&gt;&gt; RPAE(5, 0)\n    Traceback (most recent call last):\n    ...\n    ValueError: The test error is zero. RPAE is undefined.\n    \"\"\"\n\n    if(test_error == 0):\n\n        raise ValueError(\"The test error is zero. RPAE is undefined.\");\n\n    return (estimated_error - test_error) / test_error;\n</code></pre>"},{"location":"API_ref/metrics/smpae/","title":"sMPAE","text":""},{"location":"API_ref/metrics/smpae/#timecave.validation_strategy_metrics.sMPAE","title":"<code>timecave.validation_strategy_metrics.sMPAE(estimated_error, test_error)</code>","text":"<p>Compute the symmetric Mean Predictive Accuracy Error (sMPAE).</p> <p>This function computes the sMPAE metric. Both the estimated (i.e. validation) error and the test error must be passed as parameters.</p> <p>Parameters:</p> Name Type Description Default <code>estimated_error</code> <code>float | int</code> <p>Validation error.</p> required <code>test_error</code> <code>float | int</code> <p>True (i.e. test) error.</p> required <p>Returns:</p> Type Description <code>float</code> <p>Symmetric Mean Predictive Accuracy Error.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>test_error</code> is zero.</p> See also <p>PAE:     Predictive Accuracy Error.</p> <p>APAE:     Absolute Predictive Accuracy Error.</p> <p>RPAE:     Relative Predictive Accuracy Error.</p> <p>RAPAE:     Relative Absolute Predictive Accuracy Error.</p> Notes <p>The symmetric Mean Predictive Accuracy Error is obtained by dividing the Predictive Accuracy Error (PAE)     by half the sum of the absolute values of both the error estimate and the true error:</p> \\[ sMPAE = 2 \\cdot \\frac{(\\hat{L}_m - L_m)}{|\\hat{L}_m| + |L_m|} = 2 \\cdot \\frac{PAE}{|\\hat{L}_m| + |L_m|} \\] <p>Similarly to the Relative Predictive Accuracy Error (RPAE), this metric can be seen as a scaled version of     the PAE. Unlike the RPAE, however, the sMPAE is symmetric, as all possible values lie in the interval of \\([-2, 2]\\). If the     error estimate is equal to the true error (perfect estimation), the sMPAE is zero.     Since this metric is based on the PAE, the sign retains its significance (negative sign for underestimation, positive sign for overestimation).</p> <p>Note that, in all likelihood, the true error will not be known. It is usually estimated using an independent test set.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from timecave.validation_strategy_metrics import sMPAE\n&gt;&gt;&gt; sMPAE(3, 2)\n0.4\n&gt;&gt;&gt; sMPAE(3, 5)\n-0.5\n&gt;&gt;&gt; sMPAE(5, 5)\n0.0\n&gt;&gt;&gt; sMPAE(5, 0)\n2.0\n&gt;&gt;&gt; sMPAE(0, 5)\n-2.0\n</code></pre> <p>If both the true error and the estimated error are zero, this metric is undefined:</p> <pre><code>&gt;&gt;&gt; sMPAE(0, 0)\nTraceback (most recent call last):\n...\nValueError: sMPAE is undefined.\n</code></pre> Source code in <code>timecave/validation_strategy_metrics.py</code> <pre><code>def sMPAE(estimated_error: float | int, test_error: float | int) -&gt; float:\n\n    \"\"\"\n    Compute the symmetric Mean Predictive Accuracy Error (sMPAE).\n\n    This function computes the sMPAE metric. Both the estimated (i.e. validation) error\n    and the test error must be passed as parameters.\n\n    Parameters\n    ----------\n    estimated_error : float | int\n        Validation error.\n\n    test_error : float | int\n        True (i.e. test) error.\n\n    Returns\n    -------\n    float\n        Symmetric Mean Predictive Accuracy Error.\n\n    Raises\n    ------\n    ValueError\n        If `test_error` is zero.\n\n    See also\n    --------\n    [PAE](pae.md):\n        Predictive Accuracy Error.\n\n    [APAE](apae.md):\n        Absolute Predictive Accuracy Error.\n\n    [RPAE](rpae.md):\n        Relative Predictive Accuracy Error.\n\n    [RAPAE](rapae.md):\n        Relative Absolute Predictive Accuracy Error.\n\n    Notes\n    -----\n    The symmetric Mean Predictive Accuracy Error is obtained by dividing the Predictive Accuracy Error (PAE) \\\n    by half the sum of the absolute values of both the error estimate and the true error:\n\n    $$\n    sMPAE = 2 \\cdot \\\\frac{(\\hat{L}_m - L_m)}{|\\hat{L}_m| + |L_m|} = 2 \\cdot \\\\frac{PAE}{|\\hat{L}_m| + |L_m|}\n    $$\n\n    Similarly to the Relative Predictive Accuracy Error (RPAE), this metric can be seen as a scaled version of \\\n    the PAE. Unlike the RPAE, however, the sMPAE is symmetric, as all possible values lie in the interval of $[-2, 2]$. If the \\\n    error estimate is equal to the true error (perfect estimation), the sMPAE is zero. \\\n    Since this metric is based on the PAE, the sign retains its significance (negative sign for underestimation, positive sign for overestimation).\n\n    Note that, in all likelihood, the true error will not be known. It is usually estimated using an independent test set.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from timecave.validation_strategy_metrics import sMPAE\n    &gt;&gt;&gt; sMPAE(3, 2)\n    0.4\n    &gt;&gt;&gt; sMPAE(3, 5)\n    -0.5\n    &gt;&gt;&gt; sMPAE(5, 5)\n    0.0\n    &gt;&gt;&gt; sMPAE(5, 0)\n    2.0\n    &gt;&gt;&gt; sMPAE(0, 5)\n    -2.0\n\n    If both the true error and the estimated error are zero, this metric is undefined:\n\n    &gt;&gt;&gt; sMPAE(0, 0)\n    Traceback (most recent call last):\n    ...\n    ValueError: sMPAE is undefined.\n    \"\"\"\n\n    if((abs(estimated_error) + abs(test_error)) == 0):\n\n        raise ValueError(\"sMPAE is undefined.\");\n\n    return 2*(estimated_error - test_error) / (abs(estimated_error) + abs(test_error));\n</code></pre>"},{"location":"API_ref/metrics/under_over/","title":"under_over_estimation","text":""},{"location":"API_ref/metrics/under_over/#timecave.validation_strategy_metrics.under_over_estimation","title":"<code>timecave.validation_strategy_metrics.under_over_estimation(estimated_error_list, test_error_list, metric)</code>","text":"<p>Compute separate validation strategy metrics for underestimation and overestimation instances (for N different experiments).</p> <p>This function processes the results of a Monte Carlo experiment and outputs two separate sets of summary statistics: one for cases where the true error is underestimated, and another one for cases  where the validation method overestimates the error. This can be useful if one needs to analyse the performance of a given validation method on several different time series or using different models. Users may provide a custom metric if they so desire, but it must have the same function signature as the metrics provided by this package.</p> <p>Parameters:</p> Name Type Description Default <code>estimated_error_list</code> <code>list[float | int]</code> <p>List of estimated (i.e. validation) errors, one for each experiment / trial.</p> required <code>test_error_list</code> <code>list[float | int]</code> <p>List of test errors, one for each experiment / trial.</p> required <code>metric</code> <code>callable</code> <p>Validation strategy metric.</p> required <p>Returns:</p> Type Description <code>tuple[dict]</code> <p>[Separate] Statistical summaries for the overestimation and underestimation cases.         The first dictionary is for the underestimation cases.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the estimator error list and the test error list differ in length.</p> See also <p>MC_metric:     Computes relevant statistics for the whole Monte Carlo experiment (i.e. does not differentiate between overestimation and underestimation).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from timecave.validation_strategy_metrics import under_over_estimation, PAE\n&gt;&gt;&gt; true_errors = [10, 30, 10, 50];\n&gt;&gt;&gt; validation_errors = [20, 20, 50, 30];\n&gt;&gt;&gt; under_over_estimation(validation_errors, true_errors, PAE)\n({'Mean': -15.0, 'Median': -15.0, '1st_Quartile': -17.5, '3rd_Quartile': -12.5, 'Minimum': -20.0, 'Maximum': -10.0, 'Standard_deviation': 5.0, 'N': 2, '%': 50.0}, {'Mean': 25.0, 'Median': 25.0, '1st_Quartile': 17.5, '3rd_Quartile': 32.5, 'Minimum': 10.0, 'Maximum': 40.0, 'Standard_deviation': 15.0, 'N': 2, '%': 50.0})\n</code></pre> <p>If there are no overestimation or underestimation cases, the respective dictionary will be empty:</p> <pre><code>&gt;&gt;&gt; under_over_estimation([10, 20, 30], [5, 10, 15], PAE)\nNo errors were underestimated. Underestimation data dictionary empty.\n({}, {'Mean': 10.0, 'Median': 10.0, '1st_Quartile': 7.5, '3rd_Quartile': 12.5, 'Minimum': 5.0, 'Maximum': 15.0, 'Standard_deviation': 4.08248290463863, 'N': 3, '%': 100.0})\n</code></pre> <p>If the lengths of the estimated error and test error lists do not match, an exception is thrown:</p> <pre><code>&gt;&gt;&gt; under_over_estimation(validation_errors, [10], PAE)\nTraceback (most recent call last):\n...\nValueError: The estimated error and test error lists must have the same length.\n</code></pre> Source code in <code>timecave/validation_strategy_metrics.py</code> <pre><code>def under_over_estimation(estimated_error_list: list[float | int], test_error_list: list[float | int], metric: callable) -&gt; tuple[dict]:\n\n    \"\"\"\n    Compute separate validation strategy metrics for underestimation and overestimation instances (for N different experiments).\n\n    This function processes the results of a Monte Carlo experiment and outputs two separate\n    sets of summary statistics: one for cases where the true error is underestimated, and another one for cases \n    where the validation method overestimates the error.\n    This can be useful if one needs to analyse the performance of a given validation method on several different time series or using different models.  \n    Users may provide a custom metric if they so desire, but it must have the same function signature as the metrics provided by this package.\n\n    Parameters\n    ----------\n    estimated_error_list : list[float  |  int]\n        List of estimated (i.e. validation) errors, one for each experiment / trial.\n\n    test_error_list : list[float  |  int]\n        List of test errors, one for each experiment / trial.\n\n    metric : callable\n        Validation strategy metric.\n\n    Returns\n    -------\n    tuple[dict]\n        [Separate] Statistical summaries for the overestimation and underestimation cases. \\\n        The first dictionary is for the underestimation cases.\n\n    Raises\n    ------\n    ValueError\n        If the estimator error list and the test error list differ in length.\n\n    See also\n    --------\n    [MC_metric](MC_metric.md):\n        Computes relevant statistics for the whole Monte Carlo experiment (i.e. does not differentiate between overestimation and underestimation).\n\n    Examples\n    --------\n    &gt;&gt;&gt; from timecave.validation_strategy_metrics import under_over_estimation, PAE\n    &gt;&gt;&gt; true_errors = [10, 30, 10, 50];\n    &gt;&gt;&gt; validation_errors = [20, 20, 50, 30];\n    &gt;&gt;&gt; under_over_estimation(validation_errors, true_errors, PAE)\n    ({'Mean': -15.0, 'Median': -15.0, '1st_Quartile': -17.5, '3rd_Quartile': -12.5, 'Minimum': -20.0, 'Maximum': -10.0, 'Standard_deviation': 5.0, 'N': 2, '%': 50.0}, {'Mean': 25.0, 'Median': 25.0, '1st_Quartile': 17.5, '3rd_Quartile': 32.5, 'Minimum': 10.0, 'Maximum': 40.0, 'Standard_deviation': 15.0, 'N': 2, '%': 50.0})\n\n    If there are no overestimation or underestimation cases, the respective dictionary will be empty:\n\n    &gt;&gt;&gt; under_over_estimation([10, 20, 30], [5, 10, 15], PAE)\n    No errors were underestimated. Underestimation data dictionary empty.\n    ({}, {'Mean': 10.0, 'Median': 10.0, '1st_Quartile': 7.5, '3rd_Quartile': 12.5, 'Minimum': 5.0, 'Maximum': 15.0, 'Standard_deviation': 4.08248290463863, 'N': 3, '%': 100.0})\n\n    If the lengths of the estimated error and test error lists do not match, an exception is thrown:\n\n    &gt;&gt;&gt; under_over_estimation(validation_errors, [10], PAE)\n    Traceback (most recent call last):\n    ...\n    ValueError: The estimated error and test error lists must have the same length.\n    \"\"\"\n\n    if(len(estimated_error_list) != len(test_error_list)):\n\n        raise ValueError(\"The estimated error and test error lists must have the same length.\");\n\n    estimated_errors = np.array(estimated_error_list);\n    test_errors = np.array(test_error_list);\n\n    under_est = estimated_errors[estimated_errors &lt; test_errors].tolist();\n    under_test = test_errors[estimated_errors &lt; test_errors].tolist();\n    over_est = estimated_errors[estimated_errors &gt; test_errors].tolist();\n    over_test = test_errors[estimated_errors &gt; test_errors].tolist();\n\n    if(len(under_est) &gt; 0):\n\n        under_estimation_stats = MC_metric(under_est, under_test, metric);\n        under_estimation_stats[\"N\"] = len(under_est);\n        under_estimation_stats[\"%\"] = np.round(len(under_est) / len(estimated_error_list) * 100, 2);\n\n    else:\n\n        under_estimation_stats = {};\n        print(\"No errors were underestimated. Underestimation data dictionary empty.\");\n\n    if(len(over_est) &gt; 0):\n\n        over_estimation_stats = MC_metric(over_est, over_test, metric);\n        over_estimation_stats[\"N\"] = len(over_est);\n        over_estimation_stats[\"%\"] = np.round(len(over_est) / len(estimated_error_list) * 100, 2);\n\n    else:\n\n        over_estimation_stats = {};\n        print(\"No errors were overestimated. Overestimation data dictionary empty.\");\n\n    return (under_estimation_stats, over_estimation_stats);\n</code></pre>"},{"location":"API_ref/utils/","title":"Utilities module","text":""},{"location":"API_ref/utils/#timecave.utils","title":"<code>timecave.utils</code>","text":"<p>This module contains utility functions to help the users make the most of their data. More specifically, it provides routines to aid users during the data collection process and validation procedures.</p> <p>Functions:</p> Name Description <code>Nyquist_min_samples</code> <p>Computes the minimum amount of samples needed to capture a frequency of interest using the Nyquist theorem.</p> <code>heuristic_min_samples</code> <p>Computes the minimum amount of samples needed to capture a frequency of interest using an heuristic algorithm.</p> <code>true_test_indices</code> <p>Readies an array of validation indices for insertion into a model.</p>"},{"location":"API_ref/utils/heuristic/","title":"heuristic_min_samples","text":""},{"location":"API_ref/utils/heuristic/#timecave.utils.heuristic_min_samples","title":"<code>timecave.utils.heuristic_min_samples(fs, freq_limit)</code>","text":"<p>Compute the minimum number of samples the series should have according to the 10 / 20 sampling heuristic.</p> <p>This function computes the minimum and maximum lengths for capturing a given frequency,  assuming the time series was  sampled at a frequency of <code>fs</code> Hertz and the largest frequency  of interest for modelling purposes is <code>freq_limit</code> Hertz. The interval in which the sampling frequency should lie for <code>freq_limit</code>  to be effectively captured is also derived. The 10 / 20 sampling  heuristic is used to derive both results.</p> <p>Parameters:</p> Name Type Description Default <code>fs</code> <code>float | int</code> <p>The time series' sampling frequency (Hz).</p> required <code>freq_limit</code> <code>float | int</code> <p>Largest frequency of interest (Hz).</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Minimum and maximum number of samples (Min_samples and Max_samples, respectively) required to capture freq_limit with a sampling frequency of fs,  according to the 10 / 20 heuristic rule.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If either <code>fs</code> or <code>freq_limit</code> is neither a float nor an integer.</p> <code>ValueError</code> <p>If either <code>fs</code> or <code>freq_limit</code> are non-positive.</p> <code>Warning</code> <p>If the choice of <code>fs</code> and <code>freq_limit</code> does not abide by the 10 / 20 heuristic.</p> See also <p>Nyquist_min_samples:     Performs the same computations using the Nyquist theorem.</p> Notes <p>Under certain circumstances, the conditions of the Nyquist theorem might not be enough to guarantee that the reconstruction of the signal is possible. To address this isssue, a heuristic has been developed in the field of control engineering, according to which the sampling frequency should be 10 to 20 times higher than the largest     frequency of interest:</p> \\[ 10 \\cdot f &lt;= f_s &lt;= 20 \\cdot f \\] <p>Theoretically, the higher the sampling frequency, the better (i.e. the easier it can be to reconstruct the original signal),     though hardware limitations naturally come into play here.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from timecave.utils import heuristic_min_samples\n&gt;&gt;&gt; n_samples = heuristic_min_samples(150, 10);\n10 / 20 sampling heuristic results\n----------------------------------\nMinimum sampling rate required to capture a frequency of 10 Hz : 100 Hz\nMaximum sampling rate required to capture a frequency of 10 Hz : 200 Hz\n----------------------------------------------------------------------------------------------\nCapturing a frequency of 10 Hz with a sampling frequency of 150 Hz would require:\n150 to 300 samples\n&gt;&gt;&gt; n_samples\n{'Min_samples': 150, 'Max_samples': 300}\n</code></pre> <p>If the frequency of interest cannot be captured using the sampling frequency provided     by the user according to the heuristic, an exception is thrown:</p> <pre><code>&gt;&gt;&gt; samples = heuristic_min_samples(80, 10);\nTraceback (most recent call last):\n...\nWarning: This choice of sampling frequency and frequency of interest is not compliant with the 10 / 20 sampling heuristic.\n</code></pre> <p>If negative frequencies are passed, or if their values are neither integers nor floats, exceptions are thrown as well:</p> <pre><code>&gt;&gt;&gt; samples = heuristic_min_samples(-2, 1);\nTraceback (most recent call last):\n...\nValueError: Frequencies should be non-negative.\n&gt;&gt;&gt; samples = heuristic_min_samples(1, \"a\");\nTraceback (most recent call last):\n...\nTypeError: Both 'fs' and 'freq_limit' should be either integers or floats.\n</code></pre> Source code in <code>timecave/utils.py</code> <pre><code>def heuristic_min_samples(fs: float | int, freq_limit: float | int) -&gt; dict:\n\n    \"\"\"\n    Compute the minimum number of samples the series should have\n    according to the 10 / 20 sampling heuristic.\n\n    This function computes the minimum and maximum lengths for capturing a given frequency, \n    assuming the time series was \n    sampled at a frequency of `fs` Hertz and the largest frequency \n    of interest for modelling purposes is `freq_limit` Hertz. The\n    interval in which the sampling frequency should lie for `freq_limit` \n    to be effectively captured is also derived. The 10 / 20 sampling \n    heuristic is used to derive both results.\n\n    Parameters\n    ----------\n    fs : float | int\n        The time series' sampling frequency (Hz).\n\n    freq_limit : float | int\n        Largest frequency of interest (Hz).\n\n    Returns\n    -------\n    dict\n        Minimum and maximum number of samples (Min_samples and Max_samples, respectively)\n        required to capture freq_limit with a sampling frequency of fs, \n        according to the 10 / 20 heuristic rule.\n\n    Raises\n    ------\n    TypeError\n        If either `fs` or `freq_limit` is neither a float nor an integer.\n\n    ValueError\n        If either `fs` or `freq_limit` are non-positive.\n\n    Warning\n        If the choice of `fs` and `freq_limit` does not abide by the 10 / 20 heuristic.\n\n    See also\n    --------\n    [Nyquist_min_samples](nyquist.md):\n        Performs the same computations using the Nyquist theorem.\n\n    Notes\n    -----\n    Under certain circumstances, the conditions of the Nyquist theorem might not be enough to guarantee that the reconstruction of the signal is possible.\n    To address this isssue, a heuristic has been developed in the field of control engineering, according to which the sampling frequency should be 10 to 20 times higher than the largest \\\n    frequency of interest:\n\n    $$\n    10 \\cdot f &lt;= f_s &lt;= 20 \\cdot f\n    $$\n\n    Theoretically, the higher the sampling frequency, the better (i.e. the easier it can be to reconstruct the original signal), \\\n    though hardware limitations naturally come into play here.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from timecave.utils import heuristic_min_samples\n    &gt;&gt;&gt; n_samples = heuristic_min_samples(150, 10);\n    10 / 20 sampling heuristic results\n    ----------------------------------\n    Minimum sampling rate required to capture a frequency of 10 Hz : 100 Hz\n    Maximum sampling rate required to capture a frequency of 10 Hz : 200 Hz\n    ----------------------------------------------------------------------------------------------\n    Capturing a frequency of 10 Hz with a sampling frequency of 150 Hz would require:\n    150 to 300 samples\n    &gt;&gt;&gt; n_samples\n    {'Min_samples': 150, 'Max_samples': 300}\n\n    If the frequency of interest cannot be captured using the sampling frequency provided \\\n    by the user according to the heuristic, an exception is thrown:\n\n    &gt;&gt;&gt; samples = heuristic_min_samples(80, 10);\n    Traceback (most recent call last):\n    ...\n    Warning: This choice of sampling frequency and frequency of interest is not compliant with the 10 / 20 sampling heuristic.\n\n    If negative frequencies are passed, or if their values are neither integers nor floats, exceptions are thrown as well:\n\n    &gt;&gt;&gt; samples = heuristic_min_samples(-2, 1);\n    Traceback (most recent call last):\n    ...\n    ValueError: Frequencies should be non-negative.\n    &gt;&gt;&gt; samples = heuristic_min_samples(1, \"a\");\n    Traceback (most recent call last):\n    ...\n    TypeError: Both 'fs' and 'freq_limit' should be either integers or floats.\n    \"\"\"\n\n    _check_frequencies(fs, freq_limit);\n    _check_heuristic(fs, freq_limit);\n\n    ts = 1 / fs;\n    T_limit = 1 / freq_limit;\n    t_lower = 10 * T_limit;\n    t_upper = 20 * T_limit;\n    n_lower = int(np.ceil(t_lower / ts));\n    n_upper = int(np.ceil(t_upper / ts));\n\n    print(\"10 / 20 sampling heuristic results\");\n    print(\"----------------------------------\");\n\n    print(f\"Minimum sampling rate required to capture a frequency of {freq_limit} Hz : {10*freq_limit} Hz\");\n    print(f\"Maximum sampling rate required to capture a frequency of {freq_limit} Hz : {20*freq_limit} Hz\");\n    print(\"----------------------------------------------------------------------------------------------\");\n    print(f\"Capturing a frequency of {freq_limit} Hz with a sampling frequency of {fs} Hz would require:\");\n    print(f\"{n_lower} to {n_upper} samples\");\n\n    return {\"Min_samples\": n_lower, \"Max_samples\": n_upper};\n</code></pre>"},{"location":"API_ref/utils/indices/","title":"true_test_indices","text":""},{"location":"API_ref/utils/indices/#timecave.utils.true_test_indices","title":"<code>timecave.utils.true_test_indices(test_ind, model_order)</code>","text":"<p>Modify an array of validation indices for modelling purposes.</p> <p>This function modifies the array of validation indices yielded by a  splitter so that it includes the previous time steps that should be passed  as inputs to the model in order for it to predict the series' next value.</p> <p>Parameters:</p> Name Type Description Default <code>test_ind</code> <code>ndarray</code> <p>Array of test (validation, really) indices yielded by a splitter.</p> required <code>model_order</code> <code>int</code> <p>The number of previous time steps the model needs to take as input in order to predict the series' value at the next time step.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Array of validation indices including the time steps required by the model to predict the series' value at the first validation index.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If <code>model_order</code> is not an integer.</p> <code>ValueError</code> <p>If <code>model_order</code> is not positive.</p> <code>ValueError</code> <p>If <code>model_order</code> is larger than the amount of samples in the training set, assuming it precedes the validation set.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from timecave.utils import true_test_indices\n&gt;&gt;&gt; test_indices = np.array([8, 9, 10]);\n&gt;&gt;&gt; model_order = 2;\n&gt;&gt;&gt; true_test_indices(test_indices, model_order)\narray([ 6,  7,  8,  9, 10])\n</code></pre> <p>The order of a model must be an integer value:</p> <pre><code>&gt;&gt;&gt; true_test_indices(test_indices, 0.5)\nTraceback (most recent call last):\n...\nTypeError: 'model_order' should be an integer.\n</code></pre> <p>The order of a model must not be a negative value:</p> <pre><code>&gt;&gt;&gt; true_test_indices(test_indices, -1)\nTraceback (most recent call last):\n...\nValueError: 'model_order' should be positive.\n</code></pre> <p>This function assumes training and validation are done sequentially.     Therefore, an exception will be thrown if the amount of samples preceding the validation     set is smaller than the order of the model:</p> <pre><code>&gt;&gt;&gt; true_test_indices(test_indices, 10)\nTraceback (most recent call last):\n...\nValueError: 'model_order' should be smaller than the amount of samples in the training set.\n</code></pre> Source code in <code>timecave/utils.py</code> <pre><code>def true_test_indices(test_ind: np.ndarray, model_order: int) -&gt; np.ndarray:\n\n    \"\"\"\n    Modify an array of validation indices for modelling purposes.\n\n    This function modifies the array of validation indices yielded by a \n    splitter so that it includes the previous time steps that should be passed \n    as inputs to the model in order for it to predict the series' next value.\n\n    Parameters\n    ----------\n    test_ind : np.ndarray\n        Array of test (validation, really) indices yielded by a splitter.\n\n    model_order : int\n        The number of previous time steps the model needs to take as input in order\n        to predict the series' value at the next time step.\n\n    Returns\n    -------\n    np.ndarray\n        Array of validation indices including the time steps required by the model\n        to predict the series' value at the first validation index.\n\n    Raises\n    ------\n    TypeError\n        If `model_order` is not an integer.\n\n    ValueError\n        If `model_order` is not positive.\n\n    ValueError\n        If `model_order` is larger than the amount of samples in the training set, assuming it precedes the validation set.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from timecave.utils import true_test_indices\n    &gt;&gt;&gt; test_indices = np.array([8, 9, 10]);\n    &gt;&gt;&gt; model_order = 2;\n    &gt;&gt;&gt; true_test_indices(test_indices, model_order)\n    array([ 6,  7,  8,  9, 10])\n\n    The order of a model must be an integer value:\n\n    &gt;&gt;&gt; true_test_indices(test_indices, 0.5)\n    Traceback (most recent call last):\n    ...\n    TypeError: 'model_order' should be an integer.\n\n    The order of a model must not be a negative value:\n\n    &gt;&gt;&gt; true_test_indices(test_indices, -1)\n    Traceback (most recent call last):\n    ...\n    ValueError: 'model_order' should be positive.\n\n    This function assumes training and validation are done sequentially. \\\n    Therefore, an exception will be thrown if the amount of samples preceding the validation \\\n    set is smaller than the order of the model:\n\n    &gt;&gt;&gt; true_test_indices(test_indices, 10)\n    Traceback (most recent call last):\n    ...\n    ValueError: 'model_order' should be smaller than the amount of samples in the training set.\n    \"\"\"\n\n    _check_order(model_order);\n    _check_ind(test_ind, model_order);\n\n    new_ind = np.arange(test_ind[0] - model_order, test_ind[0]);\n    full_test_ind = np.hstack((new_ind, test_ind));\n\n    return full_test_ind;\n</code></pre>"},{"location":"API_ref/utils/nyquist/","title":"Nyquist_min_samples","text":""},{"location":"API_ref/utils/nyquist/#timecave.utils.Nyquist_min_samples","title":"<code>timecave.utils.Nyquist_min_samples(fs, freq_limit)</code>","text":"<p>Compute the minimum number of samples the series should have for the Nyquist theorem to be satisfied.</p> <p>This function computes the minimum series length for capturing a given frequency, assuming the time series was sampled at a frequency of <code>fs</code> Hertz and the largest frequency of interest for modelling purposes is <code>freq_limit</code> Hertz. Additionally, the function computes the largest frequency that can be captured using <code>fs</code> as the sampling frequency, as well as the smallest sampling  frequency that would be required to capture <code>freq_limit</code>. Both of these  results are directly derived from the Nyquist sampling theorem.</p> <p>Parameters:</p> Name Type Description Default <code>fs</code> <code>float | int</code> <p>The time series' sampling frequency (Hz).</p> required <code>freq_limit</code> <code>float | int</code> <p>Largest frequency of interest (Hz).</p> required <p>Returns:</p> Type Description <code>int</code> <p>Minimum number of samples required to capture <code>freq_limit</code> with a sampling frequency of <code>fs</code>, according to the Nyquist sampling theorem.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If either <code>fs</code> or <code>freq_limit</code> is neither a float nor an integer.</p> <code>ValueError</code> <p>If either <code>fs</code> or <code>freq_limit</code> are non-positive.</p> <code>Warning</code> <p>If the choice of <code>fs</code> and <code>freq_limit</code> does not satisfy the Nyquist sampling theorem.</p> See also <p>heuristic_min_samples:     Performs the same computations using an heuristic rule.</p> Notes <p>The Nyquist sampling theorem is a fundamental result in digital signal processing. It states that,     for one to be able to reconstruct a continuous-time signal from its discrete counterpart, the sampling     frequency should be at least twice as high as the largest frequency of interest. Mathematically speaking, the condition on the sampling frequency is:</p> \\[ f_s &gt;= 2 \\cdot f \\] <p>where \\(f_s\\) is the sampling frequency and \\(f\\) is the frequency of interest.     The Nyquist sampling theorem is discussed in several reference books, of which [1] is but an example.</p> <p>Since time series are essentially signals, the minimum number of samples that need to be collected if one needs [is] to capture a given frequency [if a given frequency is to be captured] can be computed     using the Nyquist theorem.</p> References <p>Examples:</p> <pre><code>&gt;&gt;&gt; from timecave.utils import Nyquist_min_samples\n&gt;&gt;&gt; n_samples = Nyquist_min_samples(100, 20);\nNyquist theorem results\n-----------------------\nMaximum frequency that can be extracted using a sampling frequency of 100 Hz : 50.0 Hz\nSampling rate required to capture a frequency of 20 Hz : 40 Hz\n------------------------------------------------------------------------------------------\nMinimum number of samples required to capture a frequency of 20 Hz with a\nsampling frequency of 100 Hz: 10 samples\n&gt;&gt;&gt; n_samples\n10\n</code></pre> <p>If the frequency of interest cannot be captured using the sampling frequency provided     by the user according to the Nyquist theorem, an exception is thrown:</p> <pre><code>&gt;&gt;&gt; samples = Nyquist_min_samples(1, 2);\nTraceback (most recent call last):\n...\nWarning: According to the Nyquist theorem, the selected frequency cannot be captured using this sampling frequency.\n</code></pre> <p>If negative frequencies are passed, or if their values are neither integers nor floats, exceptions are thrown as well:</p> <pre><code>&gt;&gt;&gt; samples = Nyquist_min_samples(-2, 1);\nTraceback (most recent call last):\n...\nValueError: Frequencies should be non-negative.\n&gt;&gt;&gt; samples = Nyquist_min_samples(1, \"a\");\nTraceback (most recent call last):\n...\nTypeError: Both 'fs' and 'freq_limit' should be either integers or floats.\n</code></pre> Source code in <code>timecave/utils.py</code> <pre><code>def Nyquist_min_samples(fs: float | int, freq_limit: float | int) -&gt; int:\n\n    \"\"\"\n    Compute the minimum number of samples the series should have\n    for the Nyquist theorem to be satisfied.\n\n    This function computes the minimum series length for capturing a given frequency,\n    assuming the time series was sampled at\n    a frequency of `fs` Hertz and the largest frequency of interest\n    for modelling purposes is `freq_limit` Hertz. Additionally,\n    the function computes the largest frequency that can be captured\n    using `fs` as the sampling frequency, as well as the smallest sampling \n    frequency that would be required to capture `freq_limit`. Both of these \n    results are directly derived from the Nyquist sampling theorem.\n\n    Parameters\n    ----------\n    fs : float | int\n        The time series' sampling frequency (Hz).\n\n    freq_limit : float | int\n        Largest frequency of interest (Hz).\n\n    Returns\n    -------\n    int\n        Minimum number of samples required to capture `freq_limit`\n        with a sampling frequency of `fs`, according to the Nyquist\n        sampling theorem.\n\n    Raises\n    ------\n    TypeError\n        If either `fs` or `freq_limit` is neither a float nor an integer.\n\n    ValueError\n        If either `fs` or `freq_limit` are non-positive.\n\n    Warning\n        If the choice of `fs` and `freq_limit` does not satisfy the Nyquist sampling theorem.\n\n    See also\n    --------\n    [heuristic_min_samples](heuristic.md):\n        Performs the same computations using an heuristic rule.\n\n    Notes\n    -----\n    The Nyquist sampling theorem is a fundamental result in digital signal processing. It states that, \\\n    for one to be able to reconstruct a continuous-time signal from its discrete counterpart, the sampling \\\n    frequency should be at least twice as high as the largest frequency of interest.\n    Mathematically speaking, the condition on the sampling frequency is:\n\n    $$\n    f_s &gt;= 2 \\cdot f\n    $$\n\n    where $f_s$ is the sampling frequency and $f$ is the frequency of interest. \\\n    The Nyquist sampling theorem is discussed in several reference books, of which [[1]](#1) is but an example.\n\n    Since time series are essentially signals, the minimum number of samples that need to be collected if one needs [is] to capture a given frequency [if a given frequency is to be captured] can be computed \\\n    using the Nyquist theorem.\n\n    References\n    ----------\n    ##1\n    A. Oppenheim, R. Schafer, and J. Buck. Discrete-Time Signal Processing.\n    Prentice Hall, 1999.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from timecave.utils import Nyquist_min_samples\n    &gt;&gt;&gt; n_samples = Nyquist_min_samples(100, 20);\n    Nyquist theorem results\n    -----------------------\n    Maximum frequency that can be extracted using a sampling frequency of 100 Hz : 50.0 Hz\n    Sampling rate required to capture a frequency of 20 Hz : 40 Hz\n    ------------------------------------------------------------------------------------------\n    Minimum number of samples required to capture a frequency of 20 Hz with a\n    sampling frequency of 100 Hz: 10 samples\n    &gt;&gt;&gt; n_samples\n    10\n\n    If the frequency of interest cannot be captured using the sampling frequency provided \\\n    by the user according to the Nyquist theorem, an exception is thrown:\n\n    &gt;&gt;&gt; samples = Nyquist_min_samples(1, 2);\n    Traceback (most recent call last):\n    ...\n    Warning: According to the Nyquist theorem, the selected frequency cannot be captured using this sampling frequency.\n\n    If negative frequencies are passed, or if their values are neither integers nor floats, exceptions are thrown as well:\n\n    &gt;&gt;&gt; samples = Nyquist_min_samples(-2, 1);\n    Traceback (most recent call last):\n    ...\n    ValueError: Frequencies should be non-negative.\n    &gt;&gt;&gt; samples = Nyquist_min_samples(1, \"a\");\n    Traceback (most recent call last):\n    ...\n    TypeError: Both 'fs' and 'freq_limit' should be either integers or floats.\n    \"\"\"\n\n    _check_frequencies(fs, freq_limit);\n    _check_Nyquist(fs, freq_limit);\n\n    ts = 1 / fs;\n    T_limit = 1 / freq_limit;\n    t_final = 2 * T_limit;\n    n_samples = int(np.ceil(t_final / ts));\n\n    print(\"Nyquist theorem results\");\n    print(\"-----------------------\");\n\n    print(f\"Maximum frequency that can be extracted using a sampling frequency of {fs} Hz : {fs/2} Hz\");\n    print(f\"Sampling rate required to capture a frequency of {freq_limit} Hz : {2*freq_limit} Hz\");\n    print(\"------------------------------------------------------------------------------------------\");\n    print(f\"Minimum number of samples required to capture a frequency of {freq_limit} Hz with a\");\n    print(f\"sampling frequency of {fs} Hz: {n_samples} samples\");\n\n    return n_samples;\n</code></pre>"},{"location":"API_ref/utils/nyquist/#timecave.utils.Nyquist_min_samples--1","title":"1","text":"<p>A. Oppenheim, R. Schafer, and J. Buck. Discrete-Time Signal Processing. Prentice Hall, 1999.</p>"},{"location":"API_ref/validation_methods/","title":"Validation methods","text":"<p>This subpackage contains all the model validation methods provided by this package. When importing a validation method, be sure to specify the correct module, like so:</p> <pre><code>from timecave.validation_methods.OOS import Holdout\n</code></pre> <p>Out-of-Sample and Prequential methods ensure the training set always precedes the validation set. This is not the case for cross-validation and Markov methods.</p>"},{"location":"API_ref/validation_methods/#modules","title":"Modules","text":"<ul> <li>Base: Contains the class that serves as the basis for every validation method.</li> <li>OOS: Implements Out-of-Sample (OOS) methods.</li> <li>Prequential: Implements prequential methods (also known as forward validation methods).</li> <li>CV: Implements cross-validation (CV) methods.</li> <li>Markov: Implements the Markov cross-validation method.</li> <li>Weight functions: Provides off-the-shelf weighting functions for use with CV and prequential methods.</li> </ul>"},{"location":"API_ref/validation_methods/CV/","title":"CV methods","text":""},{"location":"API_ref/validation_methods/CV/#timecave.validation_methods.CV","title":"<code>timecave.validation_methods.CV</code>","text":"<p>This module contains all the CV ('Cross-Validation') validation methods supported by this package.</p> <p>Classes:</p> Name Description <code>BlockCV</code> <p>Implements the Block CV method, along with its weighted version.</p> <code>hvBlockCV</code> <p>Implements the hv Block method.</p> <code>AdaptedhvBlockCV</code> <p>Implements the Adapted hv Block CV method, along with its weighted version.</p> See also <p>Out-of-Sample methods: Out-of-sample methods for time series data.</p> <p>Prequential methods: Prequential or forward validation methods for time series data.</p> <p>Markov methods: Markov cross-validation method for time series data.</p> Notes <p>Cross-validation methods are one of the three main classes of validation methods for time series data (the others being out-of-sample methods and prequential methods). Like prequential methods, CV methods partition the series into equally sized folds (with the exception of the hv Block variant). However, CV methods do not preserve the temporal order of observations, meaning that a model can be trained on later data and tested on earlier data. CV methods also differ from Out-of-Sample methods, as the latter do not partition the series in the same way. For more details on this class of methods, the reader should refer to [1].</p> References"},{"location":"API_ref/validation_methods/CV/#timecave.validation_methods.CV--1","title":"1","text":"<p>Vitor Cerqueira, Luis Torgo, and Igor Mozeti\u02c7c. Evaluating time series forecasting models: An empirical study on performance estimation methods. Machine Learning, 109(11):1997\u20132028, 2020.</p>"},{"location":"API_ref/validation_methods/CV/adapted_hv/","title":"Adapted hv Block Cross-validation method","text":""},{"location":"API_ref/validation_methods/CV/adapted_hv/#timecave.validation_methods.CV.AdaptedhvBlockCV","title":"<code>timecave.validation_methods.CV.AdaptedhvBlockCV(splits, ts, fs=1, h=1)</code>","text":"<p>               Bases: <code>BaseSplitter</code></p> <p>Implements the Adapted hv Block Cross-validation method.</p> <p>This class implements the Adapted hv Block Cross-validation method. It is similar to the BlockCV class,  but it does not support weight generation. Consequently, in order to implement a weighted version of this method,  the user must implement their own derived class or compute the weights separately.</p> <p>Parameters:</p> Name Type Description Default <code>splits</code> <code>int</code> <p>The number of folds used to partition the data.</p> required <code>ts</code> <code>ndarray | Series</code> <p>Univariate time series.</p> required <code>fs</code> <code>float | int</code> <p>Sampling frequency (Hz).</p> <code>1</code> <code>h</code> <code>int</code> <p>Controls the amount of samples that will be removed from the training set.  The <code>h</code> samples immediately following and preceding the validation set are not used for training.</p> <code>1</code> <p>Attributes:</p> Name Type Description <code>n_splits</code> <code>int</code> <p>The number of splits.</p> <code>sampling_freq</code> <code>int | float</code> <p>The series' sampling frequency (Hz).</p> <p>Methods:</p> Name Description <code>split</code> <p>Split the time series into training and validation sets.</p> <code>info</code> <p>Provide additional information on the validation method.</p> <code>statistics</code> <p>Compute relevant statistics for both training and validation sets.</p> <code>plot</code> <p>Plot the partitioned time series.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If <code>h</code> is not an integer.</p> <code>ValueError</code> <p>If <code>h</code> is smaller than or equal to zero.</p> <code>ValueError</code> <p>If <code>h</code> is larger than the number of samples in a fold.</p> See also <p>Block CV: The original Block CV method, where no training samples are removed.</p> Notes <p>The Adapted hv Block Cross-validation method splits the data into \\(N\\) different folds. Then, in every iteration \\(i\\), the model is validated on data from the \\(i^{th}\\) folds and trained on data from the remaining folds. There is, however, one subtle difference from the original Block Cross-validation  method: the \\(h\\) training samples that lie closest to the validation set are removed, thereby reducing the correlation between the training set and the  validation set. The average error on the validation sets is then taken as the estimate of the model's true error. This method does not preserve the temporal order of the observations.</p> <p></p> <p>This method was proposed by Cerqueira et al. [1].</p> References Source code in <code>timecave/validation_methods/CV.py</code> <pre><code>def __init__(\n    self,\n    splits: int,\n    ts: np.ndarray | pd.Series,\n    fs: float | int = 1,\n    h: int = 1,\n) -&gt; None:\n\n    super().__init__(splits, ts, fs)\n    self._check_h(h);\n    self._h = h;\n    self._splitting_ind = self._split_ind()\n</code></pre>"},{"location":"API_ref/validation_methods/CV/adapted_hv/#timecave.validation_methods.CV.AdaptedhvBlockCV--1","title":"1","text":"<p>Vitor Cerqueira, Luis Torgo, and Igor Mozeti\u02c7c. Evaluating time series forecasting models: An empirical study on performance estimation methods. Machine Learning, 109(11):1997\u20132028, 2020.</p>"},{"location":"API_ref/validation_methods/CV/adapted_hv/#timecave.validation_methods.CV.AdaptedhvBlockCV.info","title":"<code>info()</code>","text":"<p>Provide some basic information on the training and validation sets.</p> <p>This method displays the number of splits and the fold size.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from timecave.validation_methods.CV import AdaptedhvBlockCV\n&gt;&gt;&gt; ts = np.ones(10);\n&gt;&gt;&gt; splitter = AdaptedhvBlockCV(5, ts);\n&gt;&gt;&gt; splitter.info();\nAdapted hv Block CV method\n---------------\nTime series size: 10 samples\nNumber of splits: 5\nFold size: 1 to 2 samples (10.0 to 20.0 %)\n</code></pre> Source code in <code>timecave/validation_methods/CV.py</code> <pre><code>def info(self) -&gt; None:\n    \"\"\"\n    Provide some basic information on the training and validation sets.\n\n    This method displays the number of splits and the fold size.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; from timecave.validation_methods.CV import AdaptedhvBlockCV\n    &gt;&gt;&gt; ts = np.ones(10);\n    &gt;&gt;&gt; splitter = AdaptedhvBlockCV(5, ts);\n    &gt;&gt;&gt; splitter.info();\n    Adapted hv Block CV method\n    ---------------\n    Time series size: 10 samples\n    Number of splits: 5\n    Fold size: 1 to 2 samples (10.0 to 20.0 %)\n    \"\"\"\n\n    min_fold_size = int(np.floor(self._n_samples / self.n_splits)) - self._h\n    max_fold_size = int(np.floor(self._n_samples / self.n_splits))\n\n    remainder = self._n_samples % self.n_splits\n\n    if remainder != 0:\n\n        max_fold_size += 1\n\n    min_fold_size_pct = np.round(min_fold_size / self._n_samples * 100, 2)\n    max_fold_size_pct = np.round(max_fold_size / self._n_samples * 100, 2)\n\n    print(\"Adapted hv Block CV method\")\n    print(\"---------------\")\n    print(f\"Time series size: {self._n_samples} samples\")\n    print(f\"Number of splits: {self.n_splits}\")\n    print(\n        f\"Fold size: {min_fold_size} to {max_fold_size} samples ({min_fold_size_pct} to {max_fold_size_pct} %)\"\n    )\n\n    return\n</code></pre>"},{"location":"API_ref/validation_methods/CV/adapted_hv/#timecave.validation_methods.CV.AdaptedhvBlockCV.plot","title":"<code>plot(height, width)</code>","text":"<p>Plot the partitioned time series.</p> <p>This method allows the user to plot the partitioned time series. The training and validation sets are plotted using different colours.</p> <p>Parameters:</p> Name Type Description Default <code>height</code> <code>int</code> <p>The figure's height.</p> required <code>width</code> <code>int</code> <p>The figure's width.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from timecave.validation_methods.CV import AdaptedhvBlockCV\n&gt;&gt;&gt; ts = np.ones(100);\n&gt;&gt;&gt; splitter = AdaptedhvBlockCV(5, ts, h=5);\n&gt;&gt;&gt; splitter.plot(10, 10);\n</code></pre> <p></p> Source code in <code>timecave/validation_methods/CV.py</code> <pre><code>def plot(self, height: int, width: int) -&gt; None:\n    \"\"\"\n    Plot the partitioned time series.\n\n    This method allows the user to plot the partitioned time series. The training and validation sets are plotted using different colours.\n\n    Parameters\n    ----------\n    height : int\n        The figure's height.\n\n    width : int\n        The figure's width.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; from timecave.validation_methods.CV import AdaptedhvBlockCV\n    &gt;&gt;&gt; ts = np.ones(100);\n    &gt;&gt;&gt; splitter = AdaptedhvBlockCV(5, ts, h=5);\n    &gt;&gt;&gt; splitter.plot(10, 10);\n\n    ![block_plot](../../../images/AdaptedHV_plot.png)\n    \"\"\"\n\n    fig, axs = plt.subplots(self.n_splits, 1, sharex=True)\n    fig.set_figheight(height)\n    fig.set_figwidth(width)\n    fig.supxlabel(\"Samples\")\n    fig.supylabel(\"Time Series\")\n    fig.suptitle(\"Adapted hv Block CV method\")\n\n    for it, (training, validation, w) in enumerate(self.split()):\n\n        axs[it].scatter(training, self._series[training], label=\"Training set\")\n        axs[it].scatter(\n            validation, self._series[validation], label=\"Validation set\"\n        )\n        axs[it].set_title(\"Fold: {}\".format(it + 1))\n        axs[it].set_ylim([self._series.min() - 1, self._series.max() + 1])\n        axs[it].set_xlim([- 1, self._n_samples + 1])\n        axs[it].legend()\n\n    plt.show()\n\n    return\n</code></pre>"},{"location":"API_ref/validation_methods/CV/adapted_hv/#timecave.validation_methods.CV.AdaptedhvBlockCV.split","title":"<code>split()</code>","text":"<p>Split the time series into training and validation sets.</p> <p>This method splits the series' indices into disjoint sets containing the training and validation indices. At every iteration, an array of training indices and another one containing the validation indices are generated. Note that this method is a generator. To access the indices, use the <code>next()</code> method or a <code>for</code> loop.</p> <p>Yields:</p> Type Description <code>ndarray</code> <p>Array of training indices.</p> <code>ndarray</code> <p>Array of validation indices.</p> <code>float</code> <p>Weight assigned the error estimate.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from timecave.validation_methods.CV import AdaptedhvBlockCV\n&gt;&gt;&gt; ts = np.ones(10);\n&gt;&gt;&gt; splitter = AdaptedhvBlockCV(5, ts); # Split the data into 5 different folds\n&gt;&gt;&gt; for ind, (train, val, _) in enumerate(splitter.split()):\n... \n...     print(f\"Iteration {ind+1}\");\n...     print(f\"Training set indices: {train}\");\n...     print(f\"Validation set indices: {val}\");\nIteration 1\nTraining set indices: [3 4 5 6 7 8 9]\nValidation set indices: [0 1]\nIteration 2\nTraining set indices: [0 5 6 7 8 9]\nValidation set indices: [2 3]\nIteration 3\nTraining set indices: [0 1 2 7 8 9]\nValidation set indices: [4 5]\nIteration 4\nTraining set indices: [0 1 2 3 4 9]\nValidation set indices: [6 7]\nIteration 5\nTraining set indices: [0 1 2 3 4 5 6]\nValidation set indices: [8 9]\n</code></pre> <p>If the number of samples is not divisible by the number of folds, the first folds will contain more samples:</p> <pre><code>&gt;&gt;&gt; ts2 = np.ones(17);\n&gt;&gt;&gt; splitter = AdaptedhvBlockCV(5, ts2, h=2);\n&gt;&gt;&gt; for ind, (train, val, _) in enumerate(splitter.split()):\n... \n...     print(f\"Iteration {ind+1}\");\n...     print(f\"Training set indices: {train}\");\n...     print(f\"Validation set indices: {val}\");\nIteration 1\nTraining set indices: [ 6  7  8  9 10 11 12 13 14 15 16]\nValidation set indices: [0 1 2 3]\nIteration 2\nTraining set indices: [ 0  1 10 11 12 13 14 15 16]\nValidation set indices: [4 5 6 7]\nIteration 3\nTraining set indices: [ 0  1  2  3  4  5 13 14 15 16]\nValidation set indices: [ 8  9 10]\nIteration 4\nTraining set indices: [ 0  1  2  3  4  5  6  7  8 16]\nValidation set indices: [11 12 13]\nIteration 5\nTraining set indices: [ 0  1  2  3  4  5  6  7  8  9 10 11]\nValidation set indices: [14 15 16]\n</code></pre> Source code in <code>timecave/validation_methods/CV.py</code> <pre><code>def split(self) -&gt; Generator[tuple[np.ndarray, np.ndarray, float], None, None]:\n    \"\"\"\n    Split the time series into training and validation sets.\n\n    This method splits the series' indices into disjoint sets containing the training and validation indices.\n    At every iteration, an array of training indices and another one containing the validation indices are generated.\n    Note that this method is a generator. To access the indices, use the `next()` method or a `for` loop.\n\n    Yields\n    ------\n    np.ndarray\n        Array of training indices.\n\n    np.ndarray\n        Array of validation indices.\n\n    float\n        Weight assigned the error estimate.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; from timecave.validation_methods.CV import AdaptedhvBlockCV\n    &gt;&gt;&gt; ts = np.ones(10);\n    &gt;&gt;&gt; splitter = AdaptedhvBlockCV(5, ts); # Split the data into 5 different folds\n    &gt;&gt;&gt; for ind, (train, val, _) in enumerate(splitter.split()):\n    ... \n    ...     print(f\"Iteration {ind+1}\");\n    ...     print(f\"Training set indices: {train}\");\n    ...     print(f\"Validation set indices: {val}\");\n    Iteration 1\n    Training set indices: [3 4 5 6 7 8 9]\n    Validation set indices: [0 1]\n    Iteration 2\n    Training set indices: [0 5 6 7 8 9]\n    Validation set indices: [2 3]\n    Iteration 3\n    Training set indices: [0 1 2 7 8 9]\n    Validation set indices: [4 5]\n    Iteration 4\n    Training set indices: [0 1 2 3 4 9]\n    Validation set indices: [6 7]\n    Iteration 5\n    Training set indices: [0 1 2 3 4 5 6]\n    Validation set indices: [8 9]\n\n    If the number of samples is not divisible by the number of folds, the first folds will contain more samples:\n\n    &gt;&gt;&gt; ts2 = np.ones(17);\n    &gt;&gt;&gt; splitter = AdaptedhvBlockCV(5, ts2, h=2);\n    &gt;&gt;&gt; for ind, (train, val, _) in enumerate(splitter.split()):\n    ... \n    ...     print(f\"Iteration {ind+1}\");\n    ...     print(f\"Training set indices: {train}\");\n    ...     print(f\"Validation set indices: {val}\");\n    Iteration 1\n    Training set indices: [ 6  7  8  9 10 11 12 13 14 15 16]\n    Validation set indices: [0 1 2 3]\n    Iteration 2\n    Training set indices: [ 0  1 10 11 12 13 14 15 16]\n    Validation set indices: [4 5 6 7]\n    Iteration 3\n    Training set indices: [ 0  1  2  3  4  5 13 14 15 16]\n    Validation set indices: [ 8  9 10]\n    Iteration 4\n    Training set indices: [ 0  1  2  3  4  5  6  7  8 16]\n    Validation set indices: [11 12 13]\n    Iteration 5\n    Training set indices: [ 0  1  2  3  4  5  6  7  8  9 10 11]\n    Validation set indices: [14 15 16]\n    \"\"\"\n\n    for i, (ind) in enumerate(self._splitting_ind[:-1]):\n\n        next_ind = self._splitting_ind[i + 1]\n\n        validation = self._indices[ind:next_ind]\n        h_ind = self._indices[\n            np.fmax(ind - self._h, 0) : np.fmin(\n                next_ind + self._h, self._n_samples\n            )\n        ]\n        train = np.array([el for el in self._indices if el not in h_ind])\n\n        yield (train, validation, 1.0)\n</code></pre>"},{"location":"API_ref/validation_methods/CV/adapted_hv/#timecave.validation_methods.CV.AdaptedhvBlockCV.statistics","title":"<code>statistics()</code>","text":"<p>Compute relevant statistics for both training and validation sets.</p> <p>This method computes relevant time series features, such as mean, strength-of-trend, etc. for both the whole time series, the training set and the validation set. It can and should be used to ensure that the characteristics of both the training and validation sets are, statistically speaking, similar to those of the time series one wishes to forecast. If this is not the case, using the validation method will most likely lead to a poor assessment of the model's performance.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Relevant features for the entire time series.</p> <code>DataFrame</code> <p>Relevant features for the training set.</p> <code>DataFrame</code> <p>Relevant features for the validation set.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the time series is composed of less than three samples.</p> <code>ValueError</code> <p>If the folds comprise less than two samples.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from timecave.validation_methods.CV import AdaptedhvBlockCV\n&gt;&gt;&gt; ts = np.hstack((np.ones(5), np.zeros(5)));\n&gt;&gt;&gt; splitter = AdaptedhvBlockCV(5, ts);\n&gt;&gt;&gt; ts_stats, training_stats, validation_stats = splitter.statistics();\n&gt;&gt;&gt; ts_stats\n   Mean  Median  Min  Max  Variance  P2P_amplitude  Trend_slope  Spectral_centroid  Spectral_rolloff  Spectral_entropy  Strength_of_trend  Mean_crossing_rate  Median_crossing_rate\n0   0.5     0.5  0.0  1.0      0.25            1.0    -0.151515           0.114058               0.5           0.38717            1.59099            0.111111              0.111111\n&gt;&gt;&gt; training_stats\n       Mean  Median  Min  Max  Variance  P2P_amplitude  Trend_slope  Spectral_centroid  Spectral_rolloff  Spectral_entropy  Strength_of_trend  Mean_crossing_rate  Median_crossing_rate\n0  0.285714     0.0  0.0  1.0  0.204082            1.0    -0.178571           0.146421          0.428571          0.702232           1.212183            0.166667              0.166667\n0  0.166667     0.0  0.0  1.0  0.138889            1.0    -0.142857           0.250000          0.500000          1.000000           0.931695            0.200000              0.200000\n0  0.500000     0.5  0.0  1.0  0.250000            1.0    -0.257143           0.138889          0.500000          0.455486           1.250000            0.200000              0.200000\n0  0.833333     1.0  0.0  1.0  0.138889            1.0    -0.142857           0.125000          0.500000          0.792481           0.931695            0.200000              0.200000\n0  0.714286     1.0  0.0  1.0  0.204082            1.0    -0.178571           0.094706          0.428571          0.556506           1.212183            0.166667              0.166667\n&gt;&gt;&gt; validation_stats\n   Mean  Median  Min  Max  Variance  P2P_amplitude   Trend_slope  Spectral_centroid  Spectral_rolloff  Spectral_entropy  Strength_of_trend  Mean_crossing_rate  Median_crossing_rate\n0   1.0     1.0  1.0  1.0      0.00            0.0 -7.850462e-17               0.00               0.0               0.0                inf                 0.0                   0.0\n0   1.0     1.0  1.0  1.0      0.00            0.0 -7.850462e-17               0.00               0.0               0.0                inf                 0.0                   0.0\n0   0.5     0.5  0.0  1.0      0.25            1.0 -1.000000e+00               0.25               0.5               0.0                inf                 1.0                   1.0\n0   0.0     0.0  0.0  0.0      0.00            0.0  0.000000e+00               0.00               0.0               0.0                inf                 0.0                   0.0\n0   0.0     0.0  0.0  0.0      0.00            0.0  0.000000e+00               0.00               0.0               0.0                inf                 0.0                   0.0\n</code></pre> Source code in <code>timecave/validation_methods/CV.py</code> <pre><code>def statistics(self) -&gt; tuple[pd.DataFrame]:\n    \"\"\"\n    Compute relevant statistics for both training and validation sets.\n\n    This method computes relevant time series features, such as mean, strength-of-trend, etc. for both the whole time series, the training set and the validation set.\n    It can and should be used to ensure that the characteristics of both the training and validation sets are, statistically speaking, similar to those of the time series one wishes to forecast.\n    If this is not the case, using the validation method will most likely lead to a poor assessment of the model's performance.\n\n    Returns\n    -------\n    pd.DataFrame\n        Relevant features for the entire time series.\n\n    pd.DataFrame\n        Relevant features for the training set.\n\n    pd.DataFrame\n        Relevant features for the validation set.\n\n    Raises\n    ------\n    ValueError\n        If the time series is composed of less than three samples.\n\n    ValueError\n        If the folds comprise less than two samples.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; from timecave.validation_methods.CV import AdaptedhvBlockCV\n    &gt;&gt;&gt; ts = np.hstack((np.ones(5), np.zeros(5)));\n    &gt;&gt;&gt; splitter = AdaptedhvBlockCV(5, ts);\n    &gt;&gt;&gt; ts_stats, training_stats, validation_stats = splitter.statistics();\n    &gt;&gt;&gt; ts_stats\n       Mean  Median  Min  Max  Variance  P2P_amplitude  Trend_slope  Spectral_centroid  Spectral_rolloff  Spectral_entropy  Strength_of_trend  Mean_crossing_rate  Median_crossing_rate\n    0   0.5     0.5  0.0  1.0      0.25            1.0    -0.151515           0.114058               0.5           0.38717            1.59099            0.111111              0.111111\n    &gt;&gt;&gt; training_stats\n           Mean  Median  Min  Max  Variance  P2P_amplitude  Trend_slope  Spectral_centroid  Spectral_rolloff  Spectral_entropy  Strength_of_trend  Mean_crossing_rate  Median_crossing_rate\n    0  0.285714     0.0  0.0  1.0  0.204082            1.0    -0.178571           0.146421          0.428571          0.702232           1.212183            0.166667              0.166667\n    0  0.166667     0.0  0.0  1.0  0.138889            1.0    -0.142857           0.250000          0.500000          1.000000           0.931695            0.200000              0.200000\n    0  0.500000     0.5  0.0  1.0  0.250000            1.0    -0.257143           0.138889          0.500000          0.455486           1.250000            0.200000              0.200000\n    0  0.833333     1.0  0.0  1.0  0.138889            1.0    -0.142857           0.125000          0.500000          0.792481           0.931695            0.200000              0.200000\n    0  0.714286     1.0  0.0  1.0  0.204082            1.0    -0.178571           0.094706          0.428571          0.556506           1.212183            0.166667              0.166667\n    &gt;&gt;&gt; validation_stats\n       Mean  Median  Min  Max  Variance  P2P_amplitude   Trend_slope  Spectral_centroid  Spectral_rolloff  Spectral_entropy  Strength_of_trend  Mean_crossing_rate  Median_crossing_rate\n    0   1.0     1.0  1.0  1.0      0.00            0.0 -7.850462e-17               0.00               0.0               0.0                inf                 0.0                   0.0\n    0   1.0     1.0  1.0  1.0      0.00            0.0 -7.850462e-17               0.00               0.0               0.0                inf                 0.0                   0.0\n    0   0.5     0.5  0.0  1.0      0.25            1.0 -1.000000e+00               0.25               0.5               0.0                inf                 1.0                   1.0\n    0   0.0     0.0  0.0  0.0      0.00            0.0  0.000000e+00               0.00               0.0               0.0                inf                 0.0                   0.0\n    0   0.0     0.0  0.0  0.0      0.00            0.0  0.000000e+00               0.00               0.0               0.0                inf                 0.0                   0.0\n    \"\"\"\n\n    if self._n_samples &lt;= 2:\n\n        raise ValueError(\n            \"Basic statistics can only be computed if the time series comprises more than two samples.\"\n        )\n\n    if int(np.round(self._n_samples / self.n_splits)) &lt; 2:\n\n        raise ValueError(\n            \"The folds are too small to compute most meaningful features.\"\n        )\n\n    full_features = get_features(self._series, self.sampling_freq)\n    training_stats = []\n    validation_stats = []\n\n    for (training, validation, _) in self.split():\n\n        training_feat = get_features(self._series[training], self.sampling_freq)\n        training_stats.append(training_feat)\n\n        validation_feat = get_features(self._series[validation], self.sampling_freq)\n        validation_stats.append(validation_feat)\n\n    training_features = pd.concat(training_stats)\n    validation_features = pd.concat(validation_stats)\n\n    return (full_features, training_features, validation_features)\n</code></pre>"},{"location":"API_ref/validation_methods/CV/block/","title":"Block Cross-validation method","text":""},{"location":"API_ref/validation_methods/CV/block/#timecave.validation_methods.CV.BlockCV","title":"<code>timecave.validation_methods.CV.BlockCV(splits, ts, fs=1, weight_function=constant_weights, params=None)</code>","text":"<p>               Bases: <code>BaseSplitter</code></p> <p>Implements the Block Cross-validation method, as well as its weighted variant.</p> <p>This class implements both the Block Cross-validation method and the Weighted Block Cross-validation method.  The <code>weight_function</code> argument allows the user to implement the latter in a convenient way.</p> <p>Parameters:</p> Name Type Description Default <code>splits</code> <code>int</code> <p>The number of folds used to partition the data.</p> required <code>ts</code> <code>ndarray | Series</code> <p>Univariate time series.</p> required <code>fs</code> <code>float | int</code> <p>Sampling frequency (Hz).</p> <code>1</code> <code>weight_function</code> <code>callable</code> <p>Fold weighting function. Check the weights module for more details.</p> <code>constant_weights</code> <code>params</code> <code>dict</code> <p>Parameters to be passed to the weighting functions.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>n_splits</code> <code>int</code> <p>The number of splits.</p> <code>sampling_freq</code> <code>int | float</code> <p>The series' sampling frequency (Hz).</p> <p>Methods:</p> Name Description <code>split</code> <p>Split the time series into training and validation sets.</p> <code>info</code> <p>Provide additional information on the validation method.</p> <code>statistics</code> <p>Compute relevant statistics for both training and validation sets.</p> <code>plot</code> <p>Plot the partitioned time series.</p> See also <p>hv Block CV: A blend of Block CV and leave-one-out CV.</p> <p>Adapted hv Block CV: Similar to Block CV, but the training samples that lie closest to the validation set are removed.</p> Notes <p>The Block Cross-validation method splits the data into \\(N\\) different folds. Then, in every iteration \\(i\\), the model is validated on data from the \\(i^{th}\\) folds and trained on data from the remaining folds. The average error on the validation sets  is then taken as the estimate of the model's true error. This method does not preserve the temporal order of the observations.</p> <p></p> <p>It is reasonable to assume that when the model is validated on more recent data, the error estimate will be more accurate.  To address this issue, one may use a weighted average to compute the final estimate of the error, with larger weights being assigned to the estimates obtained     using models validated on more recent data. For more details on this method, the reader should refer to [1] or [2].</p> References Source code in <code>timecave/validation_methods/CV.py</code> <pre><code>def __init__(\n    self,\n    splits: int,\n    ts: np.ndarray | pd.Series,\n    fs: float | int = 1,\n    weight_function: callable = constant_weights,\n    params: dict = None,\n) -&gt; None:\n\n    super().__init__(splits, ts, fs)\n    self._splitting_ind = self._split_ind()\n    self._weights = weight_function(self.n_splits, params=params)\n</code></pre>"},{"location":"API_ref/validation_methods/CV/block/#timecave.validation_methods.CV.BlockCV--1","title":"1","text":"<p>Christoph Bergmeir and Jos\u00e9 M Ben\u00edtez. On the use of cross-validation for time series predictor evaluation. Information Sciences, 191:192\u2013213, 2012.</p>"},{"location":"API_ref/validation_methods/CV/block/#timecave.validation_methods.CV.BlockCV--2","title":"2","text":"<p>Vitor Cerqueira, Luis Torgo, and Igor Mozeti\u02c7c. Evaluating time series forecasting models: An empirical study on performance estimation methods. Machine Learning, 109(11):1997\u20132028, 2020.</p>"},{"location":"API_ref/validation_methods/CV/block/#timecave.validation_methods.CV.BlockCV.info","title":"<code>info()</code>","text":"<p>Provide some basic information on the training and validation sets.</p> <p>This method displays the number of splits, the fold size,  and the weights that will be used to compute the error estimate.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from timecave.validation_methods.CV import BlockCV\n&gt;&gt;&gt; ts = np.ones(10);\n&gt;&gt;&gt; splitter = BlockCV(5, ts);\n&gt;&gt;&gt; splitter.info();\nBlock CV method\n---------------\nTime series size: 10 samples\nNumber of splits: 5\nFold size: 2 to 2 samples (20.0 to 20.0 %)\nWeights: [1. 1. 1. 1. 1.]\n</code></pre> Source code in <code>timecave/validation_methods/CV.py</code> <pre><code>def info(self) -&gt; None:\n    \"\"\"\n    Provide some basic information on the training and validation sets.\n\n    This method displays the number of splits, the fold size, \n    and the weights that will be used to compute the error estimate.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; from timecave.validation_methods.CV import BlockCV\n    &gt;&gt;&gt; ts = np.ones(10);\n    &gt;&gt;&gt; splitter = BlockCV(5, ts);\n    &gt;&gt;&gt; splitter.info();\n    Block CV method\n    ---------------\n    Time series size: 10 samples\n    Number of splits: 5\n    Fold size: 2 to 2 samples (20.0 to 20.0 %)\n    Weights: [1. 1. 1. 1. 1.]\n    \"\"\"\n\n    min_fold_size = int(np.floor(self._n_samples / self.n_splits))\n    max_fold_size = min_fold_size\n\n    remainder = self._n_samples % self.n_splits\n\n    if remainder != 0:\n\n        max_fold_size += 1\n\n    min_fold_size_pct = np.round(min_fold_size / self._n_samples * 100, 2)\n    max_fold_size_pct = np.round(max_fold_size / self._n_samples * 100, 2)\n\n    print(\"Block CV method\")\n    print(\"---------------\")\n    print(f\"Time series size: {self._n_samples} samples\")\n    print(f\"Number of splits: {self.n_splits}\")\n    print(\n        f\"Fold size: {min_fold_size} to {max_fold_size} samples ({min_fold_size_pct} to {max_fold_size_pct} %)\"\n    )\n    print(f\"Weights: {np.round(self._weights, 3)}\")\n\n    return\n</code></pre>"},{"location":"API_ref/validation_methods/CV/block/#timecave.validation_methods.CV.BlockCV.plot","title":"<code>plot(height, width)</code>","text":"<p>Plot the partitioned time series.</p> <p>This method allows the user to plot the partitioned time series. The training and validation sets are plotted using different colours.</p> <p>Parameters:</p> Name Type Description Default <code>height</code> <code>int</code> <p>The figure's height.</p> required <code>width</code> <code>int</code> <p>The figure's width.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from timecave.validation_methods.CV import BlockCV\n&gt;&gt;&gt; ts = np.ones(100);\n&gt;&gt;&gt; splitter = BlockCV(5, ts);\n&gt;&gt;&gt; splitter.plot(10, 10);\n</code></pre> <p></p> Source code in <code>timecave/validation_methods/CV.py</code> <pre><code>def plot(self, height: int, width: int) -&gt; None:\n    \"\"\"\n    Plot the partitioned time series.\n\n    This method allows the user to plot the partitioned time series. The training and validation sets are plotted using different colours.\n\n    Parameters\n    ----------\n    height : int\n        The figure's height.\n\n    width : int\n        The figure's width.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; from timecave.validation_methods.CV import BlockCV\n    &gt;&gt;&gt; ts = np.ones(100);\n    &gt;&gt;&gt; splitter = BlockCV(5, ts);\n    &gt;&gt;&gt; splitter.plot(10, 10);\n\n    ![block_plot](../../../images/BlockCV_plot.png)\n    \"\"\"\n\n    fig, axs = plt.subplots(self.n_splits, 1, sharex=True)\n    fig.set_figheight(height)\n    fig.set_figwidth(width)\n    fig.supxlabel(\"Samples\")\n    fig.supylabel(\"Time Series\")\n    fig.suptitle(\"Block CV method\")\n\n    for it, (training, validation, w) in enumerate(self.split()):\n\n        axs[it].scatter(training, self._series[training], label=\"Training set\")\n        axs[it].scatter(\n            validation, self._series[validation], label=\"Validation set\"\n        )\n        axs[it].set_title(\"Fold: {} Weight: {}\".format(it + 1, np.round(w, 3)))\n        axs[it].set_ylim([self._series.min() - 1, self._series.max() + 1])\n        axs[it].set_xlim([- 1, self._n_samples + 1])\n        axs[it].legend()\n\n    plt.show()\n\n    return\n</code></pre>"},{"location":"API_ref/validation_methods/CV/block/#timecave.validation_methods.CV.BlockCV.split","title":"<code>split()</code>","text":"<p>Split the time series into training and validation sets.</p> <p>This method splits the series' indices into disjoint sets containing the training and validation indices. At every iteration, an array of training indices and another one containing the validation indices are generated. Note that this method is a generator. To access the indices, use the <code>next()</code> method or a <code>for</code> loop.</p> <p>Yields:</p> Type Description <code>ndarray</code> <p>Array of training indices.</p> <code>ndarray</code> <p>Array of validation indices.</p> <code>float</code> <p>Weight assigned to the error estimate.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from timecave.validation_methods.CV import BlockCV\n&gt;&gt;&gt; ts = np.ones(10);\n&gt;&gt;&gt; splitter = BlockCV(5, ts); # Split the data into 5 different folds\n&gt;&gt;&gt; for ind, (train, val, _) in enumerate(splitter.split()):\n... \n...     print(f\"Iteration {ind+1}\");\n...     print(f\"Training set indices: {train}\");\n...     print(f\"Validation set indices: {val}\");\nIteration 1\nTraining set indices: [2 3 4 5 6 7 8 9]\nValidation set indices: [0 1]\nIteration 2\nTraining set indices: [0 1 4 5 6 7 8 9]\nValidation set indices: [2 3]\nIteration 3\nTraining set indices: [0 1 2 3 6 7 8 9]\nValidation set indices: [4 5]\nIteration 4\nTraining set indices: [0 1 2 3 4 5 8 9]\nValidation set indices: [6 7]\nIteration 5\nTraining set indices: [0 1 2 3 4 5 6 7]\nValidation set indices: [8 9]\n</code></pre> <p>If the number of samples is not divisible by the number of folds, the first folds will contain more samples:</p> <pre><code>&gt;&gt;&gt; ts2 = np.ones(17);\n&gt;&gt;&gt; splitter = BlockCV(5, ts2);\n&gt;&gt;&gt; for ind, (train, val, _) in enumerate(splitter.split()):\n... \n...     print(f\"Iteration {ind+1}\");\n...     print(f\"Training set indices: {train}\");\n...     print(f\"Validation set indices: {val}\");\nIteration 1\nTraining set indices: [ 4  5  6  7  8  9 10 11 12 13 14 15 16]\nValidation set indices: [0 1 2 3]\nIteration 2\nTraining set indices: [ 0  1  2  3  8  9 10 11 12 13 14 15 16]\nValidation set indices: [4 5 6 7]\nIteration 3\nTraining set indices: [ 0  1  2  3  4  5  6  7 11 12 13 14 15 16]\nValidation set indices: [ 8  9 10]\nIteration 4\nTraining set indices: [ 0  1  2  3  4  5  6  7  8  9 10 14 15 16]\nValidation set indices: [11 12 13]\nIteration 5\nTraining set indices: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13]\nValidation set indices: [14 15 16]\n</code></pre> <p>Weights can be assigned to the error estimates (Weighted Rolling Window method).  The parameters for the weighting functions must be passed to the class constructor:</p> <pre><code>&gt;&gt;&gt; from timecave.validation_methods.weights import exponential_weights\n&gt;&gt;&gt; splitter = BlockCV(5, ts, weight_function=exponential_weights, params={\"base\": 2});\n&gt;&gt;&gt; for ind, (train, val, weight) in enumerate(splitter.split()):\n... \n...     print(f\"Iteration {ind+1}\");\n...     print(f\"Training set indices: {train}\");\n...     print(f\"Validation set indices: {val}\");\n...     print(f\"Weight: {np.round(weight, 3)}\");\nIteration 1\nTraining set indices: [2 3 4 5 6 7 8 9]\nValidation set indices: [0 1]\nWeight: 0.032\nIteration 2\nTraining set indices: [0 1 4 5 6 7 8 9]\nValidation set indices: [2 3]\nWeight: 0.065\nIteration 3\nTraining set indices: [0 1 2 3 6 7 8 9]\nValidation set indices: [4 5]\nWeight: 0.129\nIteration 4\nTraining set indices: [0 1 2 3 4 5 8 9]\nValidation set indices: [6 7]\nWeight: 0.258\nIteration 5\nTraining set indices: [0 1 2 3 4 5 6 7]\nValidation set indices: [8 9]\nWeight: 0.516\n</code></pre> Source code in <code>timecave/validation_methods/CV.py</code> <pre><code>def split(self) -&gt; Generator[tuple[np.ndarray, np.ndarray, float], None, None]:\n    \"\"\"\n    Split the time series into training and validation sets.\n\n    This method splits the series' indices into disjoint sets containing the training and validation indices.\n    At every iteration, an array of training indices and another one containing the validation indices are generated.\n    Note that this method is a generator. To access the indices, use the `next()` method or a `for` loop.\n\n    Yields\n    ------\n    np.ndarray\n        Array of training indices.\n\n    np.ndarray\n        Array of validation indices.\n\n    float\n        Weight assigned to the error estimate.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; from timecave.validation_methods.CV import BlockCV\n    &gt;&gt;&gt; ts = np.ones(10);\n    &gt;&gt;&gt; splitter = BlockCV(5, ts); # Split the data into 5 different folds\n    &gt;&gt;&gt; for ind, (train, val, _) in enumerate(splitter.split()):\n    ... \n    ...     print(f\"Iteration {ind+1}\");\n    ...     print(f\"Training set indices: {train}\");\n    ...     print(f\"Validation set indices: {val}\");\n    Iteration 1\n    Training set indices: [2 3 4 5 6 7 8 9]\n    Validation set indices: [0 1]\n    Iteration 2\n    Training set indices: [0 1 4 5 6 7 8 9]\n    Validation set indices: [2 3]\n    Iteration 3\n    Training set indices: [0 1 2 3 6 7 8 9]\n    Validation set indices: [4 5]\n    Iteration 4\n    Training set indices: [0 1 2 3 4 5 8 9]\n    Validation set indices: [6 7]\n    Iteration 5\n    Training set indices: [0 1 2 3 4 5 6 7]\n    Validation set indices: [8 9]\n\n    If the number of samples is not divisible by the number of folds, the first folds will contain more samples:\n\n    &gt;&gt;&gt; ts2 = np.ones(17);\n    &gt;&gt;&gt; splitter = BlockCV(5, ts2);\n    &gt;&gt;&gt; for ind, (train, val, _) in enumerate(splitter.split()):\n    ... \n    ...     print(f\"Iteration {ind+1}\");\n    ...     print(f\"Training set indices: {train}\");\n    ...     print(f\"Validation set indices: {val}\");\n    Iteration 1\n    Training set indices: [ 4  5  6  7  8  9 10 11 12 13 14 15 16]\n    Validation set indices: [0 1 2 3]\n    Iteration 2\n    Training set indices: [ 0  1  2  3  8  9 10 11 12 13 14 15 16]\n    Validation set indices: [4 5 6 7]\n    Iteration 3\n    Training set indices: [ 0  1  2  3  4  5  6  7 11 12 13 14 15 16]\n    Validation set indices: [ 8  9 10]\n    Iteration 4\n    Training set indices: [ 0  1  2  3  4  5  6  7  8  9 10 14 15 16]\n    Validation set indices: [11 12 13]\n    Iteration 5\n    Training set indices: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13]\n    Validation set indices: [14 15 16]\n\n    Weights can be assigned to the error estimates (Weighted Rolling Window method). \n    The parameters for the weighting functions must be passed to the class constructor:\n\n    &gt;&gt;&gt; from timecave.validation_methods.weights import exponential_weights\n    &gt;&gt;&gt; splitter = BlockCV(5, ts, weight_function=exponential_weights, params={\"base\": 2});\n    &gt;&gt;&gt; for ind, (train, val, weight) in enumerate(splitter.split()):\n    ... \n    ...     print(f\"Iteration {ind+1}\");\n    ...     print(f\"Training set indices: {train}\");\n    ...     print(f\"Validation set indices: {val}\");\n    ...     print(f\"Weight: {np.round(weight, 3)}\");\n    Iteration 1\n    Training set indices: [2 3 4 5 6 7 8 9]\n    Validation set indices: [0 1]\n    Weight: 0.032\n    Iteration 2\n    Training set indices: [0 1 4 5 6 7 8 9]\n    Validation set indices: [2 3]\n    Weight: 0.065\n    Iteration 3\n    Training set indices: [0 1 2 3 6 7 8 9]\n    Validation set indices: [4 5]\n    Weight: 0.129\n    Iteration 4\n    Training set indices: [0 1 2 3 4 5 8 9]\n    Validation set indices: [6 7]\n    Weight: 0.258\n    Iteration 5\n    Training set indices: [0 1 2 3 4 5 6 7]\n    Validation set indices: [8 9]\n    Weight: 0.516\n    \"\"\"\n\n    for i, (ind, weight) in enumerate(zip(self._splitting_ind[:-1], self._weights)):\n\n        next_ind = self._splitting_ind[i + 1]\n\n        validation = self._indices[ind:next_ind]\n        train = np.array([el for el in self._indices if el not in validation])\n\n        yield (train, validation, weight)\n</code></pre>"},{"location":"API_ref/validation_methods/CV/block/#timecave.validation_methods.CV.BlockCV.statistics","title":"<code>statistics()</code>","text":"<p>Compute relevant statistics for both training and validation sets.</p> <p>This method computes relevant time series features, such as mean, strength-of-trend, etc. for both the whole time series, the training set and the validation set. It can and should be used to ensure that the characteristics of both the training and validation sets are, statistically speaking, similar to those of the time series one wishes to forecast. If this is not the case, using the validation method will most likely lead to a poor assessment of the model's performance.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Relevant features for the entire time series.</p> <code>DataFrame</code> <p>Relevant features for the training set.</p> <code>DataFrame</code> <p>Relevant features for the validation set.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the time series is composed of less than three samples.</p> <code>ValueError</code> <p>If the folds comprise less than two samples.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from timecave.validation_methods.CV import BlockCV\n&gt;&gt;&gt; ts = np.hstack((np.ones(5), np.zeros(5)));\n&gt;&gt;&gt; splitter = BlockCV(5, ts);\n&gt;&gt;&gt; ts_stats, training_stats, validation_stats = splitter.statistics();\nFrequency features are only meaningful if the correct sampling frequency is passed to the class.\n&gt;&gt;&gt; ts_stats\n   Mean  Median  Min  Max  Variance  P2P_amplitude  Trend_slope  Spectral_centroid  Spectral_rolloff  Spectral_entropy  Strength_of_trend  Mean_crossing_rate  Median_crossing_rate\n0   0.5     0.5  0.0  1.0      0.25            1.0    -0.151515           0.114058               0.5           0.38717            1.59099            0.111111              0.111111\n&gt;&gt;&gt; training_stats\n    Mean  Median  Min  Max  Variance  P2P_amplitude  Trend_slope  Spectral_centroid  Spectral_rolloff  Spectral_entropy  Strength_of_trend  Mean_crossing_rate  Median_crossing_rate\n0  0.375     0.0  0.0  1.0  0.234375            1.0    -0.178571           0.154195             0.500          0.600876           1.383496            0.142857              0.142857\n0  0.375     0.0  0.0  1.0  0.234375            1.0    -0.178571           0.154195             0.500          0.600876           1.383496            0.142857              0.142857\n0  0.500     0.5  0.0  1.0  0.250000            1.0    -0.190476           0.095190             0.375          0.600876           1.428869            0.142857              0.142857\n0  0.625     1.0  0.0  1.0  0.234375            1.0    -0.178571           0.122818             0.500          0.600876           1.383496            0.142857              0.142857\n0  0.625     1.0  0.0  1.0  0.234375            1.0    -0.178571           0.122818             0.500          0.600876           1.383496            0.142857              0.142857\n&gt;&gt;&gt; validation_stats\n   Mean  Median  Min  Max  Variance  P2P_amplitude   Trend_slope  Spectral_centroid  Spectral_rolloff  Spectral_entropy  Strength_of_trend  Mean_crossing_rate  Median_crossing_rate\n0   1.0     1.0  1.0  1.0      0.00            0.0 -7.850462e-17               0.00               0.0               0.0                inf                 0.0                   0.0\n0   1.0     1.0  1.0  1.0      0.00            0.0 -7.850462e-17               0.00               0.0               0.0                inf                 0.0                   0.0\n0   0.5     0.5  0.0  1.0      0.25            1.0 -1.000000e+00               0.25               0.5               0.0                inf                 1.0                   1.0\n0   0.0     0.0  0.0  0.0      0.00            0.0  0.000000e+00               0.00               0.0               0.0                inf                 0.0                   0.0\n0   0.0     0.0  0.0  0.0      0.00            0.0  0.000000e+00               0.00               0.0               0.0                inf                 0.0                   0.0\n</code></pre> Source code in <code>timecave/validation_methods/CV.py</code> <pre><code>def statistics(self) -&gt; tuple[pd.DataFrame]:\n    \"\"\"\n    Compute relevant statistics for both training and validation sets.\n\n    This method computes relevant time series features, such as mean, strength-of-trend, etc. for both the whole time series, the training set and the validation set.\n    It can and should be used to ensure that the characteristics of both the training and validation sets are, statistically speaking, similar to those of the time series one wishes to forecast.\n    If this is not the case, using the validation method will most likely lead to a poor assessment of the model's performance.\n\n    Returns\n    -------\n    pd.DataFrame\n        Relevant features for the entire time series.\n\n    pd.DataFrame\n        Relevant features for the training set.\n\n    pd.DataFrame\n        Relevant features for the validation set.\n\n    Raises\n    ------\n    ValueError\n        If the time series is composed of less than three samples.\n\n    ValueError\n        If the folds comprise less than two samples.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; from timecave.validation_methods.CV import BlockCV\n    &gt;&gt;&gt; ts = np.hstack((np.ones(5), np.zeros(5)));\n    &gt;&gt;&gt; splitter = BlockCV(5, ts);\n    &gt;&gt;&gt; ts_stats, training_stats, validation_stats = splitter.statistics();\n    Frequency features are only meaningful if the correct sampling frequency is passed to the class.\n    &gt;&gt;&gt; ts_stats\n       Mean  Median  Min  Max  Variance  P2P_amplitude  Trend_slope  Spectral_centroid  Spectral_rolloff  Spectral_entropy  Strength_of_trend  Mean_crossing_rate  Median_crossing_rate\n    0   0.5     0.5  0.0  1.0      0.25            1.0    -0.151515           0.114058               0.5           0.38717            1.59099            0.111111              0.111111\n    &gt;&gt;&gt; training_stats\n        Mean  Median  Min  Max  Variance  P2P_amplitude  Trend_slope  Spectral_centroid  Spectral_rolloff  Spectral_entropy  Strength_of_trend  Mean_crossing_rate  Median_crossing_rate\n    0  0.375     0.0  0.0  1.0  0.234375            1.0    -0.178571           0.154195             0.500          0.600876           1.383496            0.142857              0.142857\n    0  0.375     0.0  0.0  1.0  0.234375            1.0    -0.178571           0.154195             0.500          0.600876           1.383496            0.142857              0.142857\n    0  0.500     0.5  0.0  1.0  0.250000            1.0    -0.190476           0.095190             0.375          0.600876           1.428869            0.142857              0.142857\n    0  0.625     1.0  0.0  1.0  0.234375            1.0    -0.178571           0.122818             0.500          0.600876           1.383496            0.142857              0.142857\n    0  0.625     1.0  0.0  1.0  0.234375            1.0    -0.178571           0.122818             0.500          0.600876           1.383496            0.142857              0.142857\n    &gt;&gt;&gt; validation_stats\n       Mean  Median  Min  Max  Variance  P2P_amplitude   Trend_slope  Spectral_centroid  Spectral_rolloff  Spectral_entropy  Strength_of_trend  Mean_crossing_rate  Median_crossing_rate\n    0   1.0     1.0  1.0  1.0      0.00            0.0 -7.850462e-17               0.00               0.0               0.0                inf                 0.0                   0.0\n    0   1.0     1.0  1.0  1.0      0.00            0.0 -7.850462e-17               0.00               0.0               0.0                inf                 0.0                   0.0\n    0   0.5     0.5  0.0  1.0      0.25            1.0 -1.000000e+00               0.25               0.5               0.0                inf                 1.0                   1.0\n    0   0.0     0.0  0.0  0.0      0.00            0.0  0.000000e+00               0.00               0.0               0.0                inf                 0.0                   0.0\n    0   0.0     0.0  0.0  0.0      0.00            0.0  0.000000e+00               0.00               0.0               0.0                inf                 0.0                   0.0\n    \"\"\"\n\n    if self._n_samples &lt;= 2:\n\n        raise ValueError(\n            \"Basic statistics can only be computed if the time series comprises more than two samples.\"\n        )\n\n    if int(np.round(self._n_samples / self.n_splits)) &lt; 2:\n\n        raise ValueError(\n            \"The folds are too small to compute most meaningful features.\"\n        )\n\n    print(\"Frequency features are only meaningful if the correct sampling frequency is passed to the class.\")\n\n    full_features = get_features(self._series, self.sampling_freq)\n    training_stats = []\n    validation_stats = []\n\n    for (training, validation, _) in self.split():\n\n        training_feat = get_features(self._series[training], self.sampling_freq)\n        training_stats.append(training_feat)\n\n        validation_feat = get_features(self._series[validation], self.sampling_freq)\n        validation_stats.append(validation_feat)\n\n    training_features = pd.concat(training_stats)\n    validation_features = pd.concat(validation_stats)\n\n    return (full_features, training_features, validation_features)\n</code></pre>"},{"location":"API_ref/validation_methods/CV/hv/","title":"hv Block Cross-validation method","text":""},{"location":"API_ref/validation_methods/CV/hv/#timecave.validation_methods.CV.hvBlockCV","title":"<code>timecave.validation_methods.CV.hvBlockCV(ts, fs=1, h=0, v=0)</code>","text":"<p>               Bases: <code>BaseSplitter</code></p> <p>Implements the hv Block Cross-validation method.</p> <p>This class implements the hv Block Cross-validation method. It is similar to the BlockCV class,  but it does not support weight generation. Consequently, in order to implement a weighted version of this method,  the user must implement their own derived class or compute the weights separately.</p> <p>Parameters:</p> Name Type Description Default <code>ts</code> <code>ndarray | Series</code> <p>Univariate time series.</p> required <code>fs</code> <code>float | int</code> <p>Sampling frequency (Hz).</p> <code>1</code> <code>h</code> <code>int</code> <p>Controls the amount of samples that will be removed from the training set.  The <code>h</code> samples immediately following and preceding the validation set are not used for training.</p> <code>1</code> <code>v</code> <code>int</code> <p>Controls the size of the validation set. \\(2v + 1\\) samples will be used for validation.</p> <code>1</code> <p>Attributes:</p> Name Type Description <code>n_splits</code> <code>int</code> <p>The number of splits.</p> <code>sampling_freq</code> <code>int | float</code> <p>The series' sampling frequency (Hz).</p> <p>Methods:</p> Name Description <code>split</code> <p>Split the time series into training and validation sets.</p> <code>info</code> <p>Provide additional information on the validation method.</p> <code>statistics</code> <p>Compute relevant statistics for both training and validation sets.</p> <code>plot</code> <p>Plot the partitioned time series.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If either <code>h</code> or <code>v</code> are not integers.</p> <code>ValueError</code> <p>If either <code>h</code> or <code>v</code> are smaller than or equal to zero.</p> <code>ValueError</code> <p>If the sum of <code>h</code> and <code>v</code> is larger than half the amount of samples in the series. </p> Warning <p>Being a variant of the leave-one-out CV procedure, this method is computationally intensive.</p> See also <p>Block CV: The original Block CV method, which partitions the series into equally sized folds. No training samples are removed.</p> Notes <p>The hv Block Cross-validation method is essentially a leave-one-out version of the BlockCV method. There are, however, two nuances: the first one is that the \\(h\\) samples immediately following and preceding the validation set  are removed from the training set; the second one is that more than one sample can be used for validation. More specifically, the validation set comprises \\(2v + 1\\) samples. Note that, if \\(h = v = 0\\), the method boils down to the classic leave-one-out cross-validation procedure. The average error on the validation sets is taken as the estimate of the model's true error. This method does not preserve the temporal order of the observations.</p> <p>The method was first proposed by Racine [1].</p> References Source code in <code>timecave/validation_methods/CV.py</code> <pre><code>def __init__(\n    self,\n    ts: np.ndarray | pd.Series,\n    fs: float | int = 1,\n    h: int = 0,\n    v: int = 0,\n) -&gt; None:\n\n    super().__init__(ts.shape[0], ts, fs)\n    self._check_hv(h, v)\n    self._h = h\n    self._v = v\n\n    return\n</code></pre>"},{"location":"API_ref/validation_methods/CV/hv/#timecave.validation_methods.CV.hvBlockCV--1","title":"1","text":"<p>Jeff Racine. Consistent cross-validatory model-selection for dependent data:  hv-block cross-validation. Journal of econometrics, 99(1):39\u201361, 2000</p>"},{"location":"API_ref/validation_methods/CV/hv/#timecave.validation_methods.CV.hvBlockCV.info","title":"<code>info()</code>","text":"<p>Provide some basic information on the training and validation sets.</p> <p>This method displays the number of splits, the values of the <code>h</code> and <code>v</code>  parameters, and the maximum and minimum sizes of both the training and validation sets.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from timecave.validation_methods.CV import hvBlockCV\n&gt;&gt;&gt; ts = np.ones(10);\n&gt;&gt;&gt; splitter = hvBlockCV(ts, h=2, v=2);\n&gt;&gt;&gt; splitter.info();\nhv-Block CV method\n------------------\nTime series size: 10 samples\nNumber of splits: 10\nMinimum training set size: 1 samples (10.0 %)\nMaximum training set size: 5 samples (50.0 %)\nMinimum validation set size: 3 samples (30.0 %)\nMaximum validation set size: 5 samples (50.0 %)\nh: 2\nv: 2\n</code></pre> Source code in <code>timecave/validation_methods/CV.py</code> <pre><code>def info(self) -&gt; None:\n    \"\"\"\n    Provide some basic information on the training and validation sets.\n\n    This method displays the number of splits, the values of the `h` and `v` \n    parameters, and the maximum and minimum sizes of both the training and validation sets.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; from timecave.validation_methods.CV import hvBlockCV\n    &gt;&gt;&gt; ts = np.ones(10);\n    &gt;&gt;&gt; splitter = hvBlockCV(ts, h=2, v=2);\n    &gt;&gt;&gt; splitter.info();\n    hv-Block CV method\n    ------------------\n    Time series size: 10 samples\n    Number of splits: 10\n    Minimum training set size: 1 samples (10.0 %)\n    Maximum training set size: 5 samples (50.0 %)\n    Minimum validation set size: 3 samples (30.0 %)\n    Maximum validation set size: 5 samples (50.0 %)\n    h: 2\n    v: 2\n    \"\"\"\n\n    min_train_size = self._n_samples - 2 * (self._h + self._v + 1) + 1\n    max_train_size = self._n_samples - self._h - self._v - 1\n    min_val_size = self._v + 1\n    max_val_size = 2 * self._v + 1\n\n    min_train_pct = np.round(min_train_size / self._n_samples * 100, 2)\n    max_train_pct = np.round(max_train_size / self._n_samples * 100, 2)\n    min_val_pct = np.round(min_val_size / self._n_samples * 100, 2)\n    max_val_pct = np.round(max_val_size / self._n_samples * 100, 2)\n\n    print(\"hv-Block CV method\")\n    print(\"------------------\")\n    print(f\"Time series size: {self._n_samples} samples\")\n    print(f\"Number of splits: {self.n_splits}\")\n    print(\n        f\"Minimum training set size: {min_train_size} samples ({min_train_pct} %)\"\n    )\n    print(\n        f\"Maximum training set size: {max_train_size} samples ({max_train_pct} %)\"\n    )\n    print(f\"Minimum validation set size: {min_val_size} samples ({min_val_pct} %)\")\n    print(f\"Maximum validation set size: {max_val_size} samples ({max_val_pct} %)\")\n    print(f\"h: {self._h}\")\n    print(f\"v: {self._v}\")\n\n    return\n</code></pre>"},{"location":"API_ref/validation_methods/CV/hv/#timecave.validation_methods.CV.hvBlockCV.plot","title":"<code>plot(height, width)</code>","text":"<p>Plot the partitioned time series.</p> <p>This method allows the user to plot the partitioned time series. The training and validation sets are plotted using different colours.</p> <p>Parameters:</p> Name Type Description Default <code>height</code> <code>int</code> <p>The figure's height.</p> required <code>width</code> <code>int</code> <p>The figure's width.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from timecave.validation_methods.CV import hvBlockCV\n&gt;&gt;&gt; ts = np.ones(6);\n&gt;&gt;&gt; splitter = hvBlockCV(ts, h=1, v=1);\n&gt;&gt;&gt; splitter.plot(10, 10);\n</code></pre> <p></p> Source code in <code>timecave/validation_methods/CV.py</code> <pre><code>def plot(self, height: int, width: int) -&gt; None:\n    \"\"\"\n    Plot the partitioned time series.\n\n    This method allows the user to plot the partitioned time series. The training and validation sets are plotted using different colours.\n\n    Parameters\n    ----------\n    height : int\n        The figure's height.\n\n    width : int\n        The figure's width.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; from timecave.validation_methods.CV import hvBlockCV\n    &gt;&gt;&gt; ts = np.ones(6);\n    &gt;&gt;&gt; splitter = hvBlockCV(ts, h=1, v=1);\n    &gt;&gt;&gt; splitter.plot(10, 10);\n\n    ![hv](../../../images/hvBlock_plot.png)\n    \"\"\"\n\n    fig, axs = plt.subplots(self.n_splits, 1, sharex=True)\n    fig.set_figheight(height)\n    fig.set_figwidth(width)\n    fig.supxlabel(\"Samples\")\n    fig.supylabel(\"Time Series\")\n    fig.suptitle(\"hv-Block CV method\")\n\n    for it, (training, validation, _) in enumerate(self.split()):\n\n        axs[it].scatter(training, self._series[training], label=\"Training set\")\n        axs[it].scatter(\n            validation, self._series[validation], label=\"Validation set\"\n        )\n        axs[it].set_title(\"Fold {}\".format(it + 1))\n        axs[it].set_ylim([self._series.min() - 1, self._series.max() + 1])\n        axs[it].set_xlim([- 1, self._n_samples + 1])\n        axs[it].legend()\n\n    plt.show()\n\n    return\n</code></pre>"},{"location":"API_ref/validation_methods/CV/hv/#timecave.validation_methods.CV.hvBlockCV.split","title":"<code>split()</code>","text":"<p>Split the time series into training and validation sets.</p> <p>This method splits the series' indices into disjoint sets containing the training and validation indices. At every iteration, an array of training indices and another one containing the validation indices are generated. Note that this method is a generator. To access the indices, use the <code>next()</code> method or a <code>for</code> loop.</p> <p>Yields:</p> Type Description <code>ndarray</code> <p>Array of training indices.</p> <code>ndarray</code> <p>Array of validation indices.</p> <code>float</code> <p>Weight assigned to the error estimate.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from timecave.validation_methods.CV import hvBlockCV\n&gt;&gt;&gt; ts = np.ones(10);\n&gt;&gt;&gt; splitter = hvBlockCV(ts, h=2, v=1); # Use 3 samples for validation; remove 2-4 samples from the training set\n&gt;&gt;&gt; for ind, (train, val, _) in enumerate(splitter.split()):\n... \n...     print(f\"Iteration {ind+1}\");\n...     print(f\"Training set indices: {train}\");\n...     print(f\"Validation set indices: {val}\");\nIteration 1\nTraining set indices: [4 5 6 7 8 9]\nValidation set indices: [0 1]\nIteration 2\nTraining set indices: [5 6 7 8 9]\nValidation set indices: [0 1 2]\nIteration 3\nTraining set indices: [6 7 8 9]\nValidation set indices: [1 2 3]\nIteration 4\nTraining set indices: [7 8 9]\nValidation set indices: [2 3 4]\nIteration 5\nTraining set indices: [0 8 9]\nValidation set indices: [3 4 5]\nIteration 6\nTraining set indices: [0 1 9]\nValidation set indices: [4 5 6]\nIteration 7\nTraining set indices: [0 1 2]\nValidation set indices: [5 6 7]\nIteration 8\nTraining set indices: [0 1 2 3]\nValidation set indices: [6 7 8]\nIteration 9\nTraining set indices: [0 1 2 3 4]\nValidation set indices: [7 8 9]\nIteration 10\nTraining set indices: [0 1 2 3 4 5]\nValidation set indices: [8 9]\n</code></pre> Source code in <code>timecave/validation_methods/CV.py</code> <pre><code>def split(self) -&gt; Generator[tuple[np.ndarray, np.ndarray, float], None, None]:\n    \"\"\"\n    Split the time series into training and validation sets.\n\n    This method splits the series' indices into disjoint sets containing the training and validation indices.\n    At every iteration, an array of training indices and another one containing the validation indices are generated.\n    Note that this method is a generator. To access the indices, use the `next()` method or a `for` loop.\n\n    Yields\n    ------\n    np.ndarray\n        Array of training indices.\n\n    np.ndarray\n        Array of validation indices.\n\n    float\n        Weight assigned to the error estimate.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; from timecave.validation_methods.CV import hvBlockCV\n    &gt;&gt;&gt; ts = np.ones(10);\n    &gt;&gt;&gt; splitter = hvBlockCV(ts, h=2, v=1); # Use 3 samples for validation; remove 2-4 samples from the training set\n    &gt;&gt;&gt; for ind, (train, val, _) in enumerate(splitter.split()):\n    ... \n    ...     print(f\"Iteration {ind+1}\");\n    ...     print(f\"Training set indices: {train}\");\n    ...     print(f\"Validation set indices: {val}\");\n    Iteration 1\n    Training set indices: [4 5 6 7 8 9]\n    Validation set indices: [0 1]\n    Iteration 2\n    Training set indices: [5 6 7 8 9]\n    Validation set indices: [0 1 2]\n    Iteration 3\n    Training set indices: [6 7 8 9]\n    Validation set indices: [1 2 3]\n    Iteration 4\n    Training set indices: [7 8 9]\n    Validation set indices: [2 3 4]\n    Iteration 5\n    Training set indices: [0 8 9]\n    Validation set indices: [3 4 5]\n    Iteration 6\n    Training set indices: [0 1 9]\n    Validation set indices: [4 5 6]\n    Iteration 7\n    Training set indices: [0 1 2]\n    Validation set indices: [5 6 7]\n    Iteration 8\n    Training set indices: [0 1 2 3]\n    Validation set indices: [6 7 8]\n    Iteration 9\n    Training set indices: [0 1 2 3 4]\n    Validation set indices: [7 8 9]\n    Iteration 10\n    Training set indices: [0 1 2 3 4 5]\n    Validation set indices: [8 9]\n    \"\"\"\n\n    for i, _ in enumerate(self._indices):\n\n        validation = self._indices[\n            np.fmax(i - self._v, 0) : np.fmin(i + self._v + 1, self._n_samples)\n        ]\n        h_ind = self._indices[\n            np.fmax(i - self._v - self._h, 0) : np.fmin(\n                i + self._v + self._h + 1, self._n_samples\n            )\n        ]\n        train = np.array([el for el in self._indices if el not in h_ind])\n\n        yield (train, validation, 1.0)\n</code></pre>"},{"location":"API_ref/validation_methods/CV/hv/#timecave.validation_methods.CV.hvBlockCV.statistics","title":"<code>statistics()</code>","text":"<p>Compute relevant statistics for both training and validation sets.</p> <p>This method computes relevant time series features, such as mean, strength-of-trend, etc. for both the whole time series, the training set and the validation set. It can and should be used to ensure that the characteristics of both the training and validation sets are, statistically speaking, similar to those of the time series one wishes to forecast. If this is not the case, using the validation method will most likely lead to a poor assessment of the model's performance.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Relevant features for the entire time series.</p> <code>DataFrame</code> <p>Relevant features for the training set.</p> <code>DataFrame</code> <p>Relevant features for the validation set.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the time series is composed of less than three samples.</p> <code>ValueError</code> <p>If the folds comprise less than two samples.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from timecave.validation_methods.CV import hvBlockCV\n&gt;&gt;&gt; ts = np.hstack((np.ones(5), np.zeros(5)));\n&gt;&gt;&gt; splitter = hvBlockCV(ts, h=2, v=2);\n&gt;&gt;&gt; ts_stats, training_stats, validation_stats = splitter.statistics();\nFrequency features are only meaningful if the correct sampling frequency is passed to the class.\nThe training set is too small to compute most meaningful features.\nThe training set is too small to compute most meaningful features.\n&gt;&gt;&gt; ts_stats\n   Mean  Median  Min  Max  Variance  P2P_amplitude  Trend_slope  Spectral_centroid  Spectral_rolloff  Spectral_entropy  Strength_of_trend  Mean_crossing_rate  Median_crossing_rate\n0   0.5     0.5  0.0  1.0      0.25            1.0    -0.151515           0.114058               0.5           0.38717            1.59099            0.111111              0.111111\n&gt;&gt;&gt; training_stats\n   Mean  Median  Min  Max  Variance  P2P_amplitude   Trend_slope  Spectral_centroid  Spectral_rolloff  Spectral_entropy  Strength_of_trend  Mean_crossing_rate  Median_crossing_rate\n0   0.0     0.0  0.0  0.0       0.0            0.0  0.000000e+00                0.0               0.0               0.0                inf                 0.0                   0.0\n0   0.0     0.0  0.0  0.0       0.0            0.0  0.000000e+00                0.0               0.0               0.0                inf                 0.0                   0.0\n0   0.0     0.0  0.0  0.0       0.0            0.0  0.000000e+00                0.0               0.0               0.0                inf                 0.0                   0.0\n0   0.0     0.0  0.0  0.0       0.0            0.0  0.000000e+00                0.0               0.0               0.0                inf                 0.0                   0.0\n0   1.0     1.0  1.0  1.0       0.0            0.0 -7.850462e-17                0.0               0.0               0.0                inf                 0.0                   0.0\n0   1.0     1.0  1.0  1.0       0.0            0.0  8.985767e-17                0.0               0.0               0.0                inf                 0.0                   0.0\n0   1.0     1.0  1.0  1.0       0.0            0.0 -8.214890e-17                0.0               0.0               0.0                inf                 0.0                   0.0\n0   1.0     1.0  1.0  1.0       0.0            0.0 -1.050792e-16                0.0               0.0               0.0                inf                 0.0                   0.0\n&gt;&gt;&gt; validation_stats\n   Mean  Median  Min  Max  Variance  P2P_amplitude   Trend_slope  Spectral_centroid  Spectral_rolloff  Spectral_entropy  Strength_of_trend  Mean_crossing_rate  Median_crossing_rate\n0   1.0     1.0  1.0  1.0      0.00            0.0  8.985767e-17           0.000000               0.0          0.000000                inf                0.00                  0.00\n0   1.0     1.0  1.0  1.0      0.00            0.0 -8.214890e-17           0.000000               0.0          0.000000                inf                0.00                  0.00\n0   1.0     1.0  1.0  1.0      0.00            0.0 -1.050792e-16           0.000000               0.0          0.000000                inf                0.00                  0.00\n0   0.8     1.0  0.0  1.0      0.16            1.0 -2.000000e-01           0.100000               0.4          0.630930           0.923760                0.25                  0.25\n0   0.6     1.0  0.0  1.0      0.24            1.0 -3.000000e-01           0.109017               0.4          0.347041           1.131371                0.25                  0.25\n0   0.4     0.0  0.0  1.0      0.24            1.0 -3.000000e-01           0.134752               0.4          0.347041           1.131371                0.25                  0.25\n0   0.2     0.0  0.0  1.0      0.16            1.0 -2.000000e-01           0.200000               0.4          1.000000           0.923760                0.25                  0.25\n0   0.0     0.0  0.0  0.0      0.00            0.0  0.000000e+00           0.000000               0.0          0.000000                inf                0.00                  0.00\n0   0.0     0.0  0.0  0.0      0.00            0.0  0.000000e+00           0.000000               0.0          0.000000                inf                0.00                  0.00\n0   0.0     0.0  0.0  0.0      0.00            0.0  0.000000e+00           0.000000               0.0          0.000000                inf                0.00                  0.00\n</code></pre> Source code in <code>timecave/validation_methods/CV.py</code> <pre><code>def statistics(self) -&gt; tuple[pd.DataFrame]:\n    \"\"\"\n    Compute relevant statistics for both training and validation sets.\n\n    This method computes relevant time series features, such as mean, strength-of-trend, etc. for both the whole time series, the training set and the validation set.\n    It can and should be used to ensure that the characteristics of both the training and validation sets are, statistically speaking, similar to those of the time series one wishes to forecast.\n    If this is not the case, using the validation method will most likely lead to a poor assessment of the model's performance.\n\n    Returns\n    -------\n    pd.DataFrame\n        Relevant features for the entire time series.\n\n    pd.DataFrame\n        Relevant features for the training set.\n\n    pd.DataFrame\n        Relevant features for the validation set.\n\n    Raises\n    ------\n    ValueError\n        If the time series is composed of less than three samples.\n\n    ValueError\n        If the folds comprise less than two samples.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; from timecave.validation_methods.CV import hvBlockCV\n    &gt;&gt;&gt; ts = np.hstack((np.ones(5), np.zeros(5)));\n    &gt;&gt;&gt; splitter = hvBlockCV(ts, h=2, v=2);\n    &gt;&gt;&gt; ts_stats, training_stats, validation_stats = splitter.statistics();\n    Frequency features are only meaningful if the correct sampling frequency is passed to the class.\n    The training set is too small to compute most meaningful features.\n    The training set is too small to compute most meaningful features.\n    &gt;&gt;&gt; ts_stats\n       Mean  Median  Min  Max  Variance  P2P_amplitude  Trend_slope  Spectral_centroid  Spectral_rolloff  Spectral_entropy  Strength_of_trend  Mean_crossing_rate  Median_crossing_rate\n    0   0.5     0.5  0.0  1.0      0.25            1.0    -0.151515           0.114058               0.5           0.38717            1.59099            0.111111              0.111111\n    &gt;&gt;&gt; training_stats\n       Mean  Median  Min  Max  Variance  P2P_amplitude   Trend_slope  Spectral_centroid  Spectral_rolloff  Spectral_entropy  Strength_of_trend  Mean_crossing_rate  Median_crossing_rate\n    0   0.0     0.0  0.0  0.0       0.0            0.0  0.000000e+00                0.0               0.0               0.0                inf                 0.0                   0.0\n    0   0.0     0.0  0.0  0.0       0.0            0.0  0.000000e+00                0.0               0.0               0.0                inf                 0.0                   0.0\n    0   0.0     0.0  0.0  0.0       0.0            0.0  0.000000e+00                0.0               0.0               0.0                inf                 0.0                   0.0\n    0   0.0     0.0  0.0  0.0       0.0            0.0  0.000000e+00                0.0               0.0               0.0                inf                 0.0                   0.0\n    0   1.0     1.0  1.0  1.0       0.0            0.0 -7.850462e-17                0.0               0.0               0.0                inf                 0.0                   0.0\n    0   1.0     1.0  1.0  1.0       0.0            0.0  8.985767e-17                0.0               0.0               0.0                inf                 0.0                   0.0\n    0   1.0     1.0  1.0  1.0       0.0            0.0 -8.214890e-17                0.0               0.0               0.0                inf                 0.0                   0.0\n    0   1.0     1.0  1.0  1.0       0.0            0.0 -1.050792e-16                0.0               0.0               0.0                inf                 0.0                   0.0\n    &gt;&gt;&gt; validation_stats\n       Mean  Median  Min  Max  Variance  P2P_amplitude   Trend_slope  Spectral_centroid  Spectral_rolloff  Spectral_entropy  Strength_of_trend  Mean_crossing_rate  Median_crossing_rate\n    0   1.0     1.0  1.0  1.0      0.00            0.0  8.985767e-17           0.000000               0.0          0.000000                inf                0.00                  0.00\n    0   1.0     1.0  1.0  1.0      0.00            0.0 -8.214890e-17           0.000000               0.0          0.000000                inf                0.00                  0.00\n    0   1.0     1.0  1.0  1.0      0.00            0.0 -1.050792e-16           0.000000               0.0          0.000000                inf                0.00                  0.00\n    0   0.8     1.0  0.0  1.0      0.16            1.0 -2.000000e-01           0.100000               0.4          0.630930           0.923760                0.25                  0.25\n    0   0.6     1.0  0.0  1.0      0.24            1.0 -3.000000e-01           0.109017               0.4          0.347041           1.131371                0.25                  0.25\n    0   0.4     0.0  0.0  1.0      0.24            1.0 -3.000000e-01           0.134752               0.4          0.347041           1.131371                0.25                  0.25\n    0   0.2     0.0  0.0  1.0      0.16            1.0 -2.000000e-01           0.200000               0.4          1.000000           0.923760                0.25                  0.25\n    0   0.0     0.0  0.0  0.0      0.00            0.0  0.000000e+00           0.000000               0.0          0.000000                inf                0.00                  0.00\n    0   0.0     0.0  0.0  0.0      0.00            0.0  0.000000e+00           0.000000               0.0          0.000000                inf                0.00                  0.00\n    0   0.0     0.0  0.0  0.0      0.00            0.0  0.000000e+00           0.000000               0.0          0.000000                inf                0.00                  0.00\n    \"\"\"\n\n    if self._n_samples &lt;= 2:\n\n        raise ValueError(\n            \"Basic statistics can only be computed if the time series comprises more than two samples.\"\n        )\n\n    print(\"Frequency features are only meaningful if the correct sampling frequency is passed to the class.\")\n\n    full_features = get_features(self._series, self.sampling_freq)\n    training_stats = []\n    validation_stats = []\n\n    for training, validation, _ in self.split():\n\n        if self._series[training].shape[0] &gt;= 2:\n\n            training_feat = get_features(self._series[training], self.sampling_freq)\n            training_stats.append(training_feat)\n\n        else:\n\n            print(\n                \"The training set is too small to compute most meaningful features.\"\n            )\n\n        if self._series[validation].shape[0] &gt;= 2:\n\n            validation_feat = get_features(\n                self._series[validation], self.sampling_freq\n            )\n            validation_stats.append(validation_feat)\n\n        else:\n\n            print(\n                \"The validation set is too small to compute most meaningful features.\"\n            )\n\n    training_features = pd.concat(training_stats)\n    validation_features = pd.concat(validation_stats)\n\n    return (full_features, training_features, validation_features)\n</code></pre>"},{"location":"API_ref/validation_methods/OOS/","title":"Out-of-sample methods","text":""},{"location":"API_ref/validation_methods/OOS/#timecave.validation_methods.OOS","title":"<code>timecave.validation_methods.OOS</code>","text":"<p>This module contains all the Out-of-Sample (OOS) validation methods supported by this package.</p> <p>Classes:</p> Name Description <code>Holdout</code> <p>Implements the classic Holdout method.</p> <code>RepeatedHoldout</code> <p>Implements the Repeated Holdout approach.</p> <code>RollingOriginUpdate</code> <p>Implements the Rolling Origin Update method.</p> <code>RollingOriginRecalibration</code> <p>Implements the Rolling Origin Recalibration method.</p> <code>FixedSizeRollingWindow</code> <p>Implements the Fixed-size Rolling Window method.</p> See also <p>Prequential methods: Prequential or forward validation methods for time series data.</p> <p>Cross-validation methods: Cross-validation methods for time series data.</p> <p>Markov methods: Markov cross-validation method for time series data.</p> Notes <p>Out-of-sample methods are one of the three main classes of validation methods for time series data (the others being prequential methods and cross-validation methods). Unlike cross-validation methods, this class of methods preserves the temporal order of observations, although it differs from prequential methods in that it does not partition the series into equally sized folds. For a comprehensive review of this class of methods, the reader should refer to [1].</p> References"},{"location":"API_ref/validation_methods/OOS/#timecave.validation_methods.OOS--1","title":"1","text":"<p>Leonard J Tashman. Out-of-sample tests of forecasting accuracy: an analysis and review. International journal of forecasting, 16(4):437\u2013450, 2000.</p>"},{"location":"API_ref/validation_methods/OOS/fixed_roll/","title":"Fixed Size Rolling Window method","text":""},{"location":"API_ref/validation_methods/OOS/fixed_roll/#timecave.validation_methods.OOS.FixedSizeRollingWindow","title":"<code>timecave.validation_methods.OOS.FixedSizeRollingWindow(ts, fs=1, origin=0.7)</code>","text":"<p>               Bases: <code>BaseSplitter</code></p> <p>Implements the Fixed Size Rolling Window method.</p> <p>This class implements the Fixed Size Rolling Window method. This method splits the data into a single training set and several validation sets. Neither the training sets nor the validation sets are disjoint. At every iteration, a single data point is dropped from the validation set and added     to the training set. The oldest data point belonging to the training set is also discarded, so that the amount of training samples remains constant.</p> <p>Parameters:</p> Name Type Description Default <code>ts</code> <code>ndarray | Series</code> <p>Univariate time series.</p> required <code>fs</code> <code>float | int</code> <p>Sampling frequency (Hz).</p> <code>1</code> <code>origin</code> <code>int | float</code> <p>The point from which the data is split.         If an integer is passed, it is interpreted as an index.         If a float is passed instead, it is treated as the percentage of samples         that should be used for training.</p> <code>0.7</code> <p>Attributes:</p> Name Type Description <code>n_splits</code> <code>int</code> <p>The number of splits.</p> <code>sampling_freq</code> <code>int | float</code> <p>The series' sampling frequency (Hz).</p> <p>Methods:</p> Name Description <code>split</code> <p>Split the time series into training and validation sets.</p> <code>info</code> <p>Provide additional information on the validation method.</p> <code>statistics</code> <p>Compute relevant statistics for both training and validation sets.</p> <code>plot</code> <p>Plot the partitioned time series.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If <code>origin</code> is neither an integer nor a float.</p> <code>ValueError</code> <p>If <code>origin</code> is a float that does not lie in the ]0, 1[ interval.</p> <code>ValueError</code> <p>If <code>origin</code> is an integer that does not lie in the ]0, n_samples[ interval.</p> Warning <p>Depending on the time series' size, this method can have a large computational cost.</p> Notes <p>The Fixed Size Rolling Origin method consists of splitting the data into a training set and a validation set,     with the former preceding the latter. At every iteration, a single data point (the one closest to the training set) is dropped from the     validation set and added to the training set. Additionally, the oldest data point belonging to the training set is discarded, so that the amount of training samples remains constant.     The model is then trained on the new training set and tested on the new validation set. This process ends once     the validation set consists of a single data point. The estimate of the true model error is the average validation     error.</p> <p></p> <p>For more details on this method, the reader should refer to [1].</p> References Source code in <code>timecave/validation_methods/OOS.py</code> <pre><code>def __init__(\n    self, ts: np.ndarray | pd.Series, fs: float | int = 1, origin: int | float = 0.7\n) -&gt; None:\n\n    super().__init__(2, ts, fs)\n    self._check_origin(origin)\n    self._origin = self._convert_origin(origin)\n    self._splitting_ind = np.arange(self._origin + 1, self._n_samples)\n    self._n_splits = self._splitting_ind.shape[0]\n\n    return\n</code></pre>"},{"location":"API_ref/validation_methods/OOS/fixed_roll/#timecave.validation_methods.OOS.FixedSizeRollingWindow--1","title":"1","text":"<p>Leonard J Tashman. Out-of-sample tests of forecasting accuracy: an analysis and review. International journal of forecasting, 16(4):437\u2013450, 2000.</p>"},{"location":"API_ref/validation_methods/OOS/fixed_roll/#timecave.validation_methods.OOS.FixedSizeRollingWindow.info","title":"<code>info()</code>","text":"<p>Provide some basic information on the training and validation sets.</p> <p>This method displays the minimum and maximum validation set size, as well as the training set size.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from timecave.validation_methods.OOS import FixedSizeRollingWindow\n&gt;&gt;&gt; ts = np.ones(10);\n&gt;&gt;&gt; splitter = FixedSizeRollingWindow(ts);\n&gt;&gt;&gt; splitter.info();\nFixed-size Rolling Window method\n--------------------------------\nTime series size: 10 samples\nTraining set size (fixed parameter): 7 samples (70.0 %)\nMaximum validation set size: 3 samples (30.0 %)\nMinimum validation set size: 1 sample (10.0 %)\n</code></pre> Source code in <code>timecave/validation_methods/OOS.py</code> <pre><code>def info(self) -&gt; None:\n    \"\"\"\n    Provide some basic information on the training and validation sets.\n\n    This method displays the minimum and maximum validation set size, as well as the training set size.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; from timecave.validation_methods.OOS import FixedSizeRollingWindow\n    &gt;&gt;&gt; ts = np.ones(10);\n    &gt;&gt;&gt; splitter = FixedSizeRollingWindow(ts);\n    &gt;&gt;&gt; splitter.info();\n    Fixed-size Rolling Window method\n    --------------------------------\n    Time series size: 10 samples\n    Training set size (fixed parameter): 7 samples (70.0 %)\n    Maximum validation set size: 3 samples (30.0 %)\n    Minimum validation set size: 1 sample (10.0 %)\n    \"\"\"\n\n    training_size = self._origin + 1\n    max_size = self._n_samples - self._origin - 1\n    min_size = 1\n\n    training_pct = np.round(training_size / self._n_samples, 4) * 100\n    max_pct = np.round(max_size / self._n_samples, 4) * 100\n    min_pct = np.round(1 / self._n_samples, 4) * 100\n\n    print(\"Fixed-size Rolling Window method\")\n    print(\"--------------------------------\")\n    print(f\"Time series size: {self._n_samples} samples\")\n    print(\n        f\"Training set size (fixed parameter): {training_size} samples ({training_pct} %)\"\n    )\n    print(f\"Maximum validation set size: {max_size} samples ({max_pct} %)\")\n    print(f\"Minimum validation set size: {min_size} sample ({min_pct} %)\")\n\n    return\n</code></pre>"},{"location":"API_ref/validation_methods/OOS/fixed_roll/#timecave.validation_methods.OOS.FixedSizeRollingWindow.plot","title":"<code>plot(height, width)</code>","text":"<p>Plot the partitioned time series.</p> <p>This method allows the user to plot the partitioned time series. The training and validation sets are plotted using different colours.</p> <p>Parameters:</p> Name Type Description Default <code>height</code> <code>int</code> <p>The figure's height.</p> required <code>width</code> <code>int</code> <p>The figure's width.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from timecave.validation_methods.OOS import FixedSizeRollingWindow\n&gt;&gt;&gt; ts = np.ones(10);\n&gt;&gt;&gt; splitter = FixedSizeRollingWindow(ts);\n&gt;&gt;&gt; splitter.plot(10, 10);\n</code></pre> <p></p> Source code in <code>timecave/validation_methods/OOS.py</code> <pre><code>def plot(self, height: int, width: int) -&gt; None:\n    \"\"\"\n    Plot the partitioned time series.\n\n    This method allows the user to plot the partitioned time series. The training and validation sets are plotted using different colours.\n\n    Parameters\n    ----------\n    height : int\n        The figure's height.\n\n    width : int\n        The figure's width.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; from timecave.validation_methods.OOS import FixedSizeRollingWindow\n    &gt;&gt;&gt; ts = np.ones(10);\n    &gt;&gt;&gt; splitter = FixedSizeRollingWindow(ts);\n    &gt;&gt;&gt; splitter.plot(10, 10);\n\n    ![Holdout_plot_image](../../../images/FixedRoll_plot.png)\n    \"\"\"\n\n    fig, axs = plt.subplots(self._n_samples - self._origin - 1, 1, sharex=True)\n    fig.set_figheight(height)\n    fig.set_figwidth(width)\n    fig.supxlabel(\"Samples\")\n    fig.supylabel(\"Time Series\")\n    fig.suptitle(\"Fixed-size Rolling Window method\")\n\n    for it, (training, validation, _) in enumerate(self.split()):\n\n        axs[it].scatter(training, self._series[training], label=\"Training set\")\n        axs[it].scatter(\n            validation, self._series[validation], label=\"Validation set\"\n        )\n        axs[it].set_title(\"Iteration {}\".format(it + 1))\n        axs[it].legend()\n\n    plt.show()\n\n    return\n</code></pre>"},{"location":"API_ref/validation_methods/OOS/fixed_roll/#timecave.validation_methods.OOS.FixedSizeRollingWindow.split","title":"<code>split()</code>","text":"<p>Split the time series into training and validation sets.</p> <p>This method splits the series' indices into disjoint sets containing the training and validation indices. At every iteration, an array of training indices and another one containing the validation indices are generated. Note that this method is a generator. To access the indices, use the <code>next()</code> method or a <code>for</code> loop.</p> <p>Yields:</p> Type Description <code>ndarray</code> <p>Array of training indices.</p> <code>ndarray</code> <p>Array of validation indices.</p> <code>float</code> <p>Used for compatibility reasons. Irrelevant for this method.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from timecave.validation_methods.OOS import FixedSizeRollingWindow\n&gt;&gt;&gt; ts = np.ones(10);\n&gt;&gt;&gt; splitter = FixedSizeRollingWindow(ts);\n&gt;&gt;&gt; for ind, (train, val, _) in enumerate(splitter.split()):\n...\n...     print(f\"Iteration {ind+1}\");\n...     print(f\"Training set indices: {train}\");\n...     print(f\"Validation set indices: {val}\");\nIteration 1\nTraining set indices: [0 1 2 3 4 5 6]\nValidation set indices: [7 8 9]\nIteration 2\nTraining set indices: [1 2 3 4 5 6 7]\nValidation set indices: [8 9]\nIteration 3\nTraining set indices: [2 3 4 5 6 7 8]\nValidation set indices: [9]\n</code></pre> Source code in <code>timecave/validation_methods/OOS.py</code> <pre><code>def split(self) -&gt; Generator[tuple[np.ndarray, np.ndarray, float], None, None]:\n    \"\"\"\n    Split the time series into training and validation sets.\n\n    This method splits the series' indices into disjoint sets containing the training and validation indices.\n    At every iteration, an array of training indices and another one containing the validation indices are generated.\n    Note that this method is a generator. To access the indices, use the `next()` method or a `for` loop.\n\n    Yields\n    ------\n    np.ndarray\n        Array of training indices.\n\n    np.ndarray\n        Array of validation indices.\n\n    float\n        Used for compatibility reasons. Irrelevant for this method.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; from timecave.validation_methods.OOS import FixedSizeRollingWindow\n    &gt;&gt;&gt; ts = np.ones(10);\n    &gt;&gt;&gt; splitter = FixedSizeRollingWindow(ts);\n    &gt;&gt;&gt; for ind, (train, val, _) in enumerate(splitter.split()):\n    ...\n    ...     print(f\"Iteration {ind+1}\");\n    ...     print(f\"Training set indices: {train}\");\n    ...     print(f\"Validation set indices: {val}\");\n    Iteration 1\n    Training set indices: [0 1 2 3 4 5 6]\n    Validation set indices: [7 8 9]\n    Iteration 2\n    Training set indices: [1 2 3 4 5 6 7]\n    Validation set indices: [8 9]\n    Iteration 3\n    Training set indices: [2 3 4 5 6 7 8]\n    Validation set indices: [9]\n    \"\"\"\n    start_training_ind = self._splitting_ind - self._origin - 1\n\n    for start_ind, end_ind in zip(start_training_ind, self._splitting_ind):\n\n        training = self._indices[start_ind:end_ind]\n        validation = self._indices[end_ind:]\n\n        yield (training, validation, 1.0)\n</code></pre>"},{"location":"API_ref/validation_methods/OOS/fixed_roll/#timecave.validation_methods.OOS.FixedSizeRollingWindow.statistics","title":"<code>statistics()</code>","text":"<p>Compute relevant statistics for both training and validation sets.</p> <p>This method computes relevant time series features, such as mean, strength-of-trend, etc. for both the whole time series, the training set and the validation set. It can and should be used to ensure that the characteristics of both the training and validation sets are, statistically speaking, similar to those of the time series one wishes to forecast. If this is not the case, using the validation method will most likely lead to a poor assessment of the model's performance.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Relevant features for the entire time series.</p> <code>DataFrame</code> <p>Relevant features for the training set.</p> <code>DataFrame</code> <p>Relevant features for the validation set.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the time series is composed of less than three samples.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from timecave.validation_methods.OOS import FixedSizeRollingWindow\n&gt;&gt;&gt; ts = np.hstack((np.ones(5), np.zeros(5)));\n&gt;&gt;&gt; splitter = FixedSizeRollingWindow(ts);\n&gt;&gt;&gt; ts_stats, training_stats, validation_stats = splitter.statistics();\nFrequency features are only meaningful if the correct sampling frequency is passed to the class.\nTraining and validation set features can only computed if each set is composed of two or more samples.\n&gt;&gt;&gt; ts_stats\n   Mean  Median  Min  Max  Variance  P2P_amplitude  Trend_slope  Spectral_centroid  Spectral_rolloff  Spectral_entropy  Strength_of_trend  Mean_crossing_rate  Median_crossing_rate\n0   0.5     0.5  0.0  1.0      0.25            1.0    -0.151515           0.114058               0.5           0.38717            1.59099            0.111111              0.111111\n&gt;&gt;&gt; training_stats\n       Mean  Median  Min  Max  Variance  P2P_amplitude  Trend_slope  Spectral_centroid  Spectral_rolloff  Spectral_entropy  Strength_of_trend  Mean_crossing_rate  Median_crossing_rate\n0  0.714286     1.0  0.0  1.0  0.204082            1.0    -0.178571           0.094706          0.428571          0.556506           1.212183            0.166667              0.166667\n0  0.571429     1.0  0.0  1.0  0.244898            1.0    -0.214286           0.108266          0.428571          0.387375           1.327880            0.166667              0.166667\n0  0.428571     0.0  0.0  1.0  0.244898            1.0    -0.214286           0.124661          0.428571          0.387375           1.327880            0.166667              0.166667\n&gt;&gt;&gt; validation_stats\n   Mean  Median  Min  Max  Variance  P2P_amplitude  Trend_slope  Spectral_centroid  Spectral_rolloff  Spectral_entropy  Strength_of_trend  Mean_crossing_rate  Median_crossing_rate\n0   0.0     0.0  0.0  0.0       0.0            0.0          0.0                  0               0.0               0.0                inf                 0.0                   0.0\n0   0.0     0.0  0.0  0.0       0.0            0.0          0.0                  0               0.0               0.0                inf                 0.0                   0.0\n</code></pre> Source code in <code>timecave/validation_methods/OOS.py</code> <pre><code>def statistics(self) -&gt; tuple[pd.DataFrame]:\n    \"\"\"\n    Compute relevant statistics for both training and validation sets.\n\n    This method computes relevant time series features, such as mean, strength-of-trend, etc. for both the whole time series, the training set and the validation set.\n    It can and should be used to ensure that the characteristics of both the training and validation sets are, statistically speaking, similar to those of the time series one wishes to forecast.\n    If this is not the case, using the validation method will most likely lead to a poor assessment of the model's performance.\n\n    Returns\n    -------\n    pd.DataFrame\n        Relevant features for the entire time series.\n\n    pd.DataFrame\n        Relevant features for the training set.\n\n    pd.DataFrame\n        Relevant features for the validation set.\n\n    Raises\n    ------\n    ValueError\n        If the time series is composed of less than three samples.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; from timecave.validation_methods.OOS import FixedSizeRollingWindow\n    &gt;&gt;&gt; ts = np.hstack((np.ones(5), np.zeros(5)));\n    &gt;&gt;&gt; splitter = FixedSizeRollingWindow(ts);\n    &gt;&gt;&gt; ts_stats, training_stats, validation_stats = splitter.statistics();\n    Frequency features are only meaningful if the correct sampling frequency is passed to the class.\n    Training and validation set features can only computed if each set is composed of two or more samples.\n    &gt;&gt;&gt; ts_stats\n       Mean  Median  Min  Max  Variance  P2P_amplitude  Trend_slope  Spectral_centroid  Spectral_rolloff  Spectral_entropy  Strength_of_trend  Mean_crossing_rate  Median_crossing_rate\n    0   0.5     0.5  0.0  1.0      0.25            1.0    -0.151515           0.114058               0.5           0.38717            1.59099            0.111111              0.111111\n    &gt;&gt;&gt; training_stats\n           Mean  Median  Min  Max  Variance  P2P_amplitude  Trend_slope  Spectral_centroid  Spectral_rolloff  Spectral_entropy  Strength_of_trend  Mean_crossing_rate  Median_crossing_rate\n    0  0.714286     1.0  0.0  1.0  0.204082            1.0    -0.178571           0.094706          0.428571          0.556506           1.212183            0.166667              0.166667\n    0  0.571429     1.0  0.0  1.0  0.244898            1.0    -0.214286           0.108266          0.428571          0.387375           1.327880            0.166667              0.166667\n    0  0.428571     0.0  0.0  1.0  0.244898            1.0    -0.214286           0.124661          0.428571          0.387375           1.327880            0.166667              0.166667\n    &gt;&gt;&gt; validation_stats\n       Mean  Median  Min  Max  Variance  P2P_amplitude  Trend_slope  Spectral_centroid  Spectral_rolloff  Spectral_entropy  Strength_of_trend  Mean_crossing_rate  Median_crossing_rate\n    0   0.0     0.0  0.0  0.0       0.0            0.0          0.0                  0               0.0               0.0                inf                 0.0                   0.0\n    0   0.0     0.0  0.0  0.0       0.0            0.0          0.0                  0               0.0               0.0                inf                 0.0                   0.0\n    \"\"\"\n\n    if self._n_samples &lt;= 2:\n\n        raise ValueError(\n            \"Basic statistics can only be computed if the time series comprises more than two samples.\"\n        )\n\n    print(\"Frequency features are only meaningful if the correct sampling frequency is passed to the class.\")\n\n    full_features = get_features(self._series, self.sampling_freq)\n    training_stats = []\n    validation_stats = []\n\n    print(\n        \"Training and validation set features can only computed if each set is composed of two or more samples.\"\n    )\n\n    for training, validation, _ in self.split():\n\n        if self._series[training].shape[0] &gt;= 2:\n\n            training_feat = get_features(self._series[training], self.sampling_freq)\n            training_stats.append(training_feat)\n\n        if self._series[validation].shape[0] &gt;= 2:\n\n            validation_feat = get_features(\n                self._series[validation], self.sampling_freq\n            )\n            validation_stats.append(validation_feat)\n\n    training_features = pd.concat(training_stats)\n    validation_features = pd.concat(validation_stats)\n\n    return (full_features, training_features, validation_features)\n</code></pre>"},{"location":"API_ref/validation_methods/OOS/holdout/","title":"Holdout method","text":""},{"location":"API_ref/validation_methods/OOS/holdout/#timecave.validation_methods.OOS.Holdout","title":"<code>timecave.validation_methods.OOS.Holdout(ts, fs=1, validation_size=0.3)</code>","text":"<p>               Bases: <code>BaseSplitter</code></p> <p>Implements the classic Holdout method.</p> <p>This class implements the classic Holdout method, which splits the time series into two disjoint sets: one used for training, and another one used for validation purposes. Note that the larger the validation set, the smaller the training set, and vice-versa. As this is an Out-of-Sample method, the training indices precede the validation ones.</p> <p>Parameters:</p> Name Type Description Default <code>ts</code> <code>ndarray | Series</code> <p>Univariate time series.</p> required <code>fs</code> <code>float | int</code> <p>Sampling frequency (Hz).</p> <code>1</code> <code>validation_size</code> <code>float</code> <p>Validation set size (relative to the time series size).</p> <code>0.3</code> <p>Attributes:</p> Name Type Description <code>n_splits</code> <code>int</code> <p>The number of splits.</p> <code>sampling_freq</code> <code>int | float</code> <p>The series' sampling frequency (Hz).</p> <p>Methods:</p> Name Description <code>split</code> <p>Split the time series into training and validation sets.</p> <code>info</code> <p>Provide additional information on the validation method.</p> <code>statistics</code> <p>Compute relevant statistics for both training and validation sets.</p> <code>plot</code> <p>Plot the partitioned time series.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If the validation size is not a float.</p> <code>ValueError</code> <p>If the validation size does not lie in the ]0, 1[ interval.</p> See also <p>RepeatedHoldout: Perform several iterations of the Holdout method with a randomised validation set size.</p> Notes <p>The classic Holdout method consists of splitting the time series in two different sets: one for training and one for validation. This method preserves the temporal order of observations:     the oldest set of observations is used for training, while the most recent data is used for validating the model. The model's error on the validation set data is used as an estimate of its true error.</p> <p></p> <p>This method's computational cost is negligible.</p> Source code in <code>timecave/validation_methods/OOS.py</code> <pre><code>def __init__(\n    self, ts: np.ndarray | pd.Series, fs: float | int = 1, validation_size: float = 0.3\n) -&gt; None:\n\n    super().__init__(2, ts, fs)\n    self._check_validation_size(validation_size)\n    self._val_size = validation_size\n\n    return\n</code></pre>"},{"location":"API_ref/validation_methods/OOS/holdout/#timecave.validation_methods.OOS.Holdout.info","title":"<code>info()</code>","text":"<p>Provide some basic information on the training and validation sets.</p> <p>This method displays the time series size along with those of the training and validation sets.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from timecave.validation_methods.OOS import Holdout\n&gt;&gt;&gt; ts = np.ones(10);\n&gt;&gt;&gt; splitter = Holdout(ts);\n&gt;&gt;&gt; splitter.info();\nHoldout method\n--------------\nTime series size: 10 samples\nTraining set size: 7 samples (70.0 %)\nValidation set size: 3 samples (30.0 %)\n</code></pre> Source code in <code>timecave/validation_methods/OOS.py</code> <pre><code>def info(self) -&gt; None:\n    \"\"\"\n    Provide some basic information on the training and validation sets.\n\n    This method displays the time series size along with those of the training and validation sets.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; from timecave.validation_methods.OOS import Holdout\n    &gt;&gt;&gt; ts = np.ones(10);\n    &gt;&gt;&gt; splitter = Holdout(ts);\n    &gt;&gt;&gt; splitter.info();\n    Holdout method\n    --------------\n    Time series size: 10 samples\n    Training set size: 7 samples (70.0 %)\n    Validation set size: 3 samples (30.0 %)\n    \"\"\"\n\n    print(\"Holdout method\")\n    print(\"--------------\")\n    print(f\"Time series size: {self._n_samples} samples\")\n    print(\n        f\"Training set size: {int(np.round((1 - self._val_size) * self._n_samples))} samples ({np.round(1 - self._val_size, 4) * 100} %)\"\n    )\n    print(\n        f\"Validation set size: {int(np.round(self._val_size * self._n_samples))} samples ({np.round(self._val_size, 4) * 100} %)\"\n    )\n\n    return\n</code></pre>"},{"location":"API_ref/validation_methods/OOS/holdout/#timecave.validation_methods.OOS.Holdout.plot","title":"<code>plot(height, width)</code>","text":"<p>Plot the partitioned time series.</p> <p>This method allows the user to plot the partitioned time series. The training and validation sets are plotted using different colours. </p> <p>Parameters:</p> Name Type Description Default <code>height</code> <code>int</code> <p>The figure's height.</p> required <code>width</code> <code>int</code> <p>The figure's width.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from timecave.validation_methods.OOS import Holdout\n&gt;&gt;&gt; ts = np.arange(1, 11);\n&gt;&gt;&gt; splitter = Holdout(ts);\n&gt;&gt;&gt; splitter.plot(10, 10);\n</code></pre> <p></p> Source code in <code>timecave/validation_methods/OOS.py</code> <pre><code>def plot(self, height: int, width: int) -&gt; None:\n    \"\"\"\n    Plot the partitioned time series.\n\n    This method allows the user to plot the partitioned time series. The training and validation sets are plotted using different colours. \n\n    Parameters\n    ----------\n    height : int\n        The figure's height.\n\n    width : int\n        The figure's width.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; from timecave.validation_methods.OOS import Holdout\n    &gt;&gt;&gt; ts = np.arange(1, 11);\n    &gt;&gt;&gt; splitter = Holdout(ts);\n    &gt;&gt;&gt; splitter.plot(10, 10);\n\n    ![Holdout_plot_image](../../../images/Holdout_plot.png)\n    \"\"\"\n\n    split = self.split()\n    training, validation, _ = next(split)\n\n    fig = plt.figure(figsize=(height, width))\n    ax = fig.add_subplot(1, 1, 1)\n    ax.scatter(training, self._series[training], label=\"Training set\")\n    ax.scatter(validation, self._series[validation], label=\"Validation set\")\n    ax.set_xlabel(\"Samples\")\n    ax.set_ylabel(\"Time Series\")\n    ax.set_title(\"Holdout method\")\n    ax.legend()\n    plt.show()\n\n    return\n</code></pre>"},{"location":"API_ref/validation_methods/OOS/holdout/#timecave.validation_methods.OOS.Holdout.split","title":"<code>split()</code>","text":"<p>Split the time series into training and validation sets.</p> <p>This method splits the series' indices into two disjoint sets: one containing the training indices, and another one with the validation indices. Note that this method is a generator. To access the indices, use the <code>next()</code> method or a <code>for</code> loop.</p> <p>Yields:</p> Type Description <code>ndarray</code> <p>Array of training indices.</p> <code>ndarray</code> <p>Array of validation indices.</p> <code>float</code> <p>Used for compatibility reasons. Irrelevant for this method.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from timecave.validation_methods.OOS import Holdout\n&gt;&gt;&gt; ts = np.ones(10);\n&gt;&gt;&gt; splitter = Holdout(ts);\n&gt;&gt;&gt; for train, val, _ in splitter.split():\n...     \n...     # Print the training indices and their respective values\n...     print(f\"Training indices: {train}\");\n...     print(f\"Training values: {ts[train]}\");\n...     \n...     # Do the same for the validation indices\n...     print(f\"Validation indices: {val}\");\n...     print(f\"Validation values: {ts[val]}\");\nTraining indices: [0 1 2 3 4 5 6]\nTraining values: [1. 1. 1. 1. 1. 1. 1.]\nValidation indices: [7 8 9]\nValidation values: [1. 1. 1.]\n</code></pre> Source code in <code>timecave/validation_methods/OOS.py</code> <pre><code>def split(self) -&gt; Generator[tuple[np.ndarray, np.ndarray, float], None, None]:\n    \"\"\"\n    Split the time series into training and validation sets.\n\n    This method splits the series' indices into two disjoint sets: one containing the training indices, and another one with the validation indices.\n    Note that this method is a generator. To access the indices, use the `next()` method or a `for` loop.\n\n    Yields\n    ------\n    np.ndarray\n        Array of training indices.\n\n    np.ndarray\n        Array of validation indices.\n\n    float\n        Used for compatibility reasons. Irrelevant for this method.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; from timecave.validation_methods.OOS import Holdout\n    &gt;&gt;&gt; ts = np.ones(10);\n    &gt;&gt;&gt; splitter = Holdout(ts);\n    &gt;&gt;&gt; for train, val, _ in splitter.split():\n    ...     \n    ...     # Print the training indices and their respective values\n    ...     print(f\"Training indices: {train}\");\n    ...     print(f\"Training values: {ts[train]}\");\n    ...     \n    ...     # Do the same for the validation indices\n    ...     print(f\"Validation indices: {val}\");\n    ...     print(f\"Validation values: {ts[val]}\");\n    Training indices: [0 1 2 3 4 5 6]\n    Training values: [1. 1. 1. 1. 1. 1. 1.]\n    Validation indices: [7 8 9]\n    Validation values: [1. 1. 1.]\n    \"\"\"\n\n    split_ind = int(np.round((1 - self._val_size) * self._n_samples))\n\n    train = self._indices[:split_ind]\n    validation = self._indices[split_ind:]\n\n    yield (train, validation, 1.0)\n</code></pre>"},{"location":"API_ref/validation_methods/OOS/holdout/#timecave.validation_methods.OOS.Holdout.statistics","title":"<code>statistics()</code>","text":"<p>Compute relevant statistics for both training and validation sets.</p> <p>This method computes relevant time series features, such as mean, strength-of-trend, etc. for both the whole time series, the training set and the validation set. It can and should be used to ensure that the characteristics of both the training and validation sets are, statistically speaking, similar to those of the time series one wishes to forecast. If this is not the case, using the validation method will most likely lead to a poor assessment of the model's performance.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Relevant features for the entire time series.</p> <code>DataFrame</code> <p>Relevant features for the training set.</p> <code>DataFrame</code> <p>Relevant features for the validation set.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the time series is composed of less than three samples.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from timecave.validation_methods.OOS import Holdout\n&gt;&gt;&gt; ts = np.hstack((np.ones(5), np.zeros(5)));\n&gt;&gt;&gt; splitter = Holdout(ts, validation_size=0.5);\n&gt;&gt;&gt; ts_stats, training_stats, validation_stats = splitter.statistics();\nFrequency features are only meaningful if the correct sampling frequency is passed to the class.\n&gt;&gt;&gt; ts_stats\n   Mean  Median  Min  Max  Variance  P2P_amplitude  Trend_slope  Spectral_centroid  Spectral_rolloff  Spectral_entropy  Strength_of_trend  Mean_crossing_rate  Median_crossing_rate\n0   0.5     0.5  0.0  1.0      0.25            1.0    -0.151515           0.114058               0.5           0.38717            1.59099            0.111111              0.111111\n&gt;&gt;&gt; training_stats\n   Mean  Median  Min  Max  Variance  P2P_amplitude   Trend_slope  Spectral_centroid  Spectral_rolloff  Spectral_entropy  Strength_of_trend  Mean_crossing_rate  Median_crossing_rate\n0   1.0     1.0  1.0  1.0       0.0            0.0 -1.050792e-16                0.0               0.0               0.0                inf                 0.0                   0.0\n&gt;&gt;&gt; validation_stats\n   Mean  Median  Min  Max  Variance  P2P_amplitude  Trend_slope  Spectral_centroid  Spectral_rolloff  Spectral_entropy  Strength_of_trend  Mean_crossing_rate  Median_crossing_rate\n0   0.0     0.0  0.0  0.0       0.0            0.0          0.0                  0               0.0               0.0                inf                 0.0                   0.0\n</code></pre> Source code in <code>timecave/validation_methods/OOS.py</code> <pre><code>def statistics(self) -&gt; tuple[pd.DataFrame]:\n    \"\"\"\n    Compute relevant statistics for both training and validation sets.\n\n    This method computes relevant time series features, such as mean, strength-of-trend, etc. for both the whole time series, the training set and the validation set.\n    It can and should be used to ensure that the characteristics of both the training and validation sets are, statistically speaking, similar to those of the time series one wishes to forecast.\n    If this is not the case, using the validation method will most likely lead to a poor assessment of the model's performance.\n\n    Returns\n    -------\n    pd.DataFrame\n        Relevant features for the entire time series.\n\n    pd.DataFrame\n        Relevant features for the training set.\n\n    pd.DataFrame\n        Relevant features for the validation set.\n\n    Raises\n    ------\n    ValueError\n        If the time series is composed of less than three samples.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; from timecave.validation_methods.OOS import Holdout\n    &gt;&gt;&gt; ts = np.hstack((np.ones(5), np.zeros(5)));\n    &gt;&gt;&gt; splitter = Holdout(ts, validation_size=0.5);\n    &gt;&gt;&gt; ts_stats, training_stats, validation_stats = splitter.statistics();\n    Frequency features are only meaningful if the correct sampling frequency is passed to the class.\n    &gt;&gt;&gt; ts_stats\n       Mean  Median  Min  Max  Variance  P2P_amplitude  Trend_slope  Spectral_centroid  Spectral_rolloff  Spectral_entropy  Strength_of_trend  Mean_crossing_rate  Median_crossing_rate\n    0   0.5     0.5  0.0  1.0      0.25            1.0    -0.151515           0.114058               0.5           0.38717            1.59099            0.111111              0.111111\n    &gt;&gt;&gt; training_stats\n       Mean  Median  Min  Max  Variance  P2P_amplitude   Trend_slope  Spectral_centroid  Spectral_rolloff  Spectral_entropy  Strength_of_trend  Mean_crossing_rate  Median_crossing_rate\n    0   1.0     1.0  1.0  1.0       0.0            0.0 -1.050792e-16                0.0               0.0               0.0                inf                 0.0                   0.0\n    &gt;&gt;&gt; validation_stats\n       Mean  Median  Min  Max  Variance  P2P_amplitude  Trend_slope  Spectral_centroid  Spectral_rolloff  Spectral_entropy  Strength_of_trend  Mean_crossing_rate  Median_crossing_rate\n    0   0.0     0.0  0.0  0.0       0.0            0.0          0.0                  0               0.0               0.0                inf                 0.0                   0.0\n    \"\"\"\n\n    if self._n_samples &lt;= 2:\n\n        raise ValueError(\n            \"Basic statistics can only be computed if the time series comprises more than two samples.\"\n        )\n\n    print(\"Frequency features are only meaningful if the correct sampling frequency is passed to the class.\")\n\n    split = self.split()\n    training, validation, _ = next(split)\n\n    full_feat = get_features(self._series, self.sampling_freq)\n\n    if self._series[training].shape[0] &gt;= 2:\n\n        training_feat = get_features(self._series[training], self.sampling_freq)\n\n    else:\n\n        training_feat = pd.DataFrame(columns=full_feat.columns)\n        warn(\"Training and validation set statistics can only be computed if each of these comprise two or more samples.\")\n\n    if self._series[validation].shape[0] &gt;= 2:\n\n        validation_feat = get_features(self._series[validation], self.sampling_freq)\n\n    else:\n\n        validation_feat = pd.DataFrame(columns=full_feat.columns)\n        warn(\"Training and validation set statistics can only be computed if each of these comprise two or more samples.\")\n\n    return (full_feat, training_feat, validation_feat)\n</code></pre>"},{"location":"API_ref/validation_methods/OOS/rep_holdout/","title":"Repeated Holdout method","text":""},{"location":"API_ref/validation_methods/OOS/rep_holdout/#timecave.validation_methods.OOS.RepeatedHoldout","title":"<code>timecave.validation_methods.OOS.RepeatedHoldout(ts, fs=1, iterations=5, splitting_interval=[0.7, 0.8], seed=0)</code>","text":"<p>               Bases: <code>BaseSplitter</code></p> <p>Implements the Repeated Holdout method.</p> <p>This class implements the Repeated Holdout method. This is essentially an extension of the classic Holdout method, as it simply applies     this method multiple times with a randomised splitting point. At every iteration, this point is chosen at random from an interval of values     specified by the user. For this purpose, our implementation uses a uniform distribution, though, in theory, any continuous distribution could be used.</p> <p>Parameters:</p> Name Type Description Default <code>ts</code> <code>ndarray | Series</code> <p>Univariate time series.</p> required <code>fs</code> <code>float | int</code> <p>Sampling frequency (Hz).</p> <code>1</code> <code>iterations</code> <code>int</code> <p>Number of iterations that should be performed.</p> <code>5</code> <code>splitting_interval</code> <code>list[int | float]</code> <p>Interval from which the splitting point will be drawn.         If the values are integers, they are interpreted as indices.         Otherwise, they are regarded as the minimum and maximum allowable         sizes for the training set.</p> <code>[0.7, 0.8]</code> <code>seed</code> <code>int</code> <p>Random seed.</p> <code>0</code> <p>Attributes:</p> Name Type Description <code>n_splits</code> <code>int</code> <p>The number of splits.</p> <code>sampling_freq</code> <code>int | float</code> <p>The series' sampling frequency (Hz).</p> <p>Methods:</p> Name Description <code>split</code> <p>Split the time series into training and validation sets.</p> <code>info</code> <p>Provide additional information on the validation method.</p> <code>statistics</code> <p>Compute relevant statistics for both training and validation sets.</p> <code>plot</code> <p>Plot the partitioned time series.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If the <code>iterations</code> parameter is not an integer.</p> <code>ValueError</code> <p>If the <code>iterations</code> parameter is not positive.</p> <code>TypeError</code> <p>If the splitting interval is not a list.</p> <code>ValueError</code> <p>If the splitting interval list does not contain two values.</p> See also <p>Holdout: The classic Holdout method.</p> Notes <p>The Repeated Holdout method is an extension of the classic Holdout method. Essentially, the Holdout method is applied     multiple times, and an average of the error on the validation set is used as an estimate of the model's true error. At every iteration, the splitting point (and therefore the training and validation set sizes) is computed randomly from     an interval of values specified by the user.</p> <p></p> <p>Compared to the classic Holdout method, it has a greater computational cost, though, depending on the number     of iterations and the prediction model, this may be negligible. For more details on this method, the reader should     refer to [1].</p> References Source code in <code>timecave/validation_methods/OOS.py</code> <pre><code>def __init__(\n    self,\n    ts: np.ndarray | pd.Series,\n    fs: float | int = 1,\n    iterations: int = 5,\n    splitting_interval: list[int | float] = [0.7, 0.8],\n    seed: int = 0,\n) -&gt; None:\n\n    self._check_iterations(iterations)\n    self._check_splits(splitting_interval)\n    super().__init__(iterations, ts, fs)\n    self._iter = iterations\n    self._interval = self._convert_interval(splitting_interval)\n    self._seed = seed\n    self._splitting_ind = self._get_splitting_ind()\n\n    return\n</code></pre>"},{"location":"API_ref/validation_methods/OOS/rep_holdout/#timecave.validation_methods.OOS.RepeatedHoldout--1","title":"1","text":"<p>Vitor Cerqueira, Luis Torgo, and Igor Mozeti\u02c7c. Evaluating time series fore- casting models: An empirical study on performance estimation methods. Machine Learning, 109(11):1997\u20132028, 2020.</p>"},{"location":"API_ref/validation_methods/OOS/rep_holdout/#timecave.validation_methods.OOS.RepeatedHoldout.info","title":"<code>info()</code>","text":"<p>Provide some basic information on the training and validation sets.</p> <p>This method displays the average, minimum and maximum validation set sizes.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from timecave.validation_methods.OOS import RepeatedHoldout\n&gt;&gt;&gt; ts = np.ones(10);\n&gt;&gt;&gt; splitter = RepeatedHoldout(ts, splitting_interval=[0.6, 0.9]);\n&gt;&gt;&gt; splitter.info();\nRepeated Holdout method\n-----------------------\nTime series size: 10 samples\nAverage validation set size: 3.4 samples (34.0 %)\nMaximum validation set size: 4 samples (40.0 %)\nMinimum validation set size: 3 samples (30.0 %)\n</code></pre> Source code in <code>timecave/validation_methods/OOS.py</code> <pre><code>def info(self) -&gt; None:\n    \"\"\"\n    Provide some basic information on the training and validation sets.\n\n    This method displays the average, minimum and maximum validation set sizes.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; from timecave.validation_methods.OOS import RepeatedHoldout\n    &gt;&gt;&gt; ts = np.ones(10);\n    &gt;&gt;&gt; splitter = RepeatedHoldout(ts, splitting_interval=[0.6, 0.9]);\n    &gt;&gt;&gt; splitter.info();\n    Repeated Holdout method\n    -----------------------\n    Time series size: 10 samples\n    Average validation set size: 3.4 samples (34.0 %)\n    Maximum validation set size: 4 samples (40.0 %)\n    Minimum validation set size: 3 samples (30.0 %)\n    \"\"\"\n\n    mean_size = self._n_samples - self._splitting_ind.mean()\n    min_size = self._n_samples - self._splitting_ind.max()\n    max_size = self._n_samples - self._splitting_ind.min()\n\n    mean_pct = np.round(mean_size / self._n_samples, 4) * 100\n    max_pct = np.round(max_size / self._n_samples, 4) * 100\n    min_pct = np.round(min_size / self._n_samples, 4) * 100\n\n    print(\"Repeated Holdout method\")\n    print(\"-----------------------\")\n    print(f\"Time series size: {self._n_samples} samples\")\n    print(f\"Average validation set size: {np.round(mean_size, 4)} samples ({mean_pct} %)\")\n    print(f\"Maximum validation set size: {max_size} samples ({max_pct} %)\")\n    print(f\"Minimum validation set size: {min_size} samples ({min_pct} %)\")\n</code></pre>"},{"location":"API_ref/validation_methods/OOS/rep_holdout/#timecave.validation_methods.OOS.RepeatedHoldout.plot","title":"<code>plot(height, width)</code>","text":"<p>Plot the partitioned time series.</p> <p>This method allows the user to plot the partitioned time series. The training and validation sets are plotted using different colours. </p> <p>Parameters:</p> Name Type Description Default <code>height</code> <code>int</code> <p>The figure's height.</p> required <code>width</code> <code>int</code> <p>The figure's width.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from timecave.validation_methods.OOS import RepeatedHoldout\n&gt;&gt;&gt; ts = np.ones(100);\n&gt;&gt;&gt; splitter = RepeatedHoldout(ts, splitting_interval=[0.6, 0.9]);\n&gt;&gt;&gt; splitter.plot(10, 10);\n</code></pre> <p></p> Source code in <code>timecave/validation_methods/OOS.py</code> <pre><code>def plot(self, height: int, width: int) -&gt; None:\n    \"\"\"\n    Plot the partitioned time series.\n\n    This method allows the user to plot the partitioned time series. The training and validation sets are plotted using different colours. \n\n    Parameters\n    ----------\n    height : int\n        The figure's height.\n\n    width : int\n        The figure's width.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; from timecave.validation_methods.OOS import RepeatedHoldout\n    &gt;&gt;&gt; ts = np.ones(100);\n    &gt;&gt;&gt; splitter = RepeatedHoldout(ts, splitting_interval=[0.6, 0.9]);\n    &gt;&gt;&gt; splitter.plot(10, 10);\n\n    ![Holdout_plot_image](../../../images/RepHoldout_plot.png)\n    \"\"\"\n\n    fig, axs = plt.subplots(self._iter, 1, sharex=True)\n    fig.set_figheight(height)\n    fig.set_figwidth(width)\n    fig.supxlabel(\"Samples\")\n    fig.supylabel(\"Time Series\")\n    fig.suptitle(\"Repeated Holdout method\")\n\n    for it, (training, validation, _) in enumerate(self.split()):\n\n        axs[it].scatter(training, self._series[training], label=\"Training set\")\n        axs[it].scatter(\n            validation, self._series[validation], label=\"Validation set\"\n        )\n        axs[it].set_title(\"Iteration {}\".format(it + 1))\n        axs[it].legend()\n\n    plt.show()\n\n    return\n</code></pre>"},{"location":"API_ref/validation_methods/OOS/rep_holdout/#timecave.validation_methods.OOS.RepeatedHoldout.split","title":"<code>split()</code>","text":"<p>Split the time series into training and validation sets.</p> <p>This method splits the series' indices into disjoint sets containing the training and validation indices. At every iteration, an array of training indices and another one containing the validation indices are generated. Note that this method is a generator. To access the indices, use the <code>next()</code> method or a <code>for</code> loop.</p> <p>Yields:</p> Type Description <code>ndarray</code> <p>Array of training indices.</p> <code>ndarray</code> <p>Array of validation indices.</p> <code>float</code> <p>Used for compatibility reasons. Irrelevant for this method.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from timecave.validation_methods.OOS import RepeatedHoldout\n&gt;&gt;&gt; ts = np.ones(100);\n</code></pre> <p>If the splitting interval consists of two floats, the method assumes they define the minimum and maximum training set sizes:</p> <pre><code>&gt;&gt;&gt; splitter = RepeatedHoldout(ts, splitting_interval=[0.6, 0.8]);\n&gt;&gt;&gt; for ind, (train, val, _) in enumerate(splitter.split()):\n...\n...     print(f\"Iteration {ind+1}\");\n...     print(f\"# training samples: {train.shape[0]}\");\n...     print(f\"# validation samples: {val.shape[0]}\");\nIteration 1\n# training samples: 72\n# validation samples: 28\nIteration 2\n# training samples: 75\n# validation samples: 25\nIteration 3\n# training samples: 60\n# validation samples: 40\nIteration 4\n# training samples: 63\n# validation samples: 37\nIteration 5\n# training samples: 63\n# validation samples: 37\n</code></pre> <p>If two integers are specified instead, they will be regarded as indices.</p> <pre><code>&gt;&gt;&gt; splitter = RepeatedHoldout(ts, splitting_interval=[80, 95]);\n&gt;&gt;&gt; for ind, (train, val, _) in enumerate(splitter.split()):\n...\n...     print(f\"Iteration {ind+1}\");\n...     print(f\"# training samples: {train.shape[0]}\");\n...     print(f\"# validation samples: {val.shape[0]}\");\nIteration 1\n# training samples: 92\n# validation samples: 8\nIteration 2\n# training samples: 85\n# validation samples: 15\nIteration 3\n# training samples: 80\n# validation samples: 20\nIteration 4\n# training samples: 83\n# validation samples: 17\nIteration 5\n# training samples: 91\n# validation samples: 9\n</code></pre> Source code in <code>timecave/validation_methods/OOS.py</code> <pre><code>def split(self) -&gt; Generator[tuple[np.ndarray, np.ndarray, float], None, None]:\n    \"\"\"\n    Split the time series into training and validation sets.\n\n    This method splits the series' indices into disjoint sets containing the training and validation indices.\n    At every iteration, an array of training indices and another one containing the validation indices are generated.\n    Note that this method is a generator. To access the indices, use the `next()` method or a `for` loop.\n\n    Yields\n    ------\n    np.ndarray\n        Array of training indices.\n\n    np.ndarray\n        Array of validation indices.\n\n    float\n        Used for compatibility reasons. Irrelevant for this method.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; from timecave.validation_methods.OOS import RepeatedHoldout\n    &gt;&gt;&gt; ts = np.ones(100);\n\n    If the splitting interval consists of two floats, the method assumes they define the minimum and maximum training set sizes:\n\n    &gt;&gt;&gt; splitter = RepeatedHoldout(ts, splitting_interval=[0.6, 0.8]);\n    &gt;&gt;&gt; for ind, (train, val, _) in enumerate(splitter.split()):\n    ...\n    ...     print(f\"Iteration {ind+1}\");\n    ...     print(f\"# training samples: {train.shape[0]}\");\n    ...     print(f\"# validation samples: {val.shape[0]}\");\n    Iteration 1\n    # training samples: 72\n    # validation samples: 28\n    Iteration 2\n    # training samples: 75\n    # validation samples: 25\n    Iteration 3\n    # training samples: 60\n    # validation samples: 40\n    Iteration 4\n    # training samples: 63\n    # validation samples: 37\n    Iteration 5\n    # training samples: 63\n    # validation samples: 37\n\n    If two integers are specified instead, they will be regarded as indices.\n\n    &gt;&gt;&gt; splitter = RepeatedHoldout(ts, splitting_interval=[80, 95]);\n    &gt;&gt;&gt; for ind, (train, val, _) in enumerate(splitter.split()):\n    ...\n    ...     print(f\"Iteration {ind+1}\");\n    ...     print(f\"# training samples: {train.shape[0]}\");\n    ...     print(f\"# validation samples: {val.shape[0]}\");\n    Iteration 1\n    # training samples: 92\n    # validation samples: 8\n    Iteration 2\n    # training samples: 85\n    # validation samples: 15\n    Iteration 3\n    # training samples: 80\n    # validation samples: 20\n    Iteration 4\n    # training samples: 83\n    # validation samples: 17\n    Iteration 5\n    # training samples: 91\n    # validation samples: 9\n    \"\"\"\n\n    for ind in self._splitting_ind:\n\n        training = self._indices[:ind]\n        validation = self._indices[ind:]\n\n        yield (training, validation, 1.0)\n</code></pre>"},{"location":"API_ref/validation_methods/OOS/rep_holdout/#timecave.validation_methods.OOS.RepeatedHoldout.statistics","title":"<code>statistics()</code>","text":"<p>Compute relevant statistics for both training and validation sets.</p> <p>This method computes relevant time series features, such as mean, strength-of-trend, etc. for both the whole time series, the training set and the validation set. It can and should be used to ensure that the characteristics of both the training and validation sets are, statistically speaking, similar to those of the time series one wishes to forecast. If this is not the case, using the validation method will most likely lead to a poor assessment of the model's performance.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Relevant features for the entire time series.</p> <code>DataFrame</code> <p>Relevant features for the training set.</p> <code>DataFrame</code> <p>Relevant features for the validation set.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the time series is composed of less than three samples.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from timecave.validation_methods.OOS import RepeatedHoldout\n&gt;&gt;&gt; ts = np.hstack((np.ones(5), np.zeros(5)));\n&gt;&gt;&gt; splitter = RepeatedHoldout(ts, splitting_interval=[0.6, 0.9]);\n&gt;&gt;&gt; ts_stats, training_stats, validation_stats = splitter.statistics();\nFrequency features are only meaningful if the correct sampling frequency is passed to the class.\n&gt;&gt;&gt; ts_stats\n   Mean  Median  Min  Max  Variance  P2P_amplitude  Trend_slope  Spectral_centroid  Spectral_rolloff  Spectral_entropy  Strength_of_trend  Mean_crossing_rate  Median_crossing_rate\n0   0.5     0.5  0.0  1.0      0.25            1.0    -0.151515           0.114058               0.5           0.38717            1.59099            0.111111              0.111111\n&gt;&gt;&gt; training_stats\n       Mean  Median  Min  Max  Variance  P2P_amplitude  Trend_slope  Spectral_centroid  Spectral_rolloff  Spectral_entropy  Strength_of_trend  Mean_crossing_rate  Median_crossing_rate\n0  0.833333     1.0  0.0  1.0  0.138889            1.0    -0.142857           0.125000          0.500000          0.792481           0.931695            0.200000              0.200000\n0  0.714286     1.0  0.0  1.0  0.204082            1.0    -0.178571           0.094706          0.428571          0.556506           1.212183            0.166667              0.166667\n0  0.833333     1.0  0.0  1.0  0.138889            1.0    -0.142857           0.125000          0.500000          0.792481           0.931695            0.200000              0.200000\n0  0.714286     1.0  0.0  1.0  0.204082            1.0    -0.178571           0.094706          0.428571          0.556506           1.212183            0.166667              0.166667\n0  0.714286     1.0  0.0  1.0  0.204082            1.0    -0.178571           0.094706          0.428571          0.556506           1.212183            0.166667              0.166667\n&gt;&gt;&gt; validation_stats\n   Mean  Median  Min  Max  Variance  P2P_amplitude  Trend_slope  Spectral_centroid  Spectral_rolloff  Spectral_entropy  Strength_of_trend  Mean_crossing_rate  Median_crossing_rate\n0   0.0     0.0  0.0  0.0       0.0            0.0          0.0                  0               0.0               0.0                inf                 0.0                   0.0\n0   0.0     0.0  0.0  0.0       0.0            0.0          0.0                  0               0.0               0.0                inf                 0.0                   0.0\n0   0.0     0.0  0.0  0.0       0.0            0.0          0.0                  0               0.0               0.0                inf                 0.0                   0.0\n0   0.0     0.0  0.0  0.0       0.0            0.0          0.0                  0               0.0               0.0                inf                 0.0                   0.0\n0   0.0     0.0  0.0  0.0       0.0            0.0          0.0                  0               0.0               0.0                inf                 0.0                   0.0\n</code></pre> Source code in <code>timecave/validation_methods/OOS.py</code> <pre><code>def statistics(self) -&gt; tuple[pd.DataFrame]:\n    \"\"\"\n    Compute relevant statistics for both training and validation sets.\n\n    This method computes relevant time series features, such as mean, strength-of-trend, etc. for both the whole time series, the training set and the validation set.\n    It can and should be used to ensure that the characteristics of both the training and validation sets are, statistically speaking, similar to those of the time series one wishes to forecast.\n    If this is not the case, using the validation method will most likely lead to a poor assessment of the model's performance.\n\n    Returns\n    -------\n    pd.DataFrame\n        Relevant features for the entire time series.\n\n    pd.DataFrame\n        Relevant features for the training set.\n\n    pd.DataFrame\n        Relevant features for the validation set.\n\n    Raises\n    ------\n    ValueError\n        If the time series is composed of less than three samples.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; from timecave.validation_methods.OOS import RepeatedHoldout\n    &gt;&gt;&gt; ts = np.hstack((np.ones(5), np.zeros(5)));\n    &gt;&gt;&gt; splitter = RepeatedHoldout(ts, splitting_interval=[0.6, 0.9]);\n    &gt;&gt;&gt; ts_stats, training_stats, validation_stats = splitter.statistics();\n    Frequency features are only meaningful if the correct sampling frequency is passed to the class.\n    &gt;&gt;&gt; ts_stats\n       Mean  Median  Min  Max  Variance  P2P_amplitude  Trend_slope  Spectral_centroid  Spectral_rolloff  Spectral_entropy  Strength_of_trend  Mean_crossing_rate  Median_crossing_rate\n    0   0.5     0.5  0.0  1.0      0.25            1.0    -0.151515           0.114058               0.5           0.38717            1.59099            0.111111              0.111111\n    &gt;&gt;&gt; training_stats\n           Mean  Median  Min  Max  Variance  P2P_amplitude  Trend_slope  Spectral_centroid  Spectral_rolloff  Spectral_entropy  Strength_of_trend  Mean_crossing_rate  Median_crossing_rate\n    0  0.833333     1.0  0.0  1.0  0.138889            1.0    -0.142857           0.125000          0.500000          0.792481           0.931695            0.200000              0.200000\n    0  0.714286     1.0  0.0  1.0  0.204082            1.0    -0.178571           0.094706          0.428571          0.556506           1.212183            0.166667              0.166667\n    0  0.833333     1.0  0.0  1.0  0.138889            1.0    -0.142857           0.125000          0.500000          0.792481           0.931695            0.200000              0.200000\n    0  0.714286     1.0  0.0  1.0  0.204082            1.0    -0.178571           0.094706          0.428571          0.556506           1.212183            0.166667              0.166667\n    0  0.714286     1.0  0.0  1.0  0.204082            1.0    -0.178571           0.094706          0.428571          0.556506           1.212183            0.166667              0.166667\n    &gt;&gt;&gt; validation_stats\n       Mean  Median  Min  Max  Variance  P2P_amplitude  Trend_slope  Spectral_centroid  Spectral_rolloff  Spectral_entropy  Strength_of_trend  Mean_crossing_rate  Median_crossing_rate\n    0   0.0     0.0  0.0  0.0       0.0            0.0          0.0                  0               0.0               0.0                inf                 0.0                   0.0\n    0   0.0     0.0  0.0  0.0       0.0            0.0          0.0                  0               0.0               0.0                inf                 0.0                   0.0\n    0   0.0     0.0  0.0  0.0       0.0            0.0          0.0                  0               0.0               0.0                inf                 0.0                   0.0\n    0   0.0     0.0  0.0  0.0       0.0            0.0          0.0                  0               0.0               0.0                inf                 0.0                   0.0\n    0   0.0     0.0  0.0  0.0       0.0            0.0          0.0                  0               0.0               0.0                inf                 0.0                   0.0\n    \"\"\"\n\n    if self._n_samples &lt;= 2:\n\n        raise ValueError(\n            \"Basic statistics can only be computed if the time series comprises more than two samples.\"\n        )\n\n    print(\"Frequency features are only meaningful if the correct sampling frequency is passed to the class.\")\n\n    full_features = get_features(self._series, self.sampling_freq)\n\n    training_stats = []\n    validation_stats = []\n\n    # for ind in self._splitting_ind:\n\n    #    training_feat = get_features(self._series[:ind], self.sampling_freq);\n    #    validation_feat = get_features(self._series[ind:], self.sampling_freq);\n    #    training_stats.append(training_feat);\n    #    validation_stats.append(validation_feat);\n\n    for training, validation, _ in self.split():\n\n        if self._series[training].shape[0] &gt;= 2:\n\n            training_feat = get_features(self._series[training], self.sampling_freq)\n            training_stats.append(training_feat)\n\n        else:\n\n            warn(\n                \"The training set is too small to compute most meaningful features.\"\n            )\n\n        if self._series[validation].shape[0] &gt;= 2:\n\n            validation_feat = get_features(\n                self._series[validation], self.sampling_freq\n            )\n            validation_stats.append(validation_feat)\n\n        else:\n\n            warn(\n                \"The validation set is too small to compute most meaningful features.\"\n            )\n\n    training_features = pd.concat(training_stats)\n    validation_features = pd.concat(validation_stats)\n\n    return (full_features, training_features, validation_features)\n</code></pre>"},{"location":"API_ref/validation_methods/OOS/roll_recal/","title":"Rolling Origin Recalibration method","text":""},{"location":"API_ref/validation_methods/OOS/roll_recal/#timecave.validation_methods.OOS.RollingOriginRecalibration","title":"<code>timecave.validation_methods.OOS.RollingOriginRecalibration(ts, fs=1, origin=0.7)</code>","text":"<p>               Bases: <code>BaseSplitter</code></p> <p>Implements the Rolling Origin Recalibration method.</p> <p>This class implements the Rolling Origin Recalibration method. This method splits the data into various training and validation sets. Neither the training sets nor the validation sets are disjoint. At every iteration, a single data point is dropped from the validation set and added     to the training set.</p> <p>Parameters:</p> Name Type Description Default <code>ts</code> <code>ndarray | Series</code> <p>Univariate time series.</p> required <code>fs</code> <code>float | int</code> <p>Sampling frequency (Hz).</p> <code>1</code> <code>origin</code> <code>int | float</code> <p>The point from which the data is split.         If an integer is passed, it is interpreted as an index.         If a float is passed instead, it is treated as the percentage of samples         that should be used for training.</p> <code>0.7</code> <p>Attributes:</p> Name Type Description <code>n_splits</code> <code>int</code> <p>The number of splits.</p> <code>sampling_freq</code> <code>int | float</code> <p>The series' sampling frequency (Hz).</p> <p>Methods:</p> Name Description <code>split</code> <p>Split the time series into training and validation sets.</p> <code>info</code> <p>Provide additional information on the validation method.</p> <code>statistics</code> <p>Compute relevant statistics for both training and validation sets.</p> <code>plot</code> <p>Plot the partitioned time series.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If <code>origin</code> is neither an integer nor a float.</p> <code>ValueError</code> <p>If <code>origin</code> is a float that does not lie in the ]0, 1[ interval.</p> <code>ValueError</code> <p>If <code>origin</code> is an integer that does not lie in the ]0, n_samples[ interval.</p> Warning <p>Depending on the time series' size, this method can have a large computational cost.</p> See also <p>Rolling Origin Update: Similar to the Rolling Origin Recalibration method, but the model is only trained once.</p> Notes <p>The Rolling Origin Recalibration method consists of splitting the data into a training set and a validation set,     with the former preceding the latter. At every iteration, a single data point (the one closest to the training set) is dropped from the     validation set and added to the training set. The model's performance is then assessed on the new validation set. This process ends once     the validation set consists of a single data point. The estimate of the true model error is the average validation     error across [over] all iterations.</p> <p></p> <p>For more details on this method, the reader should refer to [1].</p> References Source code in <code>timecave/validation_methods/OOS.py</code> <pre><code>def __init__(\n    self, ts: np.ndarray | pd.Series, fs: float | int = 1, origin: int | float = 0.7\n) -&gt; None:\n\n    super().__init__(2, ts, fs)\n    self._check_origin(origin)\n    self._origin = self._convert_origin(origin)\n    self._splitting_ind = np.arange(self._origin + 1, self._n_samples)\n    self._n_splits = self._splitting_ind.shape[0]\n\n    return\n</code></pre>"},{"location":"API_ref/validation_methods/OOS/roll_recal/#timecave.validation_methods.OOS.RollingOriginRecalibration--1","title":"1","text":"<p>Leonard J Tashman. Out-of-sample tests of forecasting accuracy: an analysis and review. International journal of forecasting, 16(4):437\u2013450, 2000.</p>"},{"location":"API_ref/validation_methods/OOS/roll_recal/#timecave.validation_methods.OOS.RollingOriginRecalibration.info","title":"<code>info()</code>","text":"<p>Provide some basic information on the training and validation sets.</p> <p>This method displays the minimum and maximum sizes for both the training and validation sets.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from timecave.validation_methods.OOS import RollingOriginRecalibration\n&gt;&gt;&gt; ts = np.ones(10);\n&gt;&gt;&gt; splitter = RollingOriginRecalibration(ts);\n&gt;&gt;&gt; splitter.info();\nRolling Origin Recalibration method\n-----------------------------------\nTime series size: 10 samples\nMinimum training set size: 7 samples (70.0 %)\nMaximum validation set size: 3 samples (30.0 %)\nMaximum training set size: 9 samples (90.0 %)\nMinimum validation set size: 1 samples (10.0 %)\n</code></pre> Source code in <code>timecave/validation_methods/OOS.py</code> <pre><code>def info(self) -&gt; None:\n    \"\"\"\n    Provide some basic information on the training and validation sets.\n\n    This method displays the minimum and maximum sizes for both the training and validation sets.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; from timecave.validation_methods.OOS import RollingOriginRecalibration\n    &gt;&gt;&gt; ts = np.ones(10);\n    &gt;&gt;&gt; splitter = RollingOriginRecalibration(ts);\n    &gt;&gt;&gt; splitter.info();\n    Rolling Origin Recalibration method\n    -----------------------------------\n    Time series size: 10 samples\n    Minimum training set size: 7 samples (70.0 %)\n    Maximum validation set size: 3 samples (30.0 %)\n    Maximum training set size: 9 samples (90.0 %)\n    Minimum validation set size: 1 samples (10.0 %)\n    \"\"\"\n\n    max_training_size = self._n_samples - 1\n    min_training_size = self._origin + 1\n    max_validation_size = self._n_samples - self._origin - 1\n    min_validation_size = 1\n\n    max_training_pct = np.round(max_training_size / self._n_samples, 4) * 100\n    min_training_pct = np.round(min_training_size / self._n_samples, 4) * 100\n    max_validation_pct = np.round(max_validation_size / self._n_samples, 4) * 100\n    min_validation_pct = np.round(min_validation_size / self._n_samples, 4) * 100\n\n    print(\"Rolling Origin Recalibration method\")\n    print(\"-----------------------------------\")\n    print(f\"Time series size: {self._n_samples} samples\")\n    print(\n        f\"Minimum training set size: {min_training_size} samples ({min_training_pct} %)\"\n    )\n    print(\n        f\"Maximum validation set size: {max_validation_size} samples ({max_validation_pct} %)\"\n    )\n    print(\n        f\"Maximum training set size: {max_training_size} samples ({max_training_pct} %)\"\n    )\n    print(\n        f\"Minimum validation set size: {min_validation_size} samples ({min_validation_pct} %)\"\n    )\n\n    return\n</code></pre>"},{"location":"API_ref/validation_methods/OOS/roll_recal/#timecave.validation_methods.OOS.RollingOriginRecalibration.plot","title":"<code>plot(height, width)</code>","text":"<p>Plot the partitioned time series.</p> <p>This method allows the user to plot the partitioned time series. The training and validation sets are plotted using different colours.</p> <p>Parameters:</p> Name Type Description Default <code>height</code> <code>int</code> <p>The figure's height.</p> required <code>width</code> <code>int</code> <p>The figure's width.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from timecave.validation_methods.OOS import RollingOriginRecalibration\n&gt;&gt;&gt; ts = np.arange(1, 11);\n&gt;&gt;&gt; splitter = RollingOriginRecalibration(ts);\n&gt;&gt;&gt; splitter.plot(10, 10);\n</code></pre> <p></p> Source code in <code>timecave/validation_methods/OOS.py</code> <pre><code>def plot(self, height: int, width: int) -&gt; None:\n    \"\"\"\n    Plot the partitioned time series.\n\n    This method allows the user to plot the partitioned time series. The training and validation sets are plotted using different colours.\n\n    Parameters\n    ----------\n    height : int\n        The figure's height.\n\n    width : int\n        The figure's width.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; from timecave.validation_methods.OOS import RollingOriginRecalibration\n    &gt;&gt;&gt; ts = np.arange(1, 11);\n    &gt;&gt;&gt; splitter = RollingOriginRecalibration(ts);\n    &gt;&gt;&gt; splitter.plot(10, 10);\n\n    ![Holdout_plot_image](../../../images/RollRecal_plot.png)\n    \"\"\"\n\n    fig, axs = plt.subplots(self._n_samples - self._origin - 1, 1, sharex=True)\n    fig.set_figheight(height)\n    fig.set_figwidth(width)\n    fig.supxlabel(\"Samples\")\n    fig.supylabel(\"Time Series\")\n    fig.suptitle(\"Rolling Origin Recalibration method\")\n\n    for it, (training, validation, _) in enumerate(self.split()):\n\n        axs[it].scatter(training, self._series[training], label=\"Training set\")\n        axs[it].scatter(\n            validation, self._series[validation], label=\"Validation set\"\n        )\n        axs[it].set_title(\"Iteration {}\".format(it + 1))\n        axs[it].legend()\n\n    plt.show()\n\n    return\n</code></pre>"},{"location":"API_ref/validation_methods/OOS/roll_recal/#timecave.validation_methods.OOS.RollingOriginRecalibration.split","title":"<code>split()</code>","text":"<p>Split the time series into training and validation sets.</p> <p>This method splits the series' indices into disjoint sets containing the training and validation indices. At every iteration, an array of training indices and another one containing the validation indices are generated. Note that this method is a generator. To access the indices, use the <code>next()</code> method or a <code>for</code> loop.</p> <p>Yields:</p> Type Description <code>ndarray</code> <p>Array of training indices.</p> <code>ndarray</code> <p>Array of validation indices.</p> <code>float</code> <p>Used for compatibility reasons. Irrelevant for this method.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from timecave.validation_methods.OOS import RollingOriginRecalibration\n&gt;&gt;&gt; ts = np.ones(10);\n&gt;&gt;&gt; splitter = RollingOriginRecalibration(ts);\n&gt;&gt;&gt; for ind, (train, val, _) in enumerate(splitter.split()):\n...\n...     print(f\"Iteration {ind+1}\");\n...     print(f\"Training set indices: {train}\");\n...     print(f\"Validation set indices: {val}\");\nIteration 1\nTraining set indices: [0 1 2 3 4 5 6]\nValidation set indices: [7 8 9]\nIteration 2\nTraining set indices: [0 1 2 3 4 5 6 7]\nValidation set indices: [8 9]\nIteration 3\nTraining set indices: [0 1 2 3 4 5 6 7 8]\nValidation set indices: [9]\n</code></pre> Source code in <code>timecave/validation_methods/OOS.py</code> <pre><code>def split(self) -&gt; Generator[tuple[np.ndarray, np.ndarray, float], None, None]:\n    \"\"\"\n    Split the time series into training and validation sets.\n\n    This method splits the series' indices into disjoint sets containing the training and validation indices.\n    At every iteration, an array of training indices and another one containing the validation indices are generated.\n    Note that this method is a generator. To access the indices, use the `next()` method or a `for` loop.\n\n    Yields\n    ------\n    np.ndarray\n        Array of training indices.\n\n    np.ndarray\n        Array of validation indices.\n\n    float\n        Used for compatibility reasons. Irrelevant for this method.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; from timecave.validation_methods.OOS import RollingOriginRecalibration\n    &gt;&gt;&gt; ts = np.ones(10);\n    &gt;&gt;&gt; splitter = RollingOriginRecalibration(ts);\n    &gt;&gt;&gt; for ind, (train, val, _) in enumerate(splitter.split()):\n    ...\n    ...     print(f\"Iteration {ind+1}\");\n    ...     print(f\"Training set indices: {train}\");\n    ...     print(f\"Validation set indices: {val}\");\n    Iteration 1\n    Training set indices: [0 1 2 3 4 5 6]\n    Validation set indices: [7 8 9]\n    Iteration 2\n    Training set indices: [0 1 2 3 4 5 6 7]\n    Validation set indices: [8 9]\n    Iteration 3\n    Training set indices: [0 1 2 3 4 5 6 7 8]\n    Validation set indices: [9]\n    \"\"\"\n\n    for ind in self._splitting_ind:\n\n        training = self._indices[:ind]\n        validation = self._indices[ind:]\n\n        yield (training, validation, 1.0)\n</code></pre>"},{"location":"API_ref/validation_methods/OOS/roll_recal/#timecave.validation_methods.OOS.RollingOriginRecalibration.statistics","title":"<code>statistics()</code>","text":"<p>Compute relevant statistics for both training and validation sets.</p> <p>This method computes relevant time series features, such as mean, strength-of-trend, etc. for both the whole time series, the training set and the validation set. It can and should be used to ensure that the characteristics of both the training and validation sets are, statistically speaking, similar to those of the time series one wishes to forecast. If this is not the case, using the validation method will most likely lead to a poor assessment of the model's performance.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Relevant features for the entire time series.</p> <code>DataFrame</code> <p>Relevant features for the training set.</p> <code>DataFrame</code> <p>Relevant features for the validation set.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the time series is composed of less than three samples.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from timecave.validation_methods.OOS import RollingOriginRecalibration\n&gt;&gt;&gt; ts = np.hstack((np.ones(5), np.zeros(5)));\n&gt;&gt;&gt; splitter = RollingOriginRecalibration(ts);\n&gt;&gt;&gt; ts_stats, training_stats, validation_stats = splitter.statistics();\nFrequency features are only meaningful if the correct sampling frequency is passed to the class.\nTraining and validation set features can only computed if each set is composed of two or more samples.\n&gt;&gt;&gt; ts_stats\n   Mean  Median  Min  Max  Variance  P2P_amplitude  Trend_slope  Spectral_centroid  Spectral_rolloff  Spectral_entropy  Strength_of_trend  Mean_crossing_rate  Median_crossing_rate\n0   0.5     0.5  0.0  1.0      0.25            1.0    -0.151515           0.114058               0.5           0.38717            1.59099            0.111111              0.111111\n&gt;&gt;&gt; training_stats\n       Mean  Median  Min  Max  Variance  P2P_amplitude  Trend_slope  Spectral_centroid  Spectral_rolloff  Spectral_entropy  Strength_of_trend  Mean_crossing_rate  Median_crossing_rate\n0  0.714286     1.0  0.0  1.0  0.204082            1.0    -0.178571           0.094706          0.428571          0.556506           1.212183            0.166667              0.166667\n0  0.625000     1.0  0.0  1.0  0.234375            1.0    -0.178571           0.122818          0.500000          0.600876           1.383496            0.142857              0.142857\n0  0.555556     1.0  0.0  1.0  0.246914            1.0    -0.166667           0.105483          0.444444          0.385860           1.502496            0.125000              0.125000\n&gt;&gt;&gt; validation_stats\n   Mean  Median  Min  Max  Variance  P2P_amplitude  Trend_slope  Spectral_centroid  Spectral_rolloff  Spectral_entropy  Strength_of_trend  Mean_crossing_rate  Median_crossing_rate\n0   0.0     0.0  0.0  0.0       0.0            0.0          0.0                  0               0.0               0.0                inf                 0.0                   0.0\n0   0.0     0.0  0.0  0.0       0.0            0.0          0.0                  0               0.0               0.0                inf                 0.0                   0.0\n</code></pre> Source code in <code>timecave/validation_methods/OOS.py</code> <pre><code>def statistics(self) -&gt; tuple[pd.DataFrame]:\n    \"\"\"\n    Compute relevant statistics for both training and validation sets.\n\n    This method computes relevant time series features, such as mean, strength-of-trend, etc. for both the whole time series, the training set and the validation set.\n    It can and should be used to ensure that the characteristics of both the training and validation sets are, statistically speaking, similar to those of the time series one wishes to forecast.\n    If this is not the case, using the validation method will most likely lead to a poor assessment of the model's performance.\n\n    Returns\n    -------\n    pd.DataFrame\n        Relevant features for the entire time series.\n\n    pd.DataFrame\n        Relevant features for the training set.\n\n    pd.DataFrame\n        Relevant features for the validation set.\n\n    Raises\n    ------\n    ValueError\n        If the time series is composed of less than three samples.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; from timecave.validation_methods.OOS import RollingOriginRecalibration\n    &gt;&gt;&gt; ts = np.hstack((np.ones(5), np.zeros(5)));\n    &gt;&gt;&gt; splitter = RollingOriginRecalibration(ts);\n    &gt;&gt;&gt; ts_stats, training_stats, validation_stats = splitter.statistics();\n    Frequency features are only meaningful if the correct sampling frequency is passed to the class.\n    Training and validation set features can only computed if each set is composed of two or more samples.\n    &gt;&gt;&gt; ts_stats\n       Mean  Median  Min  Max  Variance  P2P_amplitude  Trend_slope  Spectral_centroid  Spectral_rolloff  Spectral_entropy  Strength_of_trend  Mean_crossing_rate  Median_crossing_rate\n    0   0.5     0.5  0.0  1.0      0.25            1.0    -0.151515           0.114058               0.5           0.38717            1.59099            0.111111              0.111111\n    &gt;&gt;&gt; training_stats\n           Mean  Median  Min  Max  Variance  P2P_amplitude  Trend_slope  Spectral_centroid  Spectral_rolloff  Spectral_entropy  Strength_of_trend  Mean_crossing_rate  Median_crossing_rate\n    0  0.714286     1.0  0.0  1.0  0.204082            1.0    -0.178571           0.094706          0.428571          0.556506           1.212183            0.166667              0.166667\n    0  0.625000     1.0  0.0  1.0  0.234375            1.0    -0.178571           0.122818          0.500000          0.600876           1.383496            0.142857              0.142857\n    0  0.555556     1.0  0.0  1.0  0.246914            1.0    -0.166667           0.105483          0.444444          0.385860           1.502496            0.125000              0.125000\n    &gt;&gt;&gt; validation_stats\n       Mean  Median  Min  Max  Variance  P2P_amplitude  Trend_slope  Spectral_centroid  Spectral_rolloff  Spectral_entropy  Strength_of_trend  Mean_crossing_rate  Median_crossing_rate\n    0   0.0     0.0  0.0  0.0       0.0            0.0          0.0                  0               0.0               0.0                inf                 0.0                   0.0\n    0   0.0     0.0  0.0  0.0       0.0            0.0          0.0                  0               0.0               0.0                inf                 0.0                   0.0\n    \"\"\"\n\n    if self._n_samples &lt;= 2:\n\n        raise ValueError(\n            \"Basic statistics can only be computed if the time series comprises more than two samples.\"\n        )\n\n    print(\"Frequency features are only meaningful if the correct sampling frequency is passed to the class.\")\n\n    full_features = get_features(self._series, self.sampling_freq)\n    training_stats = []\n    validation_stats = []\n\n    print(\n        \"Training and validation set features can only computed if each set is composed of two or more samples.\"\n    )\n\n    for training, validation, _ in self.split():\n\n        if self._series[training].shape[0] &gt;= 2:\n\n            training_feat = get_features(self._series[training], self.sampling_freq)\n            training_stats.append(training_feat)\n\n        if self._series[validation].shape[0] &gt;= 2:\n\n            validation_feat = get_features(\n                self._series[validation], self.sampling_freq\n            )\n            validation_stats.append(validation_feat)\n\n    training_features = pd.concat(training_stats)\n    validation_features = pd.concat(validation_stats)\n\n    return (full_features, training_features, validation_features)\n</code></pre>"},{"location":"API_ref/validation_methods/OOS/roll_update/","title":"Rolling Origin Update method","text":""},{"location":"API_ref/validation_methods/OOS/roll_update/#timecave.validation_methods.OOS.RollingOriginUpdate","title":"<code>timecave.validation_methods.OOS.RollingOriginUpdate(ts, fs=1, origin=0.7)</code>","text":"<p>               Bases: <code>BaseSplitter</code></p> <p>Implements the Rolling Origin Update method.</p> <p>This class implements the Rolling Origin Update method. This method splits the data into a single training set and several validation sets. The validation sets are NOT disjoint, and, even though the model is validated several times, it only undergoes the training process once.</p> <p>Parameters:</p> Name Type Description Default <code>ts</code> <code>ndarray | Series</code> <p>Univariate time series.</p> required <code>fs</code> <code>float | int</code> <p>Sampling frequency (Hz).</p> <code>1</code> <code>origin</code> <code>int | float</code> <p>The point from which the data is split.         If an integer is passed, it is interpreted as an index.         If a float is passed instead, it is treated as the percentage of samples         that should be used for training.</p> <code>0.7</code> <p>Attributes:</p> Name Type Description <code>n_splits</code> <code>int</code> <p>The number of splits.</p> <code>sampling_freq</code> <code>int | float</code> <p>The series' sampling frequency (Hz).</p> <p>Methods:</p> Name Description <code>split</code> <p>Split the time series into training and validation sets.</p> <code>info</code> <p>Provide additional information on the validation method.</p> <code>statistics</code> <p>Compute relevant statistics for both training and validation sets.</p> <code>plot</code> <p>Plot the partitioned time series.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If <code>origin</code> is neither an integer nor a float.</p> <code>ValueError</code> <p>If <code>origin</code> is a float that does not lie in the ]0, 1[ interval.</p> <code>ValueError</code> <p>If <code>origin</code> is an integer that does not lie in the ]0, n_samples[ interval.</p> Warning <p>The model should only be trained ONCE.</p> See also <p>Rolling Origin Recalibration: Similar to the Rolling Origin Update method, but the training set is updated along with the validation set.</p> Notes <p>The Rolling Origin Update method consists of splitting the data into a training set and a validation set,     with the former preceding the latter. The model is only trained once.     Then, at every iteration, a single data point (the one closest to the training set) is dropped from the     validation set, and the model's performance is assessed using the new validation set. This process ends once     the validation set consists of a single data point. The estimate of the true model error is the average validation     error.</p> <p></p> <p>For more details on this method, the reader should refer to [1].</p> References Source code in <code>timecave/validation_methods/OOS.py</code> <pre><code>def __init__(\n    self, ts: np.ndarray | pd.Series, fs: float | int = 1, origin: int | float = 0.7\n) -&gt; None:\n\n    super().__init__(2, ts, fs)\n    self._check_origin(origin)\n    self._origin = self._convert_origin(origin)\n    self._splitting_ind = np.arange(self._origin + 1, self._n_samples)\n    self._n_splits = self._splitting_ind.shape[0]\n\n    return\n</code></pre>"},{"location":"API_ref/validation_methods/OOS/roll_update/#timecave.validation_methods.OOS.RollingOriginUpdate--1","title":"1","text":"<p>Leonard J Tashman. Out-of-sample tests of forecasting accuracy: an analysis and review. International journal of forecasting, 16(4):437\u2013450, 2000.</p>"},{"location":"API_ref/validation_methods/OOS/roll_update/#timecave.validation_methods.OOS.RollingOriginUpdate.info","title":"<code>info()</code>","text":"<p>Provide some basic information on the training and validation sets.</p> <p>This method displays the training set size and both the minimum and maximum validation set size.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from timecave.validation_methods.OOS import RollingOriginUpdate\n&gt;&gt;&gt; ts = np.ones(10);\n&gt;&gt;&gt; splitter = RollingOriginUpdate(ts);\n&gt;&gt;&gt; splitter.info();\nRolling Origin Update method\n----------------------------\nTime series size: 10 samples\nTraining set size (fixed parameter): 7 samples (70.0 %)\nMaximum validation set size: 3 samples (30.0 %)\nMinimum validation set size: 1 sample (10.0 %)\n</code></pre> Source code in <code>timecave/validation_methods/OOS.py</code> <pre><code>def info(self) -&gt; None:\n    \"\"\"\n    Provide some basic information on the training and validation sets.\n\n    This method displays the training set size and both the minimum and maximum validation set size.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; from timecave.validation_methods.OOS import RollingOriginUpdate\n    &gt;&gt;&gt; ts = np.ones(10);\n    &gt;&gt;&gt; splitter = RollingOriginUpdate(ts);\n    &gt;&gt;&gt; splitter.info();\n    Rolling Origin Update method\n    ----------------------------\n    Time series size: 10 samples\n    Training set size (fixed parameter): 7 samples (70.0 %)\n    Maximum validation set size: 3 samples (30.0 %)\n    Minimum validation set size: 1 sample (10.0 %)\n    \"\"\"\n\n    training_size = self._origin + 1\n    max_size = self._n_samples - self._origin - 1\n    min_size = 1\n\n    training_pct = np.round(training_size / self._n_samples, 4) * 100\n    max_pct = np.round(max_size / self._n_samples, 4) * 100\n    min_pct = np.round(1 / self._n_samples, 4) * 100\n\n    print(\"Rolling Origin Update method\")\n    print(\"----------------------------\")\n    print(f\"Time series size: {self._n_samples} samples\")\n    print(\n        f\"Training set size (fixed parameter): {training_size} samples ({training_pct} %)\"\n    )\n    print(f\"Maximum validation set size: {max_size} samples ({max_pct} %)\")\n    print(f\"Minimum validation set size: {min_size} sample ({min_pct} %)\")\n\n    return\n</code></pre>"},{"location":"API_ref/validation_methods/OOS/roll_update/#timecave.validation_methods.OOS.RollingOriginUpdate.plot","title":"<code>plot(height, width)</code>","text":"<p>Plot the partitioned time series.</p> <p>This method allows the user to plot the partitioned time series. The training and validation sets are plotted using different colours.</p> <p>Parameters:</p> Name Type Description Default <code>height</code> <code>int</code> <p>The figure's height.</p> required <code>width</code> <code>int</code> <p>The figure's width.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from timecave.validation_methods.OOS import RollingOriginUpdate\n&gt;&gt;&gt; ts = np.arange(1, 11);\n&gt;&gt;&gt; splitter = RollingOriginUpdate(ts);\n&gt;&gt;&gt; splitter.plot(10, 10);\n</code></pre> <p></p> Source code in <code>timecave/validation_methods/OOS.py</code> <pre><code>def plot(self, height: int, width: int) -&gt; None:\n    \"\"\"\n    Plot the partitioned time series.\n\n    This method allows the user to plot the partitioned time series. The training and validation sets are plotted using different colours.\n\n    Parameters\n    ----------\n    height : int\n        The figure's height.\n\n    width : int\n        The figure's width.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; from timecave.validation_methods.OOS import RollingOriginUpdate\n    &gt;&gt;&gt; ts = np.arange(1, 11);\n    &gt;&gt;&gt; splitter = RollingOriginUpdate(ts);\n    &gt;&gt;&gt; splitter.plot(10, 10);\n\n    ![Update_plot_image](../../../images/RollUpdate_plot.png)\n    \"\"\"\n\n    fig, axs = plt.subplots(self._n_samples - self._origin - 1, 1, sharex=True)\n    fig.set_figheight(height)\n    fig.set_figwidth(width)\n    fig.supxlabel(\"Samples\")\n    fig.supylabel(\"Time Series\")\n    fig.suptitle(\"Rolling Origin Update method\")\n\n    for it, (training, validation, _) in enumerate(self.split()):\n\n        axs[it].scatter(training, self._series[training], label=\"Training set\")\n        axs[it].scatter(\n            validation, self._series[validation], label=\"Validation set\"\n        )\n        axs[it].set_title(\"Iteration {}\".format(it + 1))\n        axs[it].legend()\n\n    plt.show()\n\n    return\n</code></pre>"},{"location":"API_ref/validation_methods/OOS/roll_update/#timecave.validation_methods.OOS.RollingOriginUpdate.split","title":"<code>split()</code>","text":"<p>Split the time series into training and validation sets.</p> <p>This method splits the series' indices into disjoint sets containing the training and validation indices. At every iteration, an array of training indices and another one containing the validation indices are generated. Note that this method is a generator. To access the indices, use the <code>next()</code> method or a <code>for</code> loop.</p> <p>Yields:</p> Type Description <code>ndarray</code> <p>Array of training indices.</p> <code>ndarray</code> <p>Array of validation indices.</p> <code>float</code> <p>Used for compatibility reasons. Irrelevant for this method.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from timecave.validation_methods.OOS import RollingOriginUpdate\n&gt;&gt;&gt; ts = np.ones(10);\n&gt;&gt;&gt; splitter = RollingOriginUpdate(ts);\n&gt;&gt;&gt; for ind, (train, val, _) in enumerate(splitter.split()):\n...\n...     print(f\"Iteration {ind+1}\");\n...     print(f\"Validation set indices: {val}\");\n...     print(f\"Validation set size: {val.shape[0]}\");\nIteration 1\nValidation set indices: [7 8 9]\nValidation set size: 3\nIteration 2\nValidation set indices: [8 9]\nValidation set size: 2\nIteration 3\nValidation set indices: [9]\nValidation set size: 1\n</code></pre> Source code in <code>timecave/validation_methods/OOS.py</code> <pre><code>def split(self) -&gt; Generator[tuple[np.ndarray, np.ndarray, float], None, None]:\n    \"\"\"\n    Split the time series into training and validation sets.\n\n    This method splits the series' indices into disjoint sets containing the training and validation indices.\n    At every iteration, an array of training indices and another one containing the validation indices are generated.\n    Note that this method is a generator. To access the indices, use the `next()` method or a `for` loop.\n\n    Yields\n    ------\n    np.ndarray\n        Array of training indices.\n\n    np.ndarray\n        Array of validation indices.\n\n    float\n        Used for compatibility reasons. Irrelevant for this method.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; from timecave.validation_methods.OOS import RollingOriginUpdate\n    &gt;&gt;&gt; ts = np.ones(10);\n    &gt;&gt;&gt; splitter = RollingOriginUpdate(ts);\n    &gt;&gt;&gt; for ind, (train, val, _) in enumerate(splitter.split()):\n    ...\n    ...     print(f\"Iteration {ind+1}\");\n    ...     print(f\"Validation set indices: {val}\");\n    ...     print(f\"Validation set size: {val.shape[0]}\");\n    Iteration 1\n    Validation set indices: [7 8 9]\n    Validation set size: 3\n    Iteration 2\n    Validation set indices: [8 9]\n    Validation set size: 2\n    Iteration 3\n    Validation set indices: [9]\n    Validation set size: 1\n    \"\"\"\n\n    for ind in self._splitting_ind:\n\n        training = self._indices[: self._origin + 1]\n        validation = self._indices[ind:]\n\n        yield (training, validation, 1.0)\n</code></pre>"},{"location":"API_ref/validation_methods/OOS/roll_update/#timecave.validation_methods.OOS.RollingOriginUpdate.statistics","title":"<code>statistics()</code>","text":"<p>Compute relevant statistics for both training and validation sets.</p> <p>This method computes relevant time series features, such as mean, strength-of-trend, etc. for both the whole time series, the training set and the validation set. It can and should be used to ensure that the characteristics of both the training and validation sets are, statistically speaking, similar to those of the time series one wishes to forecast. If this is not the case, using the validation method will most likely lead to a poor assessment of the model's performance.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Relevant features for the entire time series.</p> <code>DataFrame</code> <p>Relevant features for the training set.</p> <code>DataFrame</code> <p>Relevant features for the validation set.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the time series is composed of less than three samples.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from timecave.validation_methods.OOS import RollingOriginUpdate\n&gt;&gt;&gt; ts = np.hstack((np.ones(5), np.zeros(5)));\n&gt;&gt;&gt; splitter = RollingOriginUpdate(ts);\n&gt;&gt;&gt; ts_stats, training_stats, validation_stats = splitter.statistics();\nFrequency features are only meaningful if the correct sampling frequency is passed to the class.\nTraining and validation set features can only computed if each set is composed of two or more samples.\n&gt;&gt;&gt; ts_stats\n   Mean  Median  Min  Max  Variance  P2P_amplitude  Trend_slope  Spectral_centroid  Spectral_rolloff  Spectral_entropy  Strength_of_trend  Mean_crossing_rate  Median_crossing_rate\n0   0.5     0.5  0.0  1.0      0.25            1.0    -0.151515           0.114058               0.5           0.38717            1.59099            0.111111              0.111111\n&gt;&gt;&gt; training_stats\n       Mean  Median  Min  Max  Variance  P2P_amplitude  Trend_slope  Spectral_centroid  Spectral_rolloff  Spectral_entropy  Strength_of_trend  Mean_crossing_rate  Median_crossing_rate\n0  0.714286     1.0  0.0  1.0  0.204082            1.0    -0.178571           0.094706          0.428571          0.556506           1.212183            0.166667              0.166667\n&gt;&gt;&gt; validation_stats\n   Mean  Median  Min  Max  Variance  P2P_amplitude  Trend_slope  Spectral_centroid  Spectral_rolloff  Spectral_entropy  Strength_of_trend  Mean_crossing_rate  Median_crossing_rate\n0   0.0     0.0  0.0  0.0       0.0            0.0          0.0                  0               0.0               0.0                inf                 0.0                   0.0\n0   0.0     0.0  0.0  0.0       0.0            0.0          0.0                  0               0.0               0.0                inf                 0.0                   0.0\n</code></pre> Source code in <code>timecave/validation_methods/OOS.py</code> <pre><code>def statistics(self) -&gt; tuple[pd.DataFrame]:\n    \"\"\"\n    Compute relevant statistics for both training and validation sets.\n\n    This method computes relevant time series features, such as mean, strength-of-trend, etc. for both the whole time series, the training set and the validation set.\n    It can and should be used to ensure that the characteristics of both the training and validation sets are, statistically speaking, similar to those of the time series one wishes to forecast.\n    If this is not the case, using the validation method will most likely lead to a poor assessment of the model's performance.\n\n    Returns\n    -------\n    pd.DataFrame\n        Relevant features for the entire time series.\n\n    pd.DataFrame\n        Relevant features for the training set.\n\n    pd.DataFrame\n        Relevant features for the validation set.\n\n    Raises\n    ------\n    ValueError\n        If the time series is composed of less than three samples.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; from timecave.validation_methods.OOS import RollingOriginUpdate\n    &gt;&gt;&gt; ts = np.hstack((np.ones(5), np.zeros(5)));\n    &gt;&gt;&gt; splitter = RollingOriginUpdate(ts);\n    &gt;&gt;&gt; ts_stats, training_stats, validation_stats = splitter.statistics();\n    Frequency features are only meaningful if the correct sampling frequency is passed to the class.\n    Training and validation set features can only computed if each set is composed of two or more samples.\n    &gt;&gt;&gt; ts_stats\n       Mean  Median  Min  Max  Variance  P2P_amplitude  Trend_slope  Spectral_centroid  Spectral_rolloff  Spectral_entropy  Strength_of_trend  Mean_crossing_rate  Median_crossing_rate\n    0   0.5     0.5  0.0  1.0      0.25            1.0    -0.151515           0.114058               0.5           0.38717            1.59099            0.111111              0.111111\n    &gt;&gt;&gt; training_stats\n           Mean  Median  Min  Max  Variance  P2P_amplitude  Trend_slope  Spectral_centroid  Spectral_rolloff  Spectral_entropy  Strength_of_trend  Mean_crossing_rate  Median_crossing_rate\n    0  0.714286     1.0  0.0  1.0  0.204082            1.0    -0.178571           0.094706          0.428571          0.556506           1.212183            0.166667              0.166667\n    &gt;&gt;&gt; validation_stats\n       Mean  Median  Min  Max  Variance  P2P_amplitude  Trend_slope  Spectral_centroid  Spectral_rolloff  Spectral_entropy  Strength_of_trend  Mean_crossing_rate  Median_crossing_rate\n    0   0.0     0.0  0.0  0.0       0.0            0.0          0.0                  0               0.0               0.0                inf                 0.0                   0.0\n    0   0.0     0.0  0.0  0.0       0.0            0.0          0.0                  0               0.0               0.0                inf                 0.0                   0.0\n    \"\"\"\n\n    if self._n_samples &lt;= 2:\n\n        raise ValueError(\n            \"Basic statistics can only be computed if the time series comprises more than two samples.\"\n        )\n\n    print(\"Frequency features are only meaningful if the correct sampling frequency is passed to the class.\")\n\n    full_features = get_features(self._series, self.sampling_freq)\n    it_1 = True\n    validation_stats = []\n\n    print(\n        \"Training and validation set features can only computed if each set is composed of two or more samples.\"\n    )\n\n    for training, validation, _ in self.split():\n\n        if it_1 is True and self._series[training].shape[0] &gt;= 2:\n\n            training_features = get_features(\n                self._series[training], self.sampling_freq\n            )\n            it_1 = False\n\n        if self._series[validation].shape[0] &gt;= 2:\n\n            validation_feat = get_features(\n                self._series[validation], self.sampling_freq\n            )\n            validation_stats.append(validation_feat)\n\n    validation_features = pd.concat(validation_stats)\n\n    return (full_features, training_features, validation_features)\n</code></pre>"},{"location":"API_ref/validation_methods/base/base/","title":"Base class","text":""},{"location":"API_ref/validation_methods/base/base/#timecave.validation_methods.base","title":"<code>timecave.validation_methods.base</code>","text":"<p>This module contains the base class for all time series validation methods supported by this package. This class is simply an abstract class and should not be used directly.</p> <p>Classes:</p> Name Description <code>BaseSplitter</code> <p>Abstract base class for every validation method supported by this package.</p>"},{"location":"API_ref/validation_methods/base/base/#timecave.validation_methods.base.BaseSplitter","title":"<code>BaseSplitter(splits, ts, fs=1)</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Base class for all time series validation methods supported by this package.</p> <p>This is simply an abstract class. As such, it should not be used directly.</p> <p>Parameters:</p> Name Type Description Default <code>splits</code> <code>int</code> <p>The number of splits.</p> required <code>ts</code> <code>ndarray | Series</code> <p>Univariate time series.</p> required <code>fs</code> <code>float | int</code> <p>The series' sampling frequency (Hz).</p> <code>1</code> <p>Attributes:</p> Name Type Description <code>n_splits</code> <code>int</code> <p>The number of splits.</p> <code>sampling_freq</code> <code>int | float</code> <p>The series' sampling frequency (Hz).</p> <p>Methods:</p> Name Description <code>split</code> <p>Abstract method. The implementation differs for each validation method.</p> <code>info</code> <p>Abstract method. The implementation differs for each validation method.</p> <code>statistics</code> <p>Abstract method. The implementation differs for each validation method.</p> <code>plot</code> <p>Abstract method. The implementation differs for each validation method.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If the number of splits is not an integer.</p> <code>ValueError</code> <p>If the number of splits is smaller than two.</p> <code>TypeError</code> <p>If ts is not a Numpy array nor a Pandas series.</p> <code>TypeError</code> <p>If the sampling frequency is neither a float nor an integer.</p> <code>ValueError</code> <p>If the sampling frequency is negative.</p> <code>ValueError</code> <p>If the time series is multivariate (i.e. not one-dimensional).</p> <code>ValueError</code> <p>If the number of splits is larger than the number of samples in the time series.</p> Source code in <code>timecave/validation_methods/base.py</code> <pre><code>def __init__(\n    self, splits: int, ts: np.ndarray | pd.Series, fs: float | int = 1\n) -&gt; None:\n\n    \"\"\"\n    Class constructor.\n    \"\"\"\n\n    super().__init__()\n    self._splits_check(splits)\n    self._ts_check(ts)\n    self._fs_check(fs)\n    self._n_splits = splits\n    self._series = ts\n    self._fs = fs\n    self._n_samples = self._series.shape[0]\n    self._dim_check()\n    self._indices = np.arange(0, self._n_samples)\n\n    return\n</code></pre>"},{"location":"API_ref/validation_methods/base/base/#timecave.validation_methods.base.BaseSplitter.n_splits","title":"<code>n_splits: int</code>  <code>property</code>","text":"<p>Get the number of splits for a given instance of a validation method.</p> <p>This method can be used to retrieve the number of splits for a given instance of a validation method (this is set on initialisation). Since the method is implemented as a property, this information can simply be accessed as an attribute using dot notation.</p> <p>Returns:</p> Type Description <code>int</code> <p>The number of splits.</p>"},{"location":"API_ref/validation_methods/base/base/#timecave.validation_methods.base.BaseSplitter.sampling_freq","title":"<code>sampling_freq: int | float</code>  <code>property</code>","text":"<p>Get the time series' sampling frequency.</p> <p>This method can be used to access the time series' sampling frequency, in Hertz (this is set on intialisation). Since the method is implemented as a property, this information can simply be accessed as an attribute using dot notation.</p> <p>Returns:</p> Type Description <code>int | float</code> <p>The time series' sampling frequency (Hz).</p>"},{"location":"API_ref/validation_methods/base/base/#timecave.validation_methods.base.BaseSplitter.info","title":"<code>info()</code>  <code>abstractmethod</code>","text":"<p>Provide additional information on the validation method.</p> <p>Abstract method. The implementation differs for each validation method.</p> Source code in <code>timecave/validation_methods/base.py</code> <pre><code>@abstractmethod\ndef info(self) -&gt; None:\n    \"\"\"\n    Provide additional information on the validation method.\n\n    Abstract method. The implementation differs for each validation method.\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"API_ref/validation_methods/base/base/#timecave.validation_methods.base.BaseSplitter.plot","title":"<code>plot(height, width)</code>  <code>abstractmethod</code>","text":"<p>Plot the partitioned time series.</p> <p>Abstract method. The implementation differs for each validation method.</p> <p>Parameters:</p> Name Type Description Default <code>height</code> <code>int</code> <p>Figure height.</p> required <code>width</code> <code>int</code> <p>Figure width.</p> required Source code in <code>timecave/validation_methods/base.py</code> <pre><code>@abstractmethod\ndef plot(self, height: int, width: int) -&gt; None:\n    \"\"\"\n    Plot the partitioned time series.\n\n    Abstract method. The implementation differs for each validation method.\n\n    Parameters\n    ----------\n    height : int\n        Figure height.\n\n    width : int\n        Figure width.\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"API_ref/validation_methods/base/base/#timecave.validation_methods.base.BaseSplitter.split","title":"<code>split()</code>  <code>abstractmethod</code>","text":"<p>Split the time series into training and validation sets.</p> <p>Abstract method. The implementation differs for each validation method.</p> <p>Yields:</p> Type Description <code>ndarray</code> <p>Array of training indices.</p> <code>ndarray</code> <p>Array of validation indices.</p> <code>float</code> <p>Weight assigned to the error estimate.</p> Source code in <code>timecave/validation_methods/base.py</code> <pre><code>@abstractmethod\ndef split(self) -&gt; Generator[tuple[np.ndarray, np.ndarray, float], None, None]:\n    \"\"\"\n    Split the time series into training and validation sets.\n\n    Abstract method. The implementation differs for each validation method.\n\n    Yields\n    ------\n    np.ndarray\n        Array of training indices.\n\n    np.ndarray\n        Array of validation indices.\n\n    float\n        Weight assigned to the error estimate.\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"API_ref/validation_methods/base/base/#timecave.validation_methods.base.BaseSplitter.statistics","title":"<code>statistics()</code>  <code>abstractmethod</code>","text":"<p>Compute relevant statistics for both training and validation sets.</p> <p>Abstract method. The implementation differs for each validation method.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Relevant statistics and features for training sets.</p> <code>DataFrame</code> <p>Relevant statistics and features for validation sets.</p> Source code in <code>timecave/validation_methods/base.py</code> <pre><code>@abstractmethod\ndef statistics(self) -&gt; tuple[pd.DataFrame]:\n    \"\"\"\n    Compute relevant statistics for both training and validation sets.\n\n    Abstract method. The implementation differs for each validation method.\n\n    Returns\n    -------\n    pd.DataFrame\n        Relevant statistics and features for training sets.\n\n    pd.DataFrame\n        Relevant statistics and features for validation sets.\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"API_ref/validation_methods/markov/","title":"Markov Cross-validation method","text":""},{"location":"API_ref/validation_methods/markov/#timecave.validation_methods.markov","title":"<code>timecave.validation_methods.markov</code>","text":"<p>This module contains the Markov cross-validation method.</p> <p>Classes:</p> Name Description <code>MarkovCV</code> <p>Implements the Markov cross-validation method.</p>"},{"location":"API_ref/validation_methods/markov/#timecave.validation_methods.markov.MarkovCV","title":"<code>MarkovCV(ts, p, seed=1)</code>","text":"<p>               Bases: <code>BaseSplitter</code></p> <p>Implements the Markov cross-validation method.</p> <p>This class implements the Markov cross-validation method.</p> <p>Parameters:</p> Name Type Description Default <code>ts</code> <code>ndarray | Series</code> <p>Univariate time series.</p> required <code>p</code> <code>int</code> <p>p-order autocorrelation.</p> required <code>seed</code> <code>int</code> <p>Random seed.</p> <code>1</code> <p>Attributes:</p> Name Type Description <code>n_splits</code> <code>int</code> <p>The number of splits.</p> <code>sampling_freq</code> <code>int | float</code> <p>The series' sampling frequency (Hz).</p> <p>Methods:</p> Name Description <code>split</code> <p>Split the time series into training and validation sets.</p> <code>info</code> <p>Provide additional information on the validation method.</p> <code>statistics</code> <p>Compute relevant statistics for both training and validation sets.</p> <code>plot</code> <p>Plot the partitioned time series.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If <code>seed</code> is not an integer.</p> <code>TypeError</code> <p>If <code>p</code> is not an integer.</p> <code>ValueError</code> <p>If <code>p</code> is not positive.</p> Notes <p>The Markov cross-validation method partitions the data so that every partition can be regarded as     a Markov Process. It uses the linear autocorrelation measure to ensure that the samples in both the     training set and the validation set are neither too close nor too far apart.</p> <p></p> <p>For a thorough discussion of the method, see [1].</p> References Source code in <code>timecave/validation_methods/markov.py</code> <pre><code>def __init__(self, ts: np.ndarray | pd.Series, p: int, seed: int = 1) -&gt; None:\n    self._check_seed(seed)\n    self._check_p(p)\n\n    if p % 3 == 0:\n        self._m = math.floor(2 * p / 3) + 1\n    else:\n        self._m = math.floor(2 * p / 3) + 2\n\n    self.n_subsets = (\n        2 * self._m\n    )  # total number of subsets (training + tests subsets)\n    splits = 2 * self._m  # due to 2-fold CV\n    super().__init__(splits, ts, 1)\n    self._p = p\n    self._seed = seed\n    self._suo = {}\n    self._sue = {}\n</code></pre>"},{"location":"API_ref/validation_methods/markov/#timecave.validation_methods.markov.MarkovCV--1","title":"1","text":"<p>Gaoxia Jiang and Wenjian Wang. Markov cross-validation for time series model evaluations. Information Sciences, 375:219\u2013233, 2017</p>"},{"location":"API_ref/validation_methods/markov/#timecave.validation_methods.markov.MarkovCV.sampling_freq","title":"<code>sampling_freq: int | float</code>  <code>property</code>","text":"<p>Get the time series' sampling frequency.</p> <p>This method can be used to access the time series' sampling frequency, in Hertz (this is set on intialisation). Since the method is implemented as a property, this information can simply be accessed as an attribute using dot notation.</p> <p>Returns:</p> Type Description <code>int | float</code> <p>The time series' sampling frequency (Hz).</p>"},{"location":"API_ref/validation_methods/markov/#timecave.validation_methods.markov.MarkovCV.info","title":"<code>info()</code>","text":"<p>Provide some basic information on the training and validation sets.</p> <p>This method displays the number of splits and the number of observations per set.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from timecave.validation_methods.markov import MarkovCV\n&gt;&gt;&gt; ts = np.ones(10);\n&gt;&gt;&gt; splitter = MarkovCV(ts, p=2);\n&gt;&gt;&gt; splitter.info();\nMarkov CV method\n---------------\nTime series size: 10 samples\nNumber of splits: 6\nNumber of observations per set: 1 to 3\n</code></pre> Source code in <code>timecave/validation_methods/markov.py</code> <pre><code>def info(self) -&gt; None:\n    \"\"\"\n    Provide some basic information on the training and validation sets.\n\n    This method displays the number of splits and the number of observations per set.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; from timecave.validation_methods.markov import MarkovCV\n    &gt;&gt;&gt; ts = np.ones(10);\n    &gt;&gt;&gt; splitter = MarkovCV(ts, p=2);\n    &gt;&gt;&gt; splitter.info();\n    Markov CV method\n    ---------------\n    Time series size: 10 samples\n    Number of splits: 6\n    Number of observations per set: 1 to 3\n    \"\"\"\n\n    self._markov_partitions()\n\n    lengths = []\n    for i in range(1, len(self._suo.items()) + 1):\n        lengths.extend([len(self._suo[i]), len(self._sue[i])])\n\n    print(\"Markov CV method\")\n    print(\"---------------\")\n    print(f\"Time series size: {self._n_samples} samples\")\n    print(f\"Number of splits: {self.n_splits}\")\n    print(f\"Number of observations per set: {min(lengths)} to {max(lengths)}\")\n    pass\n</code></pre>"},{"location":"API_ref/validation_methods/markov/#timecave.validation_methods.markov.MarkovCV.plot","title":"<code>plot(height, width)</code>","text":"<p>Plot the partitioned time series.</p> <p>This method allows the user to plot the partitioned time series. The training and validation sets are plotted using different colours.</p> <p>Parameters:</p> Name Type Description Default <code>height</code> <code>int</code> <p>The figure's height.</p> required <code>width</code> <code>int</code> <p>The figure's width.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from timecave.validation_methods.markov import MarkovCV\n&gt;&gt;&gt; ts = np.ones(100);\n&gt;&gt;&gt; splitter = MarkovCV(ts, p=1);\n&gt;&gt;&gt; splitter.plot(10, 10);\n</code></pre> <p></p> Source code in <code>timecave/validation_methods/markov.py</code> <pre><code>def plot(self, height: int, width: int) -&gt; None:\n    \"\"\"\n    Plot the partitioned time series.\n\n    This method allows the user to plot the partitioned time series. The training and validation sets are plotted using different colours.\n\n    Parameters\n    ----------\n    height : int\n        The figure's height.\n\n    width : int\n        The figure's width.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; from timecave.validation_methods.markov import MarkovCV\n    &gt;&gt;&gt; ts = np.ones(100);\n    &gt;&gt;&gt; splitter = MarkovCV(ts, p=1);\n    &gt;&gt;&gt; splitter.plot(10, 10);\n\n    ![markov_plot](../../../images/Markov_plot.png)\n    \"\"\"\n\n    fig, axs = plt.subplots(self.n_splits, 1, sharex=True)\n    fig.set_figheight(height)\n    fig.set_figwidth(width)\n    fig.supxlabel(\"Samples\")\n    fig.supylabel(\"Time Series\")\n    fig.suptitle(\"Markov CV method\")\n\n    for it, (training, validation, _) in enumerate(self.split()):\n\n        axs[it].scatter(training, self._series[training], label=\"Training set\")\n        axs[it].scatter(\n            validation, self._series[validation], label=\"Validation set\"\n        )\n        axs[it].set_title(\"Iteration {}\".format(it + 1))\n        axs[it].legend()\n\n    plt.subplots_adjust(hspace=0.5)\n    plt.show()\n\n    return\n</code></pre>"},{"location":"API_ref/validation_methods/markov/#timecave.validation_methods.markov.MarkovCV.split","title":"<code>split()</code>","text":"<p>Split the time series into training and validation sets.</p> <p>This method splits the series' indices into disjoint sets containing the training and validation indices. At every iteration, an array of training indices and another one containing the validation indices are generated. Note that this method is a generator. To access the indices, use the <code>next()</code> method or a <code>for</code> loop.</p> <p>Yields:</p> Type Description <code>ndarray</code> <p>Array of training indices.</p> <code>ndarray</code> <p>Array of validation indices.</p> <code>float</code> <p>Used for compatibility reasons. Irrelevant for this method.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from timecave.validation_methods.markov import MarkovCV\n&gt;&gt;&gt; ts = np.ones(10);\n&gt;&gt;&gt; splitter = MarkovCV(ts, p=2);\n&gt;&gt;&gt; for ind, (train, val, _) in enumerate(splitter.split()):\n... \n...     print(f\"Iteration {ind+1}\");\n...     print(f\"Training set indices: {train}\");\n...     print(f\"Validation set indices: {val}\");\nIteration 1\nTraining set indices: [8]\nValidation set indices: [0 6]\nIteration 2\nTraining set indices: [0 6]\nValidation set indices: [8]\nIteration 3\nTraining set indices: [3]\nValidation set indices: [5]\nIteration 4\nTraining set indices: [5]\nValidation set indices: [3]\nIteration 5\nTraining set indices: [1 2 7]\nValidation set indices: [4 9]\nIteration 6\nTraining set indices: [4 9]\nValidation set indices: [1 2 7]\n</code></pre> Source code in <code>timecave/validation_methods/markov.py</code> <pre><code>def split(self) -&gt; Generator[tuple[np.ndarray, np.ndarray, float], None, None]:\n    \"\"\"\n    Split the time series into training and validation sets.\n\n    This method splits the series' indices into disjoint sets containing the training and validation indices.\n    At every iteration, an array of training indices and another one containing the validation indices are generated.\n    Note that this method is a generator. To access the indices, use the `next()` method or a `for` loop.\n\n    Yields\n    ------\n    np.ndarray\n        Array of training indices.\n\n    np.ndarray\n        Array of validation indices.\n\n    float\n        Used for compatibility reasons. Irrelevant for this method.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; from timecave.validation_methods.markov import MarkovCV\n    &gt;&gt;&gt; ts = np.ones(10);\n    &gt;&gt;&gt; splitter = MarkovCV(ts, p=2);\n    &gt;&gt;&gt; for ind, (train, val, _) in enumerate(splitter.split()):\n    ... \n    ...     print(f\"Iteration {ind+1}\");\n    ...     print(f\"Training set indices: {train}\");\n    ...     print(f\"Validation set indices: {val}\");\n    Iteration 1\n    Training set indices: [8]\n    Validation set indices: [0 6]\n    Iteration 2\n    Training set indices: [0 6]\n    Validation set indices: [8]\n    Iteration 3\n    Training set indices: [3]\n    Validation set indices: [5]\n    Iteration 4\n    Training set indices: [5]\n    Validation set indices: [3]\n    Iteration 5\n    Training set indices: [1 2 7]\n    Validation set indices: [4 9]\n    Iteration 6\n    Training set indices: [4 9]\n    Validation set indices: [1 2 7]\n    \"\"\"\n\n    self._markov_partitions()\n    for i in range(1, len(self._suo.items()) + 1):\n        train, validation = self._suo[i], self._sue[i]\n        yield (train, validation, 1.0)\n        train, validation = self._sue[i], self._suo[i]\n        yield (train, validation, 1.0)  # two-fold cross validation\n</code></pre>"},{"location":"API_ref/validation_methods/markov/#timecave.validation_methods.markov.MarkovCV.statistics","title":"<code>statistics()</code>","text":"<p>Compute relevant statistics for both training and validation sets.</p> <p>This method computes relevant time series features, such as mean, strength-of-trend, etc. for both the whole time series, the training set and the validation set. It can and should be used to ensure that the characteristics of both the training and validation sets are, statistically speaking, similar to those of the time series one wishes to forecast. If this is not the case, using the validation method will most likely lead to a poor assessment of the model's performance.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Relevant features for the entire time series.</p> <code>DataFrame</code> <p>Relevant features for the training set.</p> <code>DataFrame</code> <p>Relevant features for the validation set.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the time series is composed of less than three samples.</p> <code>ValueError</code> <p>If the folds comprise less than two samples.</p> <p>Examples:</p> <p>Frequency-domain features are not computed for the Markov CV method:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from timecave.validation_methods.markov import MarkovCV\n&gt;&gt;&gt; ts = np.hstack((np.ones(5), np.zeros(5)));\n&gt;&gt;&gt; splitter = MarkovCV(ts, p=1);\n&gt;&gt;&gt; ts_stats, training_stats, validation_stats = splitter.statistics();\nFrequency features are only meaningful if the correct sampling frequency is passed to the class.\n&gt;&gt;&gt; ts_stats\n   Mean  Median  Min  Max  Variance  P2P_amplitude  Trend_slope  Strength_of_trend  Mean_crossing_rate  Median_crossing_rate\n0   0.5     0.5  0.0  1.0      0.25            1.0    -0.151515            1.59099            0.111111              0.111111\n&gt;&gt;&gt; training_stats\n   Mean  Median  Min  Max  Variance  P2P_amplitude  Trend_slope  Strength_of_trend  Mean_crossing_rate  Median_crossing_rate\n0   0.5     0.5  0.0  1.0      0.25            1.0         -1.0                inf            1.000000              1.000000\n0   0.5     0.5  0.0  1.0      0.25            1.0         -1.0                inf            1.000000              1.000000\n0   0.5     0.5  0.0  1.0      0.25            1.0         -1.0                inf            1.000000              1.000000\n0   0.5     0.5  0.0  1.0      0.25            1.0         -0.4            1.06066            0.333333              0.333333\n&gt;&gt;&gt; validation_stats\n   Mean  Median  Min  Max  Variance  P2P_amplitude  Trend_slope  Strength_of_trend  Mean_crossing_rate  Median_crossing_rate\n0   0.5     0.5  0.0  1.0      0.25            1.0         -1.0                inf            1.000000              1.000000\n0   0.5     0.5  0.0  1.0      0.25            1.0         -1.0                inf            1.000000              1.000000\n0   0.5     0.5  0.0  1.0      0.25            1.0         -0.4            1.06066            0.333333              0.333333\n0   0.5     0.5  0.0  1.0      0.25            1.0         -1.0                inf            1.000000              1.000000\n</code></pre> Source code in <code>timecave/validation_methods/markov.py</code> <pre><code>def statistics(self) -&gt; tuple[pd.DataFrame]:\n    \"\"\"\n    Compute relevant statistics for both training and validation sets.\n\n    This method computes relevant time series features, such as mean, strength-of-trend, etc. for both the whole time series, the training set and the validation set.\n    It can and should be used to ensure that the characteristics of both the training and validation sets are, statistically speaking, similar to those of the time series one wishes to forecast.\n    If this is not the case, using the validation method will most likely lead to a poor assessment of the model's performance.\n\n    Returns\n    -------\n    pd.DataFrame\n        Relevant features for the entire time series.\n\n    pd.DataFrame\n        Relevant features for the training set.\n\n    pd.DataFrame\n        Relevant features for the validation set.\n\n    Raises\n    ------\n    ValueError\n        If the time series is composed of less than three samples.\n\n    ValueError\n        If the folds comprise less than two samples.\n\n    Examples\n    --------\n\n    Frequency-domain features are not computed for the Markov CV method:\n\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; from timecave.validation_methods.markov import MarkovCV\n    &gt;&gt;&gt; ts = np.hstack((np.ones(5), np.zeros(5)));\n    &gt;&gt;&gt; splitter = MarkovCV(ts, p=1);\n    &gt;&gt;&gt; ts_stats, training_stats, validation_stats = splitter.statistics();\n    Frequency features are only meaningful if the correct sampling frequency is passed to the class.\n    &gt;&gt;&gt; ts_stats\n       Mean  Median  Min  Max  Variance  P2P_amplitude  Trend_slope  Strength_of_trend  Mean_crossing_rate  Median_crossing_rate\n    0   0.5     0.5  0.0  1.0      0.25            1.0    -0.151515            1.59099            0.111111              0.111111\n    &gt;&gt;&gt; training_stats\n       Mean  Median  Min  Max  Variance  P2P_amplitude  Trend_slope  Strength_of_trend  Mean_crossing_rate  Median_crossing_rate\n    0   0.5     0.5  0.0  1.0      0.25            1.0         -1.0                inf            1.000000              1.000000\n    0   0.5     0.5  0.0  1.0      0.25            1.0         -1.0                inf            1.000000              1.000000\n    0   0.5     0.5  0.0  1.0      0.25            1.0         -1.0                inf            1.000000              1.000000\n    0   0.5     0.5  0.0  1.0      0.25            1.0         -0.4            1.06066            0.333333              0.333333\n    &gt;&gt;&gt; validation_stats\n       Mean  Median  Min  Max  Variance  P2P_amplitude  Trend_slope  Strength_of_trend  Mean_crossing_rate  Median_crossing_rate\n    0   0.5     0.5  0.0  1.0      0.25            1.0         -1.0                inf            1.000000              1.000000\n    0   0.5     0.5  0.0  1.0      0.25            1.0         -1.0                inf            1.000000              1.000000\n    0   0.5     0.5  0.0  1.0      0.25            1.0         -0.4            1.06066            0.333333              0.333333\n    0   0.5     0.5  0.0  1.0      0.25            1.0         -1.0                inf            1.000000              1.000000\n    \"\"\"\n\n    columns = [\n        \"Mean\",\n        \"Median\",\n        \"Min\",\n        \"Max\",\n        \"Variance\",\n        \"P2P_amplitude\",\n        \"Trend_slope\",\n        \"Strength_of_trend\",\n        \"Mean_crossing_rate\",\n        \"Median_crossing_rate\",\n    ]\n\n    if self._n_samples &lt;= 2:\n\n        raise ValueError(\n            \"Basic statistics can only be computed if the time series comprises more than two samples.\"\n        )\n\n    print(\"Frequency features are only meaningful if the correct sampling frequency is passed to the class.\")\n\n    full_features = get_features(self._series, self._fs)[columns]\n    training_stats = []\n    validation_stats = []\n\n    for training, validation, _ in self.split():\n\n        training_feat = get_features(self._series[training], self._fs)\n        training_stats.append(training_feat[columns])\n\n        validation_feat = get_features(self._series[validation], self._fs)\n        validation_stats.append(validation_feat[columns])\n\n    training_features = pd.concat(training_stats)\n    validation_features = pd.concat(validation_stats)\n\n    return (full_features, training_features, validation_features)\n</code></pre>"},{"location":"API_ref/validation_methods/prequential/","title":"Prequential methods","text":""},{"location":"API_ref/validation_methods/prequential/#timecave.validation_methods.prequential","title":"<code>timecave.validation_methods.prequential</code>","text":"<p>This module contains all the Prequential ('Predictive Sequential') validation methods supported by this package. These methods are also known as Forward Validation methods.</p> <p>Classes:</p> Name Description <code>GrowingWindow</code> <p>Implements every variant of the Growing Window method.</p> <code>RollingWindow</code> <p>Implements every variant of the Rolling Window method.</p> See also <p>Out-of-Sample methods: Out-of-sample methods for time series data.</p> <p>Cross-validation methods: Cross-validation methods for time series data.</p> <p>Markov methods: Markov cross-validation method for time series data.</p> Notes <p>Predictive Sequential, or \"Prequential\", methods are one of the three main classes of validation methods for time series data (the others being out-of-sample methods and cross-validation methods). Unlike cross-validation methods, this class of methods preserves the temporal order of observations, although it differs from out-of-sample methods in that it partitions the series into equally sized folds. For more details on this class of methods, the reader should refer to [1].</p> References"},{"location":"API_ref/validation_methods/prequential/#timecave.validation_methods.prequential--1","title":"1","text":"<p>Vitor Cerqueira, Luis Torgo, and Igor Mozeti\u02c7c. Evaluating time series forecasting models: An empirical study on performance estimation methods. Machine Learning, 109(11):1997\u20132028, 2020.</p>"},{"location":"API_ref/validation_methods/prequential/grow/","title":"Growing Window method","text":""},{"location":"API_ref/validation_methods/prequential/grow/#timecave.validation_methods.prequential.GrowingWindow","title":"<code>timecave.validation_methods.prequential.GrowingWindow(splits, ts, fs=1, gap=0, weight_function=constant_weights, params=None)</code>","text":"<p>               Bases: <code>BaseSplitter</code></p> <p>Implements every variant of the Growing Window method.</p> <p>This class implements the Growing Window method. It also supports every variant of this method, including Gap Growing Window and  Weighted Growing Window. The <code>gap</code> parameter can be used to implement the former, while the <code>weight_function</code> argument allows the user  to implement the latter in a convenient way.</p> <p>Parameters:</p> Name Type Description Default <code>splits</code> <code>int</code> <p>The number of folds used to partition the data.</p> required <code>ts</code> <code>ndarray | Series</code> <p>Univariate time series.</p> required <code>fs</code> <code>float | int</code> <p>Sampling frequency (Hz).</p> <code>1</code> <code>gap</code> <code>int</code> <p>Number of folds separating the validation set from the training set.  If this value is set to zero, the validation set will be adjacent to the training set.</p> <code>0</code> <code>weight_function</code> <code>callable</code> <p>Fold weighting function. Check the weights module for more details.</p> <code>constant_weights</code> <code>params</code> <code>dict</code> <p>Parameters to be passed to the weighting functions.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>n_splits</code> <code>int</code> <p>The number of splits.</p> <code>sampling_freq</code> <code>int | float</code> <p>The series' sampling frequency (Hz).</p> <p>Methods:</p> Name Description <code>split</code> <p>Split the time series into training and validation sets.</p> <code>info</code> <p>Provide additional information on the validation method.</p> <code>statistics</code> <p>Compute relevant statistics for both training and validation sets.</p> <code>plot</code> <p>Plot the partitioned time series.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If <code>gap</code> is not an integer.</p> <code>ValueError</code> <p>If <code>gap</code> is a negative number.</p> <code>ValueError</code> <p>If <code>gap</code> surpasses the limit imposed by the number of folds.</p> See also <p>Rolling Window: Similar to Growing Window, but the amount of samples in the training set is kept constant.</p> Notes <p>The Growing Window method splits the data into \\(N\\) different folds. Then, in every iteration \\(i\\), the model is trained on data from the first \\(i\\) folds and validated on the \\(i+1^{th}\\) fold (assuming no gap is specified). The average error on the validation sets  is then taken as the estimate of the model's true error. This method preserves the temporal  order of observations, as the training set always precedes the validation set. If a gap is specified, the procedure runs for \\(N-1-N_{gap}\\)  iterations, where \\(N_{gap}\\) is the number of folds separating the training and validation sets.</p> <p></p> <p>Note that the amount of data used to train the model varies significantly from fold to fold. Therefore, it seems natural to assume that the models trained     on more data will better mimic the situation where the model is trained using all the available data, thus yielding a more accurate estimate of the model's true error.  To address this issue, one may use a weighted average to compute the final estimate of the error, with larger weights being assigned to the estimates obtained     using models trained on larger amounts of data. For more details on this method, the reader should refer to [1].</p> References Source code in <code>timecave/validation_methods/prequential.py</code> <pre><code>def __init__(\n    self,\n    splits: int,\n    ts: np.ndarray | pd.Series,\n    fs: float | int = 1,\n    gap: int = 0,\n    weight_function: callable = constant_weights,\n    params: dict = None,\n) -&gt; None:\n\n    super().__init__(splits, ts, fs)\n    self._check_gap(gap)\n    self._gap = gap\n    self._splitting_ind = self._split_ind()\n    self._weights = weight_function(self.n_splits, self._gap, 1, params)\n\n    return\n</code></pre>"},{"location":"API_ref/validation_methods/prequential/grow/#timecave.validation_methods.prequential.GrowingWindow--1","title":"1","text":"<p>Vitor Cerqueira, Luis Torgo, and Igor Mozeti\u02c7c. Evaluating time series forecasting models: An empirical study on performance estimation methods. Machine Learning, 109(11):1997\u20132028, 2020.</p>"},{"location":"API_ref/validation_methods/prequential/grow/#timecave.validation_methods.prequential.GrowingWindow.info","title":"<code>info()</code>","text":"<p>Provide some basic information on the training and validation sets.</p> <p>This method displays the number of splits, the fold size, the maximum and minimum training set sizes, the gap,  and the weights that will be used to compute the error estimate.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from timecave.validation_methods.prequential import GrowingWindow\n&gt;&gt;&gt; ts = np.ones(10);\n&gt;&gt;&gt; splitter = GrowingWindow(5, ts);\n&gt;&gt;&gt; splitter.info();\nGrowing Window method\n---------------------\nTime series size: 10 samples\nNumber of splits: 5\nFold size: 2 to 2 samples (20.0 to 20.0 %)\nMinimum training set size: 2 samples (20.0 %)\nMaximum training set size: 8 samples (80.0 %)\nGap: 0\nWeights: [1. 1. 1. 1.]\n</code></pre> Source code in <code>timecave/validation_methods/prequential.py</code> <pre><code>def info(self) -&gt; None:\n    \"\"\"\n    Provide some basic information on the training and validation sets.\n\n    This method displays the number of splits, the fold size, the maximum and minimum training set sizes, the gap, \n    and the weights that will be used to compute the error estimate.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; from timecave.validation_methods.prequential import GrowingWindow\n    &gt;&gt;&gt; ts = np.ones(10);\n    &gt;&gt;&gt; splitter = GrowingWindow(5, ts);\n    &gt;&gt;&gt; splitter.info();\n    Growing Window method\n    ---------------------\n    Time series size: 10 samples\n    Number of splits: 5\n    Fold size: 2 to 2 samples (20.0 to 20.0 %)\n    Minimum training set size: 2 samples (20.0 %)\n    Maximum training set size: 8 samples (80.0 %)\n    Gap: 0\n    Weights: [1. 1. 1. 1.]\n    \"\"\"\n\n    min_fold_size = int(np.floor(self._n_samples / self.n_splits))\n    max_fold_size = min_fold_size\n\n    remainder = self._n_samples % self.n_splits\n\n    if remainder != 0:\n\n        max_fold_size += 1\n\n    min_fold_size_pct = np.round(min_fold_size / self._n_samples * 100, 2)\n    max_fold_size_pct = np.round(max_fold_size / self._n_samples * 100, 2)\n\n    max_train = (\n        min_fold_size * (self.n_splits - remainder - 1) + max_fold_size * remainder\n    )\n    max_train_pct = np.round(max_train / self._n_samples * 100, 2)\n\n    print(\"Growing Window method\")\n    print(\"---------------------\")\n    print(f\"Time series size: {self._n_samples} samples\")\n    print(f\"Number of splits: {self.n_splits}\")\n    print(\n        f\"Fold size: {min_fold_size} to {max_fold_size} samples ({min_fold_size_pct} to {max_fold_size_pct} %)\"\n    )\n    print(\n        f\"Minimum training set size: {max_fold_size} samples ({max_fold_size_pct} %)\"\n    )\n    print(f\"Maximum training set size: {max_train} samples ({max_train_pct} %)\")\n    print(f\"Gap: {self._gap}\")\n    print(f\"Weights: {np.round(self._weights, 3)}\")\n\n    return\n</code></pre>"},{"location":"API_ref/validation_methods/prequential/grow/#timecave.validation_methods.prequential.GrowingWindow.plot","title":"<code>plot(height, width)</code>","text":"<p>Plot the partitioned time series.</p> <p>This method allows the user to plot the partitioned time series. The training and validation sets are plotted using different colours.</p> <p>Parameters:</p> Name Type Description Default <code>height</code> <code>int</code> <p>The figure's height.</p> required <code>width</code> <code>int</code> <p>The figure's width.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from timecave.validation_methods.prequential import GrowingWindow\n&gt;&gt;&gt; ts = np.ones(100);\n&gt;&gt;&gt; splitter = GrowingWindow(5, ts);\n&gt;&gt;&gt; splitter.plot(10, 10);\n</code></pre> <p></p> Source code in <code>timecave/validation_methods/prequential.py</code> <pre><code>def plot(self, height: int, width: int) -&gt; None:\n    \"\"\"\n    Plot the partitioned time series.\n\n    This method allows the user to plot the partitioned time series. The training and validation sets are plotted using different colours.\n\n    Parameters\n    ----------\n    height : int\n        The figure's height.\n\n    width : int\n        The figure's width.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; from timecave.validation_methods.prequential import GrowingWindow\n    &gt;&gt;&gt; ts = np.ones(100);\n    &gt;&gt;&gt; splitter = GrowingWindow(5, ts);\n    &gt;&gt;&gt; splitter.plot(10, 10);\n\n    ![grow_plot](../../../images/Grow_plot.png)\n    \"\"\"\n\n    fig, axs = plt.subplots(self.n_splits - self._gap - 1, 1, sharex=True)\n    fig.set_figheight(height)\n    fig.set_figwidth(width)\n    fig.supxlabel(\"Samples\")\n    fig.supylabel(\"Time Series\")\n    fig.suptitle(\"Growing Window method\")\n\n    if(self.n_splits - self._gap - 1 &gt; 1):\n\n        for it, (training, validation, weight) in enumerate(self.split()):\n\n            axs[it].scatter(training, self._series[training], label=\"Training set\")\n            axs[it].scatter(\n                validation, self._series[validation], label=\"Validation set\"\n            )\n            axs[it].set_title(\"Iteration: {} Weight: {}\".format(it + 1, np.round(weight, 3)))\n            axs[it].set_ylim([self._series.min() - 1, self._series.max() + 1])\n            axs[it].set_xlim([- 1, self._n_samples + 1])\n            axs[it].legend()\n\n    else:\n\n        for (training, validation, weight) in self.split():\n\n            axs.scatter(training, self._series[training], label=\"Training set\")\n            axs.scatter(\n                validation, self._series[validation], label=\"Validation set\"\n            )\n            axs.set_title(\"Iteration: {} Weight: {}\".format(1, np.round(weight, 3)))\n            axs.legend()\n\n    plt.show()\n\n    return\n</code></pre>"},{"location":"API_ref/validation_methods/prequential/grow/#timecave.validation_methods.prequential.GrowingWindow.split","title":"<code>split()</code>","text":"<p>Split the time series into training and validation sets.</p> <p>This method splits the series' indices into disjoint sets containing the training and validation indices. At every iteration, an array of training indices and another one containing the validation indices are generated. Note that this method is a generator. To access the indices, use the <code>next()</code> method or a <code>for</code> loop.</p> <p>Yields:</p> Type Description <code>ndarray</code> <p>Array of training indices.</p> <code>ndarray</code> <p>Array of validation indices.</p> <code>float</code> <p>Weight assigned to the error estimate.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from timecave.validation_methods.prequential import GrowingWindow\n&gt;&gt;&gt; ts = np.ones(10);\n&gt;&gt;&gt; splitter = GrowingWindow(5, ts); # Split the data into 5 different folds\n&gt;&gt;&gt; for ind, (train, val, _) in enumerate(splitter.split()):\n... \n...     print(f\"Iteration {ind+1}\");\n...     print(f\"Training set indices: {train}\");\n...     print(f\"Validation set indices: {val}\");\nIteration 1\nTraining set indices: [0 1]\nValidation set indices: [2 3]\nIteration 2\nTraining set indices: [0 1 2 3]\nValidation set indices: [4 5]\nIteration 3\nTraining set indices: [0 1 2 3 4 5]\nValidation set indices: [6 7]\nIteration 4\nTraining set indices: [0 1 2 3 4 5 6 7]\nValidation set indices: [8 9]\n</code></pre> <p>If the number of samples is not divisible by the number of folds, the first folds will contain more samples:</p> <pre><code>&gt;&gt;&gt; ts2 = np.ones(17);\n&gt;&gt;&gt; splitter = GrowingWindow(5, ts2);\n&gt;&gt;&gt; for ind, (train, val, _) in enumerate(splitter.split()):\n... \n...     print(f\"Iteration {ind+1}\");\n...     print(f\"Training set indices: {train}\");\n...     print(f\"Validation set indices: {val}\");\nIteration 1\nTraining set indices: [0 1 2 3]\nValidation set indices: [4 5 6 7]\nIteration 2\nTraining set indices: [0 1 2 3 4 5 6 7]\nValidation set indices: [ 8  9 10]\nIteration 3\nTraining set indices: [ 0  1  2  3  4  5  6  7  8  9 10]\nValidation set indices: [11 12 13]\nIteration 4\nTraining set indices: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13]\nValidation set indices: [14 15 16]\n</code></pre> <p>If a gap is specified (Gap Growing Window), the validation set will no longer be adjacent to the training set. Keep in mind that, the larger the gap between these two sets, the fewer iterations are run:</p> <pre><code>&gt;&gt;&gt; splitter = GrowingWindow(5, ts, gap=1);\n&gt;&gt;&gt; for ind, (train, val, _) in enumerate(splitter.split()):\n... \n...     print(f\"Iteration {ind+1}\");\n...     print(f\"Training set indices: {train}\");\n...     print(f\"Validation set indices: {val}\");\nIteration 1\nTraining set indices: [0 1]\nValidation set indices: [4 5]\nIteration 2\nTraining set indices: [0 1 2 3]\nValidation set indices: [6 7]\nIteration 3\nTraining set indices: [0 1 2 3 4 5]\nValidation set indices: [8 9]\n</code></pre> <p>Weights can be assigned to the error estimates (Weighted Growing Window method).  The parameters for the weighting functions must be passed to the class constructor:</p> <pre><code>&gt;&gt;&gt; from timecave.validation_methods.weights import exponential_weights\n&gt;&gt;&gt; splitter = GrowingWindow(5, ts, weight_function=exponential_weights, params={\"base\": 2});\n&gt;&gt;&gt; for ind, (train, val, weight) in enumerate(splitter.split()):\n... \n...     print(f\"Iteration {ind+1}\");\n...     print(f\"Training set indices: {train}\");\n...     print(f\"Validation set indices: {val}\");\n...     print(f\"Weight: {np.round(weight, 3)}\");\nIteration 1\nTraining set indices: [0 1]\nValidation set indices: [2 3]\nWeight: 0.067\nIteration 2\nTraining set indices: [0 1 2 3]\nValidation set indices: [4 5]\nWeight: 0.133\nIteration 3\nTraining set indices: [0 1 2 3 4 5]\nValidation set indices: [6 7]\nWeight: 0.267\nIteration 4\nTraining set indices: [0 1 2 3 4 5 6 7]\nValidation set indices: [8 9]\nWeight: 0.533\n</code></pre> Source code in <code>timecave/validation_methods/prequential.py</code> <pre><code>def split(self) -&gt; Generator[tuple[np.ndarray, np.ndarray, float], None, None]:\n    \"\"\"\n    Split the time series into training and validation sets.\n\n    This method splits the series' indices into disjoint sets containing the training and validation indices.\n    At every iteration, an array of training indices and another one containing the validation indices are generated.\n    Note that this method is a generator. To access the indices, use the `next()` method or a `for` loop.\n\n    Yields\n    ------\n    np.ndarray\n        Array of training indices.\n\n    np.ndarray\n        Array of validation indices.\n\n    float\n        Weight assigned to the error estimate.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; from timecave.validation_methods.prequential import GrowingWindow\n    &gt;&gt;&gt; ts = np.ones(10);\n    &gt;&gt;&gt; splitter = GrowingWindow(5, ts); # Split the data into 5 different folds\n    &gt;&gt;&gt; for ind, (train, val, _) in enumerate(splitter.split()):\n    ... \n    ...     print(f\"Iteration {ind+1}\");\n    ...     print(f\"Training set indices: {train}\");\n    ...     print(f\"Validation set indices: {val}\");\n    Iteration 1\n    Training set indices: [0 1]\n    Validation set indices: [2 3]\n    Iteration 2\n    Training set indices: [0 1 2 3]\n    Validation set indices: [4 5]\n    Iteration 3\n    Training set indices: [0 1 2 3 4 5]\n    Validation set indices: [6 7]\n    Iteration 4\n    Training set indices: [0 1 2 3 4 5 6 7]\n    Validation set indices: [8 9]\n\n    If the number of samples is not divisible by the number of folds, the first folds will contain more samples:\n\n    &gt;&gt;&gt; ts2 = np.ones(17);\n    &gt;&gt;&gt; splitter = GrowingWindow(5, ts2);\n    &gt;&gt;&gt; for ind, (train, val, _) in enumerate(splitter.split()):\n    ... \n    ...     print(f\"Iteration {ind+1}\");\n    ...     print(f\"Training set indices: {train}\");\n    ...     print(f\"Validation set indices: {val}\");\n    Iteration 1\n    Training set indices: [0 1 2 3]\n    Validation set indices: [4 5 6 7]\n    Iteration 2\n    Training set indices: [0 1 2 3 4 5 6 7]\n    Validation set indices: [ 8  9 10]\n    Iteration 3\n    Training set indices: [ 0  1  2  3  4  5  6  7  8  9 10]\n    Validation set indices: [11 12 13]\n    Iteration 4\n    Training set indices: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13]\n    Validation set indices: [14 15 16]\n\n    If a gap is specified (Gap Growing Window), the validation set will no longer be adjacent to the training set.\n    Keep in mind that, the larger the gap between these two sets, the fewer iterations are run:\n\n    &gt;&gt;&gt; splitter = GrowingWindow(5, ts, gap=1);\n    &gt;&gt;&gt; for ind, (train, val, _) in enumerate(splitter.split()):\n    ... \n    ...     print(f\"Iteration {ind+1}\");\n    ...     print(f\"Training set indices: {train}\");\n    ...     print(f\"Validation set indices: {val}\");\n    Iteration 1\n    Training set indices: [0 1]\n    Validation set indices: [4 5]\n    Iteration 2\n    Training set indices: [0 1 2 3]\n    Validation set indices: [6 7]\n    Iteration 3\n    Training set indices: [0 1 2 3 4 5]\n    Validation set indices: [8 9]\n\n    Weights can be assigned to the error estimates (Weighted Growing Window method). \n    The parameters for the weighting functions must be passed to the class constructor:\n\n    &gt;&gt;&gt; from timecave.validation_methods.weights import exponential_weights\n    &gt;&gt;&gt; splitter = GrowingWindow(5, ts, weight_function=exponential_weights, params={\"base\": 2});\n    &gt;&gt;&gt; for ind, (train, val, weight) in enumerate(splitter.split()):\n    ... \n    ...     print(f\"Iteration {ind+1}\");\n    ...     print(f\"Training set indices: {train}\");\n    ...     print(f\"Validation set indices: {val}\");\n    ...     print(f\"Weight: {np.round(weight, 3)}\");\n    Iteration 1\n    Training set indices: [0 1]\n    Validation set indices: [2 3]\n    Weight: 0.067\n    Iteration 2\n    Training set indices: [0 1 2 3]\n    Validation set indices: [4 5]\n    Weight: 0.133\n    Iteration 3\n    Training set indices: [0 1 2 3 4 5]\n    Validation set indices: [6 7]\n    Weight: 0.267\n    Iteration 4\n    Training set indices: [0 1 2 3 4 5 6 7]\n    Validation set indices: [8 9]\n    Weight: 0.533\n    \"\"\"\n\n    for i, (ind, weight) in enumerate(zip(self._splitting_ind[:-1], self._weights)):\n\n        gap_ind = self._splitting_ind[i + self._gap]\n        gap_end_ind = self._splitting_ind[i + self._gap + 1]\n\n        train = self._indices[:ind]\n        validation = self._indices[gap_ind:gap_end_ind]\n\n        yield (train, validation, weight)\n</code></pre>"},{"location":"API_ref/validation_methods/prequential/grow/#timecave.validation_methods.prequential.GrowingWindow.statistics","title":"<code>statistics()</code>","text":"<p>Compute relevant statistics for both training and validation sets.</p> <p>This method computes relevant time series features, such as mean, strength-of-trend, etc. for both the whole time series, the training set and the validation set. It can and should be used to ensure that the characteristics of both the training and validation sets are, statistically speaking, similar to those of the time series one wishes to forecast. If this is not the case, using the validation method will most likely lead to a poor assessment of the model's performance.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Relevant features for the entire time series.</p> <code>DataFrame</code> <p>Relevant features for the training set.</p> <code>DataFrame</code> <p>Relevant features for the validation set.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the time series is composed of less than three samples.</p> <code>ValueError</code> <p>If the folds comprise less than two samples.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from timecave.validation_methods.prequential import GrowingWindow\n&gt;&gt;&gt; ts = np.hstack((np.ones(5), np.zeros(5)));\n&gt;&gt;&gt; splitter = GrowingWindow(5, ts);\n&gt;&gt;&gt; ts_stats, training_stats, validation_stats = splitter.statistics();\nFrequency features are only meaningful if the correct sampling frequency is passed to the class.\n&gt;&gt;&gt; ts_stats\n   Mean  Median  Min  Max  Variance  P2P_amplitude  Trend_slope  Spectral_centroid  Spectral_rolloff  Spectral_entropy  Strength_of_trend  Mean_crossing_rate  Median_crossing_rate\n0   0.5     0.5  0.0  1.0      0.25            1.0    -0.151515           0.114058               0.5           0.38717            1.59099            0.111111              0.111111\n&gt;&gt;&gt; training_stats\n       Mean  Median  Min  Max  Variance  P2P_amplitude   Trend_slope  Spectral_centroid  Spectral_rolloff  Spectral_entropy  Strength_of_trend  Mean_crossing_rate  Median_crossing_rate\n0  1.000000     1.0  1.0  1.0  0.000000            0.0 -7.850462e-17           0.000000               0.0          0.000000                inf            0.000000              0.000000\n0  1.000000     1.0  1.0  1.0  0.000000            0.0 -8.214890e-17           0.000000               0.0          0.000000                inf            0.000000              0.000000\n0  0.833333     1.0  0.0  1.0  0.138889            1.0 -1.428571e-01           0.125000               0.5          0.792481           0.931695            0.200000              0.200000\n0  0.625000     1.0  0.0  1.0  0.234375            1.0 -1.785714e-01           0.122818               0.5          0.600876           1.383496            0.142857              0.142857\n&gt;&gt;&gt; validation_stats\n   Mean  Median  Min  Max  Variance  P2P_amplitude   Trend_slope  Spectral_centroid  Spectral_rolloff  Spectral_entropy  Strength_of_trend  Mean_crossing_rate  Median_crossing_rate\n0   1.0     1.0  1.0  1.0      0.00            0.0 -7.850462e-17               0.00               0.0               0.0                inf                 0.0                   0.0\n0   0.5     0.5  0.0  1.0      0.25            1.0 -1.000000e+00               0.25               0.5               0.0                inf                 1.0                   1.0\n0   0.0     0.0  0.0  0.0      0.00            0.0  0.000000e+00               0.00               0.0               0.0                inf                 0.0                   0.0\n0   0.0     0.0  0.0  0.0      0.00            0.0  0.000000e+00               0.00               0.0               0.0                inf                 0.0                   0.0\n</code></pre> Source code in <code>timecave/validation_methods/prequential.py</code> <pre><code>def statistics(self) -&gt; tuple[pd.DataFrame]:\n    \"\"\"\n    Compute relevant statistics for both training and validation sets.\n\n    This method computes relevant time series features, such as mean, strength-of-trend, etc. for both the whole time series, the training set and the validation set.\n    It can and should be used to ensure that the characteristics of both the training and validation sets are, statistically speaking, similar to those of the time series one wishes to forecast.\n    If this is not the case, using the validation method will most likely lead to a poor assessment of the model's performance.\n\n    Returns\n    -------\n    pd.DataFrame\n        Relevant features for the entire time series.\n\n    pd.DataFrame\n        Relevant features for the training set.\n\n    pd.DataFrame\n        Relevant features for the validation set.\n\n    Raises\n    ------\n    ValueError\n        If the time series is composed of less than three samples.\n\n    ValueError\n        If the folds comprise less than two samples.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; from timecave.validation_methods.prequential import GrowingWindow\n    &gt;&gt;&gt; ts = np.hstack((np.ones(5), np.zeros(5)));\n    &gt;&gt;&gt; splitter = GrowingWindow(5, ts);\n    &gt;&gt;&gt; ts_stats, training_stats, validation_stats = splitter.statistics();\n    Frequency features are only meaningful if the correct sampling frequency is passed to the class.\n    &gt;&gt;&gt; ts_stats\n       Mean  Median  Min  Max  Variance  P2P_amplitude  Trend_slope  Spectral_centroid  Spectral_rolloff  Spectral_entropy  Strength_of_trend  Mean_crossing_rate  Median_crossing_rate\n    0   0.5     0.5  0.0  1.0      0.25            1.0    -0.151515           0.114058               0.5           0.38717            1.59099            0.111111              0.111111\n    &gt;&gt;&gt; training_stats\n           Mean  Median  Min  Max  Variance  P2P_amplitude   Trend_slope  Spectral_centroid  Spectral_rolloff  Spectral_entropy  Strength_of_trend  Mean_crossing_rate  Median_crossing_rate\n    0  1.000000     1.0  1.0  1.0  0.000000            0.0 -7.850462e-17           0.000000               0.0          0.000000                inf            0.000000              0.000000\n    0  1.000000     1.0  1.0  1.0  0.000000            0.0 -8.214890e-17           0.000000               0.0          0.000000                inf            0.000000              0.000000\n    0  0.833333     1.0  0.0  1.0  0.138889            1.0 -1.428571e-01           0.125000               0.5          0.792481           0.931695            0.200000              0.200000\n    0  0.625000     1.0  0.0  1.0  0.234375            1.0 -1.785714e-01           0.122818               0.5          0.600876           1.383496            0.142857              0.142857\n    &gt;&gt;&gt; validation_stats\n       Mean  Median  Min  Max  Variance  P2P_amplitude   Trend_slope  Spectral_centroid  Spectral_rolloff  Spectral_entropy  Strength_of_trend  Mean_crossing_rate  Median_crossing_rate\n    0   1.0     1.0  1.0  1.0      0.00            0.0 -7.850462e-17               0.00               0.0               0.0                inf                 0.0                   0.0\n    0   0.5     0.5  0.0  1.0      0.25            1.0 -1.000000e+00               0.25               0.5               0.0                inf                 1.0                   1.0\n    0   0.0     0.0  0.0  0.0      0.00            0.0  0.000000e+00               0.00               0.0               0.0                inf                 0.0                   0.0\n    0   0.0     0.0  0.0  0.0      0.00            0.0  0.000000e+00               0.00               0.0               0.0                inf                 0.0                   0.0\n    \"\"\"\n\n    if self._n_samples &lt;= 2:\n\n        raise ValueError(\n            \"Basic statistics can only be computed if the time series comprises more than two samples.\"\n        )\n\n    if int(np.floor(self._n_samples / self.n_splits)) &lt; 2:\n\n        raise ValueError(\n            \"The folds are too small to compute most meaningful features.\"\n        )\n\n    print(\"Frequency features are only meaningful if the correct sampling frequency is passed to the class.\")\n\n    full_features = get_features(self._series, self.sampling_freq)\n    training_stats = []\n    validation_stats = []\n\n    for (training, validation, _) in self.split():\n\n        training_feat = get_features(self._series[training], self.sampling_freq)\n        training_stats.append(training_feat)\n\n        validation_feat = get_features(self._series[validation], self.sampling_freq)\n        validation_stats.append(validation_feat)\n\n    training_features = pd.concat(training_stats)\n    validation_features = pd.concat(validation_stats)\n\n    return (full_features, training_features, validation_features)\n</code></pre>"},{"location":"API_ref/validation_methods/prequential/roll/","title":"Rolling Window method","text":""},{"location":"API_ref/validation_methods/prequential/roll/#timecave.validation_methods.prequential.RollingWindow","title":"<code>timecave.validation_methods.prequential.RollingWindow(splits, ts, fs=1, gap=0, weight_function=constant_weights, params=None)</code>","text":"<p>               Bases: <code>BaseSplitter</code></p> <p>Implements every variant of the Rolling Window method.</p> <p>This class implements the Rolling Window method. It also supports every variant of this method, including Gap Rolling Window and  Weighted Rolling Window. The <code>gap</code> parameter can be used to implement the former, while the <code>weight_function</code> argument allows the user  to implement the latter in a convenient way.</p> <p>Parameters:</p> Name Type Description Default <code>splits</code> <code>int</code> <p>The number of folds used to partition the data.</p> required <code>ts</code> <code>ndarray | Series</code> <p>Univariate time series.</p> required <code>fs</code> <code>float | int</code> <p>Sampling frequency (Hz).</p> <code>1</code> <code>gap</code> <code>int</code> <p>Number of folds separating the validation set from the training set.  If this value is set to zero, the validation set will be adjacent to the training set.</p> <code>0</code> <code>weight_function</code> <code>callable</code> <p>Fold weighting function. Check the weights module for more details.</p> <code>constant_weights</code> <code>params</code> <code>dict</code> <p>Parameters to be passed to the weighting functions.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>n_splits</code> <code>int</code> <p>The number of splits.</p> <code>sampling_freq</code> <code>int | float</code> <p>The series' sampling frequency (Hz).</p> <p>Methods:</p> Name Description <code>split</code> <p>Split the time series into training and validation sets.</p> <code>info</code> <p>Provide additional information on the validation method.</p> <code>statistics</code> <p>Compute relevant statistics for both training and validation sets.</p> <code>plot</code> <p>Plot the partitioned time series.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If <code>gap</code> is not an integer.</p> <code>ValueError</code> <p>If <code>gap</code> is a negative number.</p> <code>ValueError</code> <p>If <code>gap</code> surpasses the limit imposed by the number of folds.</p> See also <p>Growing Window: Similar to Rolling Window, but the training set size gradually increases.</p> Notes <p>The Rolling Window method splits the data into \\(N\\) different folds. Then, in every iteration \\(i\\), the model is trained on data from the \\(i^{th}\\) fold and validated on the \\(i+1^{th}\\) fold (assuming no gap is specified). The average error on the validation sets  is then taken as the estimate of the model's true error. This method preserves the temporal  order of observations, as the training set always precedes the validation set. If a gap is specified, the procedure runs for \\(N-1-N_{gap}\\)  iterations, where \\(N_{gap}\\) is the number of folds separating the training and validation sets.</p> <p></p> <p>Note that, even though the size of the training set is kept constant throughout the validation procedure, the models from the last iterations are trained on more  recent data. It is therefore reasonable to assume that these models will have an advantage over the ones trained on older data, yielding a less biased estimate of the  model's true error.  To address this issue, one may use a weighted average to compute the final estimate of the error, with larger weights being assigned to the estimates obtained     using models trained on more recent data. For more details on this method, the reader should refer to [1].</p> References Source code in <code>timecave/validation_methods/prequential.py</code> <pre><code>def __init__(\n    self,\n    splits: int,\n    ts: np.ndarray | pd.Series,\n    fs: float | int = 1,\n    gap: int = 0,\n    weight_function: callable = constant_weights,\n    params: dict = None,\n) -&gt; None:\n\n    super().__init__(splits, ts, fs)\n    self._check_gap(gap)\n    self._gap = gap\n    self._splitting_ind = self._split_ind()\n    self._weights = weight_function(self.n_splits, self._gap, 1, params)\n\n    return\n</code></pre>"},{"location":"API_ref/validation_methods/prequential/roll/#timecave.validation_methods.prequential.RollingWindow--1","title":"1","text":"<p>Vitor Cerqueira, Luis Torgo, and Igor Mozeti\u02c7c. Evaluating time series forecasting models: An empirical study on performance estimation methods. Machine Learning, 109(11):1997\u20132028, 2020.</p>"},{"location":"API_ref/validation_methods/prequential/roll/#timecave.validation_methods.prequential.RollingWindow.info","title":"<code>info()</code>","text":"<p>Provide some basic information on the training and validation sets.</p> <p>This method displays the number of splits, the fold size, the gap,  and the weights that will be used to compute the error estimate.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from timecave.validation_methods.prequential import RollingWindow\n&gt;&gt;&gt; ts = np.ones(10);\n&gt;&gt;&gt; splitter = RollingWindow(5, ts);\n&gt;&gt;&gt; splitter.info();\nRolling Window method\n---------------------\nTime series size: 10 samples\nNumber of splits: 5\nFold size: 2 to 2 samples (20.0 to 20.0 %)\nGap: 0\nWeights: [1. 1. 1. 1.]\n</code></pre> Source code in <code>timecave/validation_methods/prequential.py</code> <pre><code>def info(self) -&gt; None:\n    \"\"\"\n    Provide some basic information on the training and validation sets.\n\n    This method displays the number of splits, the fold size, the gap, \n    and the weights that will be used to compute the error estimate.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; from timecave.validation_methods.prequential import RollingWindow\n    &gt;&gt;&gt; ts = np.ones(10);\n    &gt;&gt;&gt; splitter = RollingWindow(5, ts);\n    &gt;&gt;&gt; splitter.info();\n    Rolling Window method\n    ---------------------\n    Time series size: 10 samples\n    Number of splits: 5\n    Fold size: 2 to 2 samples (20.0 to 20.0 %)\n    Gap: 0\n    Weights: [1. 1. 1. 1.]\n    \"\"\"\n\n    min_fold_size = int(np.floor(self._n_samples / self.n_splits))\n    max_fold_size = min_fold_size\n\n    remainder = self._n_samples % self.n_splits\n\n    if remainder != 0:\n\n        max_fold_size += 1\n\n    min_fold_size_pct = np.round(min_fold_size / self._n_samples * 100, 2)\n    max_fold_size_pct = np.round(max_fold_size / self._n_samples * 100, 2)\n\n    print(\"Rolling Window method\")\n    print(\"---------------------\")\n    print(f\"Time series size: {self._n_samples} samples\")\n    print(f\"Number of splits: {self.n_splits}\")\n    print(\n        f\"Fold size: {min_fold_size} to {max_fold_size} samples ({min_fold_size_pct} to {max_fold_size_pct} %)\"\n    )\n    print(f\"Gap: {self._gap}\")\n    print(f\"Weights: {np.round(self._weights, 3)}\")\n\n    return\n</code></pre>"},{"location":"API_ref/validation_methods/prequential/roll/#timecave.validation_methods.prequential.RollingWindow.plot","title":"<code>plot(height, width)</code>","text":"<p>Plot the partitioned time series.</p> <p>This method allows the user to plot the partitioned time series. The training and validation sets are plotted using different colours.</p> <p>Parameters:</p> Name Type Description Default <code>height</code> <code>int</code> <p>The figure's height.</p> required <code>width</code> <code>int</code> <p>The figure's width.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from timecave.validation_methods.prequential import RollingWindow\n&gt;&gt;&gt; ts = np.ones(100);\n&gt;&gt;&gt; splitter = RollingWindow(5, ts);\n&gt;&gt;&gt; splitter.plot(10, 10);\n</code></pre> <p></p> Source code in <code>timecave/validation_methods/prequential.py</code> <pre><code>def plot(self, height: int, width: int) -&gt; None:\n    \"\"\"\n    Plot the partitioned time series.\n\n    This method allows the user to plot the partitioned time series. The training and validation sets are plotted using different colours.\n\n    Parameters\n    ----------\n    height : int\n        The figure's height.\n\n    width : int\n        The figure's width.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; from timecave.validation_methods.prequential import RollingWindow\n    &gt;&gt;&gt; ts = np.ones(100);\n    &gt;&gt;&gt; splitter = RollingWindow(5, ts);\n    &gt;&gt;&gt; splitter.plot(10, 10);\n\n    ![roll_plot](../../../images/Roll_plot.png)\n    \"\"\"\n\n    fig, axs = plt.subplots(self.n_splits - self._gap - 1, 1, sharex=True)\n    fig.set_figheight(height)\n    fig.set_figwidth(width)\n    fig.supxlabel(\"Samples\")\n    fig.supylabel(\"Time Series\")\n    fig.suptitle(\"Rolling Window method\")\n\n    if(self.n_splits - self._gap - 1 &gt; 1):\n\n        for it, (training, validation, weight) in enumerate(self.split()):\n\n            axs[it].scatter(training, self._series[training], label=\"Training set\")\n            axs[it].scatter(\n                validation, self._series[validation], label=\"Validation set\"\n            )\n            axs[it].set_title(\"Iteration: {} Weight: {}\".format(it + 1, np.round(weight, 3)))\n            axs[it].set_ylim([self._series.min() - 1, self._series.max() + 1])\n            axs[it].set_xlim([- 1, self._n_samples + 1])\n            axs[it].legend()\n\n    else:\n\n        for (training, validation, weight) in self.split():\n\n            axs.scatter(training, self._series[training], label=\"Training set\")\n            axs.scatter(\n                validation, self._series[validation], label=\"Validation set\"\n            )\n            axs.set_title(\"Iteration: {} Weight: {}\".format(1, np.round(weight, 3)))\n            axs.legend()\n\n    plt.show()\n\n    return\n</code></pre>"},{"location":"API_ref/validation_methods/prequential/roll/#timecave.validation_methods.prequential.RollingWindow.split","title":"<code>split()</code>","text":"<p>Split the time series into training and validation sets.</p> <p>This method splits the series' indices into disjoint sets containing the training and validation indices. At every iteration, an array of training indices and another one containing the validation indices are generated. Note that this method is a generator. To access the indices, use the <code>next()</code> method or a <code>for</code> loop.</p> <p>Yields:</p> Type Description <code>ndarray</code> <p>Array of training indices.</p> <code>ndarray</code> <p>Array of validation indices.</p> <code>float</code> <p>Weight assigned to the error estimate.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from timecave.validation_methods.prequential import RollingWindow\n&gt;&gt;&gt; ts = np.ones(10);\n&gt;&gt;&gt; splitter = RollingWindow(5, ts); # Split the data into 5 different folds\n&gt;&gt;&gt; for ind, (train, val, _) in enumerate(splitter.split()):\n... \n...     print(f\"Iteration {ind+1}\");\n...     print(f\"Training set indices: {train}\");\n...     print(f\"Validation set indices: {val}\");\nIteration 1\nTraining set indices: [0 1]\nValidation set indices: [2 3]\nIteration 2\nTraining set indices: [2 3]\nValidation set indices: [4 5]\nIteration 3\nTraining set indices: [4 5]\nValidation set indices: [6 7]\nIteration 4\nTraining set indices: [6 7]\nValidation set indices: [8 9]\n</code></pre> <p>If the number of samples is not divisible by the number of folds, the first folds will contain more samples:</p> <pre><code>&gt;&gt;&gt; ts2 = np.ones(17);\n&gt;&gt;&gt; splitter = RollingWindow(5, ts2);\n&gt;&gt;&gt; for ind, (train, val, _) in enumerate(splitter.split()):\n... \n...     print(f\"Iteration {ind+1}\");\n...     print(f\"Training set indices: {train}\");\n...     print(f\"Validation set indices: {val}\");\nIteration 1\nTraining set indices: [0 1 2 3]\nValidation set indices: [4 5 6 7]\nIteration 2\nTraining set indices: [4 5 6 7]\nValidation set indices: [ 8  9 10]\nIteration 3\nTraining set indices: [ 8  9 10]\nValidation set indices: [11 12 13]\nIteration 4\nTraining set indices: [11 12 13]\nValidation set indices: [14 15 16]\n</code></pre> <p>If a gap is specified (Gap Rolling Window), the validation set will no longer be adjacent to the training set. Keep in mind that, the larger the gap between these two sets, the fewer iterations are run:</p> <pre><code>&gt;&gt;&gt; splitter = RollingWindow(5, ts, gap=1);\n&gt;&gt;&gt; for ind, (train, val, _) in enumerate(splitter.split()):\n... \n...     print(f\"Iteration {ind+1}\");\n...     print(f\"Training set indices: {train}\");\n...     print(f\"Validation set indices: {val}\");\nIteration 1\nTraining set indices: [0 1]\nValidation set indices: [4 5]\nIteration 2\nTraining set indices: [2 3]\nValidation set indices: [6 7]\nIteration 3\nTraining set indices: [4 5]\nValidation set indices: [8 9]\n</code></pre> <p>Weights can be assigned to the error estimates (Weighted Rolling Window method).  The parameters for the weighting functions must be passed to the class constructor:</p> <pre><code>&gt;&gt;&gt; from timecave.validation_methods.weights import exponential_weights\n&gt;&gt;&gt; splitter = RollingWindow(5, ts, weight_function=exponential_weights, params={\"base\": 2});\n&gt;&gt;&gt; for ind, (train, val, weight) in enumerate(splitter.split()):\n... \n...     print(f\"Iteration {ind+1}\");\n...     print(f\"Training set indices: {train}\");\n...     print(f\"Validation set indices: {val}\");\n...     print(f\"Weight: {np.round(weight, 3)}\");\nIteration 1\nTraining set indices: [0 1]\nValidation set indices: [2 3]\nWeight: 0.067\nIteration 2\nTraining set indices: [2 3]\nValidation set indices: [4 5]\nWeight: 0.133\nIteration 3\nTraining set indices: [4 5]\nValidation set indices: [6 7]\nWeight: 0.267\nIteration 4\nTraining set indices: [6 7]\nValidation set indices: [8 9]\nWeight: 0.533\n</code></pre> Source code in <code>timecave/validation_methods/prequential.py</code> <pre><code>def split(self) -&gt; Generator[tuple[np.ndarray, np.ndarray, float], None, None]:\n    \"\"\"\n    Split the time series into training and validation sets.\n\n    This method splits the series' indices into disjoint sets containing the training and validation indices.\n    At every iteration, an array of training indices and another one containing the validation indices are generated.\n    Note that this method is a generator. To access the indices, use the `next()` method or a `for` loop.\n\n    Yields\n    ------\n    np.ndarray\n        Array of training indices.\n\n    np.ndarray\n        Array of validation indices.\n\n    float\n        Weight assigned to the error estimate.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; from timecave.validation_methods.prequential import RollingWindow\n    &gt;&gt;&gt; ts = np.ones(10);\n    &gt;&gt;&gt; splitter = RollingWindow(5, ts); # Split the data into 5 different folds\n    &gt;&gt;&gt; for ind, (train, val, _) in enumerate(splitter.split()):\n    ... \n    ...     print(f\"Iteration {ind+1}\");\n    ...     print(f\"Training set indices: {train}\");\n    ...     print(f\"Validation set indices: {val}\");\n    Iteration 1\n    Training set indices: [0 1]\n    Validation set indices: [2 3]\n    Iteration 2\n    Training set indices: [2 3]\n    Validation set indices: [4 5]\n    Iteration 3\n    Training set indices: [4 5]\n    Validation set indices: [6 7]\n    Iteration 4\n    Training set indices: [6 7]\n    Validation set indices: [8 9]\n\n    If the number of samples is not divisible by the number of folds, the first folds will contain more samples:\n\n    &gt;&gt;&gt; ts2 = np.ones(17);\n    &gt;&gt;&gt; splitter = RollingWindow(5, ts2);\n    &gt;&gt;&gt; for ind, (train, val, _) in enumerate(splitter.split()):\n    ... \n    ...     print(f\"Iteration {ind+1}\");\n    ...     print(f\"Training set indices: {train}\");\n    ...     print(f\"Validation set indices: {val}\");\n    Iteration 1\n    Training set indices: [0 1 2 3]\n    Validation set indices: [4 5 6 7]\n    Iteration 2\n    Training set indices: [4 5 6 7]\n    Validation set indices: [ 8  9 10]\n    Iteration 3\n    Training set indices: [ 8  9 10]\n    Validation set indices: [11 12 13]\n    Iteration 4\n    Training set indices: [11 12 13]\n    Validation set indices: [14 15 16]\n\n    If a gap is specified (Gap Rolling Window), the validation set will no longer be adjacent to the training set.\n    Keep in mind that, the larger the gap between these two sets, the fewer iterations are run:\n\n    &gt;&gt;&gt; splitter = RollingWindow(5, ts, gap=1);\n    &gt;&gt;&gt; for ind, (train, val, _) in enumerate(splitter.split()):\n    ... \n    ...     print(f\"Iteration {ind+1}\");\n    ...     print(f\"Training set indices: {train}\");\n    ...     print(f\"Validation set indices: {val}\");\n    Iteration 1\n    Training set indices: [0 1]\n    Validation set indices: [4 5]\n    Iteration 2\n    Training set indices: [2 3]\n    Validation set indices: [6 7]\n    Iteration 3\n    Training set indices: [4 5]\n    Validation set indices: [8 9]\n\n    Weights can be assigned to the error estimates (Weighted Rolling Window method). \n    The parameters for the weighting functions must be passed to the class constructor:\n\n    &gt;&gt;&gt; from timecave.validation_methods.weights import exponential_weights\n    &gt;&gt;&gt; splitter = RollingWindow(5, ts, weight_function=exponential_weights, params={\"base\": 2});\n    &gt;&gt;&gt; for ind, (train, val, weight) in enumerate(splitter.split()):\n    ... \n    ...     print(f\"Iteration {ind+1}\");\n    ...     print(f\"Training set indices: {train}\");\n    ...     print(f\"Validation set indices: {val}\");\n    ...     print(f\"Weight: {np.round(weight, 3)}\");\n    Iteration 1\n    Training set indices: [0 1]\n    Validation set indices: [2 3]\n    Weight: 0.067\n    Iteration 2\n    Training set indices: [2 3]\n    Validation set indices: [4 5]\n    Weight: 0.133\n    Iteration 3\n    Training set indices: [4 5]\n    Validation set indices: [6 7]\n    Weight: 0.267\n    Iteration 4\n    Training set indices: [6 7]\n    Validation set indices: [8 9]\n    Weight: 0.533\n    \"\"\"\n\n    #print(self._splitting_ind)\n\n    for i, (ind, weight) in enumerate(zip(self._splitting_ind[1:-1], self._weights)):\n\n        gap_ind = self._splitting_ind[i + 1 + self._gap]\n        gap_end_ind = self._splitting_ind[i + 1 + self._gap + 1]\n        start_training_ind = self._splitting_ind[i]\n\n        train = self._indices[start_training_ind:ind]\n        validation = self._indices[gap_ind:gap_end_ind]\n\n        #print(\"Bollocks\");\n        #print(train)\n        #print(validation)\n\n        yield (train, validation, weight)\n</code></pre>"},{"location":"API_ref/validation_methods/prequential/roll/#timecave.validation_methods.prequential.RollingWindow.statistics","title":"<code>statistics()</code>","text":"<p>Compute relevant statistics for both training and validation sets.</p> <p>This method computes relevant time series features, such as mean, strength-of-trend, etc. for both the whole time series, the training set and the validation set. It can and should be used to ensure that the characteristics of both the training and validation sets are, statistically speaking, similar to those of the time series one wishes to forecast. If this is not the case, using the validation method will most likely lead to a poor assessment of the model's performance.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Relevant features for the entire time series.</p> <code>DataFrame</code> <p>Relevant features for the training set.</p> <code>DataFrame</code> <p>Relevant features for the validation set.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the time series is composed of less than three samples.</p> <code>ValueError</code> <p>If the folds comprise less than two samples.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from timecave.validation_methods.prequential import RollingWindow\n&gt;&gt;&gt; ts = np.hstack((np.ones(5), np.zeros(5)));\n&gt;&gt;&gt; splitter = RollingWindow(5, ts);\n&gt;&gt;&gt; ts_stats, training_stats, validation_stats = splitter.statistics();\nFrequency features are only meaningful if the correct sampling frequency is passed to the class.\n&gt;&gt;&gt; ts_stats\n   Mean  Median  Min  Max  Variance  P2P_amplitude  Trend_slope  Spectral_centroid  Spectral_rolloff  Spectral_entropy  Strength_of_trend  Mean_crossing_rate  Median_crossing_rate\n0   0.5     0.5  0.0  1.0      0.25            1.0    -0.151515           0.114058               0.5           0.38717            1.59099            0.111111              0.111111\n&gt;&gt;&gt; training_stats\n   Mean  Median  Min  Max  Variance  P2P_amplitude   Trend_slope  Spectral_centroid  Spectral_rolloff  Spectral_entropy  Strength_of_trend  Mean_crossing_rate  Median_crossing_rate\n0   1.0     1.0  1.0  1.0      0.00            0.0 -7.850462e-17               0.00               0.0               0.0                inf                 0.0                   0.0\n0   1.0     1.0  1.0  1.0      0.00            0.0 -7.850462e-17               0.00               0.0               0.0                inf                 0.0                   0.0\n0   0.5     0.5  0.0  1.0      0.25            1.0 -1.000000e+00               0.25               0.5               0.0                inf                 1.0                   1.0\n0   0.0     0.0  0.0  0.0      0.00            0.0  0.000000e+00               0.00               0.0               0.0                inf                 0.0                   0.0\n&gt;&gt;&gt; validation_stats\n   Mean  Median  Min  Max  Variance  P2P_amplitude   Trend_slope  Spectral_centroid  Spectral_rolloff  Spectral_entropy  Strength_of_trend  Mean_crossing_rate  Median_crossing_rate\n0   1.0     1.0  1.0  1.0      0.00            0.0 -7.850462e-17               0.00               0.0               0.0                inf                 0.0                   0.0\n0   0.5     0.5  0.0  1.0      0.25            1.0 -1.000000e+00               0.25               0.5               0.0                inf                 1.0                   1.0\n0   0.0     0.0  0.0  0.0      0.00            0.0  0.000000e+00               0.00               0.0               0.0                inf                 0.0                   0.0\n0   0.0     0.0  0.0  0.0      0.00            0.0  0.000000e+00               0.00               0.0               0.0                inf                 0.0                   0.0\n</code></pre> Source code in <code>timecave/validation_methods/prequential.py</code> <pre><code>def statistics(self) -&gt; tuple[pd.DataFrame]:\n    \"\"\"\n    Compute relevant statistics for both training and validation sets.\n\n    This method computes relevant time series features, such as mean, strength-of-trend, etc. for both the whole time series, the training set and the validation set.\n    It can and should be used to ensure that the characteristics of both the training and validation sets are, statistically speaking, similar to those of the time series one wishes to forecast.\n    If this is not the case, using the validation method will most likely lead to a poor assessment of the model's performance.\n\n    Returns\n    -------\n    pd.DataFrame\n        Relevant features for the entire time series.\n\n    pd.DataFrame\n        Relevant features for the training set.\n\n    pd.DataFrame\n        Relevant features for the validation set.\n\n    Raises\n    ------\n    ValueError\n        If the time series is composed of less than three samples.\n\n    ValueError\n        If the folds comprise less than two samples.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; from timecave.validation_methods.prequential import RollingWindow\n    &gt;&gt;&gt; ts = np.hstack((np.ones(5), np.zeros(5)));\n    &gt;&gt;&gt; splitter = RollingWindow(5, ts);\n    &gt;&gt;&gt; ts_stats, training_stats, validation_stats = splitter.statistics();\n    Frequency features are only meaningful if the correct sampling frequency is passed to the class.\n    &gt;&gt;&gt; ts_stats\n       Mean  Median  Min  Max  Variance  P2P_amplitude  Trend_slope  Spectral_centroid  Spectral_rolloff  Spectral_entropy  Strength_of_trend  Mean_crossing_rate  Median_crossing_rate\n    0   0.5     0.5  0.0  1.0      0.25            1.0    -0.151515           0.114058               0.5           0.38717            1.59099            0.111111              0.111111\n    &gt;&gt;&gt; training_stats\n       Mean  Median  Min  Max  Variance  P2P_amplitude   Trend_slope  Spectral_centroid  Spectral_rolloff  Spectral_entropy  Strength_of_trend  Mean_crossing_rate  Median_crossing_rate\n    0   1.0     1.0  1.0  1.0      0.00            0.0 -7.850462e-17               0.00               0.0               0.0                inf                 0.0                   0.0\n    0   1.0     1.0  1.0  1.0      0.00            0.0 -7.850462e-17               0.00               0.0               0.0                inf                 0.0                   0.0\n    0   0.5     0.5  0.0  1.0      0.25            1.0 -1.000000e+00               0.25               0.5               0.0                inf                 1.0                   1.0\n    0   0.0     0.0  0.0  0.0      0.00            0.0  0.000000e+00               0.00               0.0               0.0                inf                 0.0                   0.0\n    &gt;&gt;&gt; validation_stats\n       Mean  Median  Min  Max  Variance  P2P_amplitude   Trend_slope  Spectral_centroid  Spectral_rolloff  Spectral_entropy  Strength_of_trend  Mean_crossing_rate  Median_crossing_rate\n    0   1.0     1.0  1.0  1.0      0.00            0.0 -7.850462e-17               0.00               0.0               0.0                inf                 0.0                   0.0\n    0   0.5     0.5  0.0  1.0      0.25            1.0 -1.000000e+00               0.25               0.5               0.0                inf                 1.0                   1.0\n    0   0.0     0.0  0.0  0.0      0.00            0.0  0.000000e+00               0.00               0.0               0.0                inf                 0.0                   0.0\n    0   0.0     0.0  0.0  0.0      0.00            0.0  0.000000e+00               0.00               0.0               0.0                inf                 0.0                   0.0\n    \"\"\"\n\n    if self._n_samples &lt;= 2:\n\n        raise ValueError(\n            \"Basic statistics can only be computed if the time series comprises more than two samples.\"\n        )\n\n    if int(np.round(self._n_samples / self.n_splits)) &lt; 2:\n\n        raise ValueError(\n            \"The folds are too small to compute most meaningful features.\"\n        )\n\n    print(\"Frequency features are only meaningful if the correct sampling frequency is passed to the class.\")\n\n    full_features = get_features(self._series, self.sampling_freq)\n    training_stats = []\n    validation_stats = []\n\n    for (training, validation, _) in self.split():\n\n        training_feat = get_features(self._series[training], self.sampling_freq)\n        training_stats.append(training_feat)\n\n        validation_feat = get_features(self._series[validation], self.sampling_freq)\n        validation_stats.append(validation_feat)\n\n    training_features = pd.concat(training_stats)\n    validation_features = pd.concat(validation_stats)\n\n    return (full_features, training_features, validation_features)\n</code></pre>"},{"location":"API_ref/validation_methods/weights/","title":"Weighting functions","text":""},{"location":"API_ref/validation_methods/weights/#timecave.validation_methods.weights","title":"<code>timecave.validation_methods.weights</code>","text":"<p>This module contains built-in functions to compute weights for validation methods that so require.</p> <p>Functions:</p> Name Description <code>constant_weights</code> <p>Generates constant weights (i.e. every fold is weighted equally).</p> <code>linear_weights</code> <p>Folds are weighted linearly.</p> <code>exponential_weights</code> <p>Folds are weighted exponentially.</p> Notes <p>Users may write their own weighting function and pass it as an argument to a validation method class.  However, to make sure that the user-defined function is compatible with TimeCaVe, the function signature  must match those of the functions provided herein.</p>"},{"location":"API_ref/validation_methods/weights/constant/","title":"Constant weights","text":""},{"location":"API_ref/validation_methods/weights/constant/#timecave.validation_methods.weights.constant_weights","title":"<code>timecave.validation_methods.weights.constant_weights(n_splits, gap=0, compensation=0, params=None)</code>","text":"<p>Compute constant weights.</p> <p>This function computes a constant weight vector. It is called by the Growing Window,  Rolling Window, and Block CV by default.</p> <p>Parameters:</p> Name Type Description Default <code>n_splits</code> <code>int</code> <p>Number of splits the validation method will use.</p> required <code>gap</code> <code>int</code> <p>Number of folds separating the validation set from the training set.         Used by prequential methods.</p> <code>0</code> <code>compensation</code> <code>int</code> <p>A compensation factor that allows the function to generate the correct amount of weights.         0 for CV methods, +1 for prequential methods.         Additionally, if a gap is specified, it must be added to this compensation factor as well.</p> <code>0</code> <code>params</code> <code>dict</code> <p>Used for compatibility. Irrelevant for this function.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Weights.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from timecave.validation_methods.weights import constant_weights\n&gt;&gt;&gt; constant_weights(5);\narray([1., 1., 1., 1., 1.])\n</code></pre> <p>If a gap is specified, there will be fewer iterations. Therefore, fewer weights should be generated:</p> <pre><code>&gt;&gt;&gt; constant_weights(5, gap=1);\narray([1., 1., 1., 1.])\n</code></pre> <p>For a given number of folds, CV methods will run for an additional iteration compared to prequential  methods. Therefore, a compensation factor of 1 must be specified if one intends to use weighted prequential  methods:</p> <pre><code>&gt;&gt;&gt; constant_weights(5, gap=1, compensation=1);\narray([1., 1., 1.])\n</code></pre> Source code in <code>timecave/validation_methods/weights.py</code> <pre><code>def constant_weights(\n    n_splits: int, gap: int = 0, compensation: int = 0, params: dict = None\n) -&gt; np.ndarray:\n    \"\"\"\n    Compute constant weights.\n\n    This function computes a constant weight vector. It is called by the [Growing Window](../prequential/grow.md), \n    [Rolling Window](../prequential/roll.md), and [Block CV](../CV/block.md) by default.\n\n    Parameters\n    ----------\n    n_splits : int\n        Number of splits the validation method will use.\n\n    gap : int, default=0\n        Number of folds separating the validation set from the training set. \\\n        Used by [prequential methods](../prequential/index.md).\n\n    compensation : int, default=0\n        A compensation factor that allows the function to generate the correct amount of weights. \\\n        0 for [CV methods](../CV/index.md), +1 for [prequential methods](../prequential/index.md). \\\n        Additionally, if a gap is specified, it must be added to this compensation factor as well.\n\n    params : dict, default=None\n        Used for compatibility. Irrelevant for this function.\n\n    Returns\n    -------\n    np.ndarray\n        Weights.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from timecave.validation_methods.weights import constant_weights\n    &gt;&gt;&gt; constant_weights(5);\n    array([1., 1., 1., 1., 1.])\n\n    If a gap is specified, there will be fewer iterations. Therefore, fewer weights should be generated:\n\n    &gt;&gt;&gt; constant_weights(5, gap=1);\n    array([1., 1., 1., 1.])\n\n    For a given number of folds, CV methods will run for an additional iteration compared to prequential \n    methods. Therefore, a compensation factor of 1 must be specified if one intends to use weighted prequential \n    methods:\n\n    &gt;&gt;&gt; constant_weights(5, gap=1, compensation=1);\n    array([1., 1., 1.])\n    \"\"\"\n\n    splits = n_splits - gap - compensation\n\n    return np.ones(splits)\n</code></pre>"},{"location":"API_ref/validation_methods/weights/exponential/","title":"Exponential weights","text":""},{"location":"API_ref/validation_methods/weights/exponential/#timecave.validation_methods.weights.exponential_weights","title":"<code>timecave.validation_methods.weights.exponential_weights(n_splits, gap=0, compensation=0, params={'base': 2})</code>","text":"<p>Compute exponential weights.</p> <p>This function computes a exponential weight vector. It may be passed to the Growing Window,  Rolling Window, and Block CV classes.</p> <p>Parameters:</p> Name Type Description Default <code>n_splits</code> <code>int</code> <p>Number of splits the validation method will use.</p> required <code>gap</code> <code>int</code> <p>Number of folds separating the validation set from the training set.         Used by prequential methods.</p> <code>0</code> <code>compensation</code> <code>int</code> <p>A compensation factor that allows the function to generate the correct amount of weights.         0 for CV methods, +1 for prequential methods.         Additionally, if a gap is specified, it must be added to this compensation factor as well.</p> <code>0</code> <code>params</code> <code>dict</code> <p>Parameters from which to generate the weights. Only <code>base</code> needs to be specified.  Any other parameter will be ignored.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Weights.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If <code>base</code> is neither an integer nor a float.</p> <code>ValueError</code> <p>If <code>base</code> is not positive.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from timecave.validation_methods.weights import exponential_weights\n&gt;&gt;&gt; exponential_weights(5);\narray([0.03225806, 0.06451613, 0.12903226, 0.25806452, 0.51612903])\n</code></pre> <p>If a gap is specified, there will be fewer iterations. Therefore, fewer weights should be generated:</p> <pre><code>&gt;&gt;&gt; exponential_weights(5, gap=1);\narray([0.06666667, 0.13333333, 0.26666667, 0.53333333])\n</code></pre> <p>For a given number of folds, CV methods will run for an additional iteration compared to prequential  methods. Therefore, a compensation factor of 1 must be specified if one intends to use weighted prequential  methods:</p> <pre><code>&gt;&gt;&gt; exponential_weights(5, gap=1, compensation=1);\narray([0.14285714, 0.28571429, 0.57142857])\n</code></pre> Source code in <code>timecave/validation_methods/weights.py</code> <pre><code>def exponential_weights(\n    n_splits: int, gap: int = 0, compensation: int = 0, params: dict = {\"base\": 2}\n) -&gt; np.ndarray:\n    \"\"\"\n    Compute exponential weights.\n\n    This function computes a exponential weight vector. It may be passed to the [Growing Window](../prequential/grow.md), \n    [Rolling Window](../prequential/roll.md), and [Block CV](../CV/block.md) classes.\n\n    Parameters\n    ----------\n    n_splits : int\n        Number of splits the validation method will use.\n\n    gap : int, default=0\n        Number of folds separating the validation set from the training set. \\\n        Used by [prequential methods](../prequential/index.md).\n\n    compensation : int, default=0\n        A compensation factor that allows the function to generate the correct amount of weights. \\\n        0 for [CV methods](../CV/index.md), +1 for [prequential methods](../prequential/index.md). \\\n        Additionally, if a gap is specified, it must be added to this compensation factor as well.\n\n    params : dict, default=None\n        Parameters from which to generate the weights. Only `base` needs to be specified. \n        Any other parameter will be ignored.\n\n    Returns\n    -------\n    np.ndarray\n        Weights.\n\n    Raises\n    ------\n    TypeError\n        If `base` is neither an integer nor a float.\n\n    ValueError\n        If `base` is not positive.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from timecave.validation_methods.weights import exponential_weights\n    &gt;&gt;&gt; exponential_weights(5);\n    array([0.03225806, 0.06451613, 0.12903226, 0.25806452, 0.51612903])\n\n    If a gap is specified, there will be fewer iterations. Therefore, fewer weights should be generated:\n\n    &gt;&gt;&gt; exponential_weights(5, gap=1);\n    array([0.06666667, 0.13333333, 0.26666667, 0.53333333])\n\n    For a given number of folds, CV methods will run for an additional iteration compared to prequential \n    methods. Therefore, a compensation factor of 1 must be specified if one intends to use weighted prequential \n    methods:\n\n    &gt;&gt;&gt; exponential_weights(5, gap=1, compensation=1);\n    array([0.14285714, 0.28571429, 0.57142857])\n    \"\"\"\n\n    _check_params(params[\"base\"])\n    splits = n_splits - gap - compensation\n    weights = np.array([params[\"base\"] ** i for i in range(splits)])\n    weights = weights / weights.sum()\n\n    return weights\n</code></pre>"},{"location":"API_ref/validation_methods/weights/linear/","title":"Linear weights","text":""},{"location":"API_ref/validation_methods/weights/linear/#timecave.validation_methods.weights.linear_weights","title":"<code>timecave.validation_methods.weights.linear_weights(n_splits, gap=0, compensation=0, params={'slope': 2})</code>","text":"<p>Compute linear weights.</p> <p>This function computes a linear weight vector. It may be passed to the Growing Window,  Rolling Window, and Block CV classes.</p> <p>Parameters:</p> Name Type Description Default <code>n_splits</code> <code>int</code> <p>Number of splits the validation method will use.</p> required <code>gap</code> <code>int</code> <p>Number of folds separating the validation set from the training set.         Used by prequential methods.</p> <code>0</code> <code>compensation</code> <code>int</code> <p>A compensation factor that allows the function to generate the correct amount of weights.         0 for CV methods, +1 for prequential methods.         Additionally, if a gap is specified, it must be added to this compensation factor as well.</p> <code>0</code> <code>params</code> <code>dict</code> <p>Parameters from which to generate the weights. Only <code>slope</code> needs to be specified.  Any other parameter will be ignored.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Weights.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If <code>slope</code> is neither an integer nor a float.</p> <code>ValueError</code> <p>If <code>slope</code> is not positive.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from timecave.validation_methods.weights import linear_weights\n&gt;&gt;&gt; linear_weights(5);\narray([0.06666667, 0.13333333, 0.2       , 0.26666667, 0.33333333])\n</code></pre> <p>If a gap is specified, there will be fewer iterations. Therefore, fewer weights should be generated:</p> <pre><code>&gt;&gt;&gt; linear_weights(5, gap=1);\narray([0.1, 0.2, 0.3, 0.4])\n</code></pre> <p>For a given number of folds, CV methods will run for an additional iteration compared to prequential  methods. Therefore, a compensation factor of 1 must be specified if one intends to use weighted prequential  methods:</p> <pre><code>&gt;&gt;&gt; linear_weights(5, gap=1, compensation=1);\narray([0.16666667, 0.33333333, 0.5       ])\n</code></pre> Source code in <code>timecave/validation_methods/weights.py</code> <pre><code>def linear_weights(\n    n_splits: int, gap: int = 0, compensation: int = 0, params: dict = {\"slope\": 2}\n) -&gt; np.ndarray:\n    \"\"\"\n    Compute linear weights.\n\n    This function computes a linear weight vector. It may be passed to the [Growing Window](../prequential/grow.md), \n    [Rolling Window](../prequential/roll.md), and [Block CV](../CV/block.md) classes.\n\n    Parameters\n    ----------\n    n_splits : int\n        Number of splits the validation method will use.\n\n    gap : int, default=0\n        Number of folds separating the validation set from the training set. \\\n        Used by [prequential methods](../prequential/index.md).\n\n    compensation : int, default=0\n        A compensation factor that allows the function to generate the correct amount of weights. \\\n        0 for [CV methods](../CV/index.md), +1 for [prequential methods](../prequential/index.md). \\\n        Additionally, if a gap is specified, it must be added to this compensation factor as well.\n\n    params : dict, default=None\n        Parameters from which to generate the weights. Only `slope` needs to be specified. \n        Any other parameter will be ignored.\n\n    Returns\n    -------\n    np.ndarray\n        Weights.\n\n    Raises\n    ------\n    TypeError\n        If `slope` is neither an integer nor a float.\n\n    ValueError\n        If `slope` is not positive.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from timecave.validation_methods.weights import linear_weights\n    &gt;&gt;&gt; linear_weights(5);\n    array([0.06666667, 0.13333333, 0.2       , 0.26666667, 0.33333333])\n\n    If a gap is specified, there will be fewer iterations. Therefore, fewer weights should be generated:\n\n    &gt;&gt;&gt; linear_weights(5, gap=1);\n    array([0.1, 0.2, 0.3, 0.4])\n\n    For a given number of folds, CV methods will run for an additional iteration compared to prequential \n    methods. Therefore, a compensation factor of 1 must be specified if one intends to use weighted prequential \n    methods:\n\n    &gt;&gt;&gt; linear_weights(5, gap=1, compensation=1);\n    array([0.16666667, 0.33333333, 0.5       ])\n    \"\"\"\n\n    _check_params(params[\"slope\"])\n    splits = n_splits - gap - compensation\n    weights = np.array([params[\"slope\"] * i for i in range(1, splits + 1)])\n    weights = weights / weights.sum()\n\n    return weights\n</code></pre>"}]}